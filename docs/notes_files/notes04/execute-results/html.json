{
  "hash": "4bc423829426cc8f303dad0795e28bc5",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"4\"\nauthor: \"Michael Weylandt\"\ntopic: \"Regression III: Non-Linear Regression\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n$$\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}} \\newcommand{\\V}{\\mathbb{V}} \\newcommand{\\argmin}{\\text{arg min}} \\newcommand{\\K}{\\mathbb{K}}$$\n\nThis week, we continue our discussion of 'fancy' regression\nfrom last week to consider _non-linear_ models. While we\nintroduce several methods for non-linear regression, we also\npause to consider _when_ non-linear methods can actually be\nexpected to outperform linear methods. \n\nAlso note that we are arguably being a bit 'sloppy' about \nwhat we mean by 'linear' methods. For purposes of these notes,\nwe define a 'linear' method as one where the fitted model has a\nlinear response to changes in the inputs; formally, one where\n\n$$\\frac{\\partial y}{\\partial x_i} = \\beta_i \\text{ (a constant) for all } i \\in \\{1, \\dots, p\\}$$\n\nThis definition includes things like best-subsets and lasso\nregression, which are not linear functions of $\\by$, but\nexcludes things like polynomial regression.[^poly] In the\nabove definition, each $\\beta_i$ is essentially a regression\ncoefficient, and values of $i$ where $\\beta_i = 0$ are\nnon-selected variables if we are using a sparse method.\n\n[^poly]: In other classes, you may have been taught that\npolynomial regression is a linear model. It is definitely linear\nin the sense that that class would have used (linear in $\\by$)\nbut we're using a different sense here. The core insight of \nthat analysis - that a non-linear function can be expressed as\na linear combination of non-linear parts - will appear several\ntimes in these notes.\n\n## Non-Linearity via Linear Combination of Non-Linear Parts\n\nWe have spent the previous few weeks developing a set of tools\nfor building _linear_ models of $\\by$. Specifically, we have\nsought ways to approximate $\\E[y | \\bx]$ as a weighted sum\nof each $x_i$: \n\n$$ \\E[y | \\bx] \\approx \\langle \\bx, \\hat{\\bbeta} \\rangle = \\sum_{i=1}^p x_i \\hat{\\beta}_i$$\n\nFor situations where the relationship is nearly (or even truly)\nlinear, this approximation can be quite useful and good\nestimates of the $\\beta_i$ coefficients can lead to accurate\nestimates of $\\E[y | \\bx]$, and ultimately accurate prediction \nof test set response $\\by$. Using the bias-variance\ndecomposition, we have seen that it is often worthwhile to accept\na bit of bias in estimation of each $\\beta_i$ if it is compensated by\na worthwhile reduction in the variance of $\\beta_i$. \n\nWe can further refine this equality by decomposing bias a bit further: \n\n$$\\text{MSE} = \\text{Irreducible Error} + \\text{Variance} + \\text{Model Bias}^2 + \\text{Estimation Bias}^2$$\n\nHere, we have decomposed $\\text{Bias}^2$ into two terms: \n\n- $\\text{Model Bias}^2$: a measure of the _systematic_ error arising\n  from use of a linear model to predict a non-linear DGP\n- $\\text{Estimation Bias}^2$: a measure of the _systematic_ error arising\n  from use of a regularized estimation procedure which exhibits shrinkage\n  \nPut another way, \"Model Bias\" results from use of a linear model when\nsomething non-linear should be used, while \"Estimation Bias\" arises from\nthe use of a biased method (like ridge regression) instead of something\nnominally unbiased like OLS. For the previous two weeks, we have mainly\nfocused on linear models for linear DGPs, so model bias has been zero. If\nwe expand our gaze to non-linear DGPs, we have to deal with model bias\na bit more directly. As with estimation bias, it is frequently worthwhile\nto accept model bias to reduce variance; see more discussion below. \n\n### Polynomial Expansion\n\nYou have likely already seen polynomial expansion (or polynomial regression)\nin previous courses. Essentially, PE fits a low(-ish)-order polynomial\ninstead of a line to data. More formally, let $f(\\bx) \\equiv \\E[y|\\bx]$\nbe the regression function (best possible predictor) that week seek to\napproximate from noisy observations. Standard linear regression essentially\nseeks to fit a first-order Taylor approximation of $f$ around some point $\\overline{\\bx}$:\n\n$$\\begin{align*}\nf(\\bx) &\\approx f(\\overline{\\bx}) + \\nabla_f(\\bx_0)^{\\top}(\\bx - \\overline{\\bx}) \\\\\n       &= f(\\overline{\\bx}) + \\sum_{i=1}^p \\left. \\frac{\\partial f}{\\partial x_i}\\right|_{x = \\overline{x}_i}(x_i - \\overline{x}_i) \n\\end{align*}$$\n\nIf we rearrange this a bit, we get\n\n$$\\begin{align*}\nf(\\bx) &\\approx f(\\overline{\\bx}) + \\sum_{i=1}^p \\left. \\frac{\\partial f}{\\partial x_i}\\right|_{x = \\overline{x}_i}(x_i - \\overline{x}_i) \\\\\n&= \\underbrace{\\left(f(\\overline{\\bx}) - \\sum_{i=1}^p \\left.\\frac{\\partial f}{\\partial x_i}\\right|_{x_i=\\overline{x}_i}\\overline{x}_i\\right)}_{=\\beta_0} + \\sum_{i=1}^p \\underbrace{\\left.\\frac{\\partial f}{\\partial x_i}\\right|_{x_i=\\overline{x}_i}}_{=\\beta_i} x_i\\\\\n&= \\beta_0 + \\sum_{i=1}^p x_i \\beta_i\n\\end{align*}$$\n\nso we see that the regression coefficients are more-or-less the\n(suitably-averaged) partial derivatives of $f$ while the intercept\nis the value of $f$ at the center of our expansion ($f(\\overline{\\bx})$)\nplus some differential adjustments. Note that, if the variables $\\bX$ are\ncentered so that $\\E[\\bX] = \\mathbf{0}$, the intercept is exactly the\naverage value of $f$, as we would expect.\n\nClearly, this linear approximation will do better in the situations\nwhere Taylor expansions generally perform better: when i) the derivative\nof $f$ is roughly constant and ii) the higher order terms are small. This \nmakes sense: if $f$ is very-close-to-linear, there is minimal loss from\nfitting a linear approximation to $f$; if $f$ is very non-linear, however,\na linear approximation can only be be so good, and we are saddled with\nsignificant model bias. \n\nIf we want to mitigate some of that model bias, we can choose to use\na higher-order Taylor series. If we examine a second order Taylor series, \nwe get a similar approximation as before, where $\\mathcal{H}$ denotes the _Hessian_\nmatrix of second derivatives: \n\n$$\\begin{align*}\nf(\\bx) &\\approx f(\\overline{\\bx}) + \\nabla_f(\\bx_0)^{\\top}(\\bx - \\overline{\\bx}) + \\frac{1}{2}(\\bx - \\overline{\\bx})^{\\top}\\mathcal{H}_f(\\overline{\\bx})(\\bx - \\overline{\\bx})^{\\top}\n\\end{align*}$$\n\nAfter some rearrangement this becomes: \n\n$$f(\\bx) \\approx \\beta_0 + \\sum_{i=1}^p \\frac{\\partial f}{\\partial x_i} x_i +\\sum_{i=1}^p \\frac{\\partial^2 f}{\\partial x_i^2} x_i^2 +  \\sum_{\\substack{i, j = 1 \\\\i \\neq j}}^p \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} x_ix_j$$\n\nWe recognize the constant term (intercept) and first\norder terms (linear slopes) from before, but now we\nhave added two types: \n\n- Second order terms ($x_i^2$): these capture higher\n  order non-linearities in a single variable. For\n  example, if $y = x^2$, then the 'true fit' really\n  lies in the second order term \n  $\\partial^2f / \\partial x^2 = 2$ instead of any \n  linear approximation\n- Cross terms ($x_ix_j$): these capture non-linear\n  (non-additive) multi-variate relationships between\n  features. You may have seen these before as\n  _interaction_ terms. \n  \nRules of thumb vary as to which set of terms are more\nimportant. Historically, statisticians have tended\nto put in the interaction (cross) terms first, though\nthis is far from universal practice. In particular, \nwhen dealing with 'binary' variables, the higher\norder term is unhelpful: if a feature is $0/1$ (did the\npatient receive the drug or not?), adding a squared\nterm has no effect since $0^2 = 0$ and $1^2 = 1$. \n\nRegardless, this is why we sometimes use 'polynomial\nexpansion' of our original variables. We are trying\nto capture the higher-order (non-linear) terms of the\nTaylor approximation. \n\nIt is worth thinking about what happens when we add\nhigher-order terms to our model. In particular, note\nthat we i) pick up some correlation among features; and\nii) we will generally have more variance since we have\nmore features. In practice, the correlation isn't too much\nof a problem and the actual polynomials fit by, *e.g.*, `poly`\nin `R` are modified to remove correlation. The variance however\ncan be a more significant problem. \n\nLet's see this in action. Let's first consider fitting polynomial regression to a simple (univariate) non-linear function:  $f(x) = \\sqrt{|x|^3 + 5} * \\cos(x)$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(sqrt(abs(x)^3 + 5) * cos(x), from=1, to=10)\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nThis is not *too* non-linear. It's still very smooth (*analytic* in \nthe jargon) and should be relatively easy to fit. Let's also generate\nsome noisy observations of this function. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 25\n# Linear regression doesn't care about the order of our data, but plotting does\n# so sorting gives better visuals\nx <- sort(runif(n, 2, 10)) \nEy <- sqrt(abs(x)^3 + 5) * cos(x)\ny <- Ey + rnorm(n, sd=3)\n\n\nx_grid <- seq(min(x), max(x), length.out=501)\nEy_grid <- sqrt(abs(x_grid)^3 + 5) * cos(x_grid)\n\nplot(x, y)\nlines(x_grid, Ey_grid)\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nWe can now fit some linear models to this data (shown in color): we use\nR's built-in `poly` function to automatically create the polynomials we\nseek. You can think of `poly(x, k)` as creating `k` features $x^1, x^2,\n\\dots, x^k$ but it actually does something a bit more subtle under the\nhood to make model fitting a bit more numerically stable.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\n\nfor(k in 1:5){\n    m <- lm(y ~ poly(x, k))\n    \n    yhat <- predict(m, data.frame(x = x_grid))\n    lines(x_grid, yhat, col=k+1)\n}\n\nlegend(\"bottomleft\",\n    c(\"True Regression Function\", \n         \"First Order (Linear) Fit\",\n         \"Second Order (Quadratic) Fit\", \n         \"Third Order (Cubic) Fit\", \n         \"Fourth Order (Quartic) Fit\",\n         \"Fifth Order (Quintic) Fit\"), \n       col=1:6, lty=1)\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nClearly, as we increase the order of the polynomial, we get a better\n(in-sample) fit. This makes sense: we know that more features gives us a\nbetter fit all else being equal, so what's the issue? \n\nAs usual, the issue occurs for extrapolation. You can actually see some\nissues already beginning to manifest at the ends of our prediction\nintervals: the higher-order polynomials go radically different directions\nas we extrapolate and even the 'best fit' (quintic) looks like it's going\nto be too steep as we go further. \n\nLet's expand the prediction region for these fits: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_pred <- seq(-5, 15, length.out=501)\nEy_pred <- sqrt(abs(x_pred)^3 + 5) * cos(x_pred)\nplot(x_pred, Ey_pred, type=\"l\")\npoints(x, y)\n\nfor(k in 1:5){\n    m <- lm(y ~ poly(x, k))\n    yhat <- predict(m, data.frame(x=x_pred))\n    lines(x_pred, yhat, col=k+1)\n}\n\nlegend(\"bottomleft\",\n    c(\"True Regression Function\", \n         \"First Order (Linear) Fit\",\n         \"Second Order (Quadratic) Fit\", \n         \"Third Order (Cubic) Fit\", \n         \"Fourth Order (Quartic) Fit\",\n         \"Fifth Order (Quintic) Fit\"), \n       col=1:6, lty=1)\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nThere's some problems!\n\nTo be fair, it is a bit unreasonable to expect these models to perform too well outside of our original sampling area. What's more worrying is that the extrapolations don't just miss the curves of the true regression function, they actually create their own even more intense wiggles. \n\nWe can also manifest this without extrapolation by fitting very high-order polynomials:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 9\nm <- lm(y ~ poly(x, degree=10 + k, raw=TRUE))\nyhat <- predict(m, data.frame(x = x_grid))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\n\nfor(k in 1:5){\n    m <- lm(y ~ poly(x, degree=14 + k, raw=TRUE))\n    yhat <- predict(m, data.frame(x = x_grid))\n    lines(x_grid, yhat, col=k)\n}\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nNote that we have to use `raw=TRUE` here to avoid `R` complaining about\nhaving too-high a degree in the polynomial. (`R` gives an error about\n\"unique points\" but the real issue is one about matrix rank, not ties in\n`x`.) I'm also surpressing some warnings here about a sub-standard fit.\n\nSo how can we address this 'over-wiggliness'? \n\nAs we discussed last time, we need to use a different set of functions: one that is smooth (like polynomials), but not too-high order. It would also be really nice if we could get something 'adaptive' - allowing for more wiggliness where we have more data (and we need more wiggliness) and less wiggliness where we don't have enough data (fall-back to linearity). \n### Local Linear Models\n\nOne way to do this is the idea of \"local linear (polynomial) models.\"\nInstead of fitting a single (global) line, we can fit different lines in\ndifferent parts of our data: that way, we can get an upward line when the\ntrue (non-linear) relationship is increasing and a downward line when the\ntrue relationship is decreasing. Or at least that's the hope!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\n\n# Divide our data into five 'buckets' and fit sub-models\n# This works because we've already sorted our data\n# (What would happen if we hadn't?)\nfor(k in 1:5){\n    xk <- x[(5 * k - 4):(5*k)]\n    yk <- y[(5 * k - 4):(5*k)]\n    mk <- lm(yk ~ xk)\n    y_hat <- predict(mk)\n    lines(xk, y_hat, col=\"red4\")\n}\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nDefinitely some rough edges here - particularly in the areas between our buckets, but we're on a good path. The `locfit` package will help us with the details:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(locfit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlocfit 1.5-9.11 \t 2025-01-27\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'locfit'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    none\n```\n\n\n:::\n\n```{.r .cell-code}\nm <- (locfit(y ~ lp(x)))\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimation type: Local Regression \n\nCall:\nlocfit(formula = y ~ lp(x))\n\nNumber of data points:  25 \nIndependent variables:  x \nEvaluation structure: Rectangular Tree \nNumber of evaluation points:  7 \nDegree of fit:  2 \nFitted Degrees of Freedom:  5.305 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat <- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nNot perfect, but pretty nice compared to what we had before. \n\nYou can also see a discussion of the \"degrees of freedom\" in the model\noutput. This is not exactly the DoF you learned in earlier classes, but\nit's sort of \"morally equivalent.\" This local fit is about as flexible as\na 4th degree polynomial would be for this problem. Even though this model\nis made out of quadratics, it's more flexible than a single quadratic.\nBut we avoid the extreme variability associated with a polynomial of that\nhigh order. Win-win!\n\nWe can tweak some parameters of the local fit to get different responses:\nthe big ones are the degree (`deg`) and the number of neighbors to use. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?lp\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- (locfit(y ~ lp(x, deg=4)))\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimation type: Local Regression \n\nCall:\nlocfit(formula = y ~ lp(x, deg = 4))\n\nNumber of data points:  25 \nIndependent variables:  x \nEvaluation structure: Rectangular Tree \nNumber of evaluation points:  7 \nDegree of fit:  4 \nFitted Degrees of Freedom:  7.473 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat <- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nPretty nice. If we 'turn down' the number of neighbors used: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- (locfit(y ~ lp(x, nn=0.2, deg=4)))\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimation type: Local Regression \n\nCall:\nlocfit(formula = y ~ lp(x, nn = 0.2, deg = 4))\n\nNumber of data points:  25 \nIndependent variables:  x \nEvaluation structure: Rectangular Tree \nNumber of evaluation points:  29 \nDegree of fit:  4 \nFitted Degrees of Freedom:  21.873 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat <- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nToo far - super crazy! But not entirely unexpected - we know that\n$K$-Nearest Neighbors for small $K$ has a huge variance. \n\nToo far!\n\nThe `loess` function in base `R` does this very nicely as well without \nrequiring additional packages. For practical work, it's a very nice\ntool for univariate modeling.[^pyloess]\n\n[^pyloess]: In Python, the `statsmodels` package also implements [LOESS\nfits](https://www.statsmodels.org/dev/examples/notebooks/generated/lowess.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- loess(y ~ x)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nloess(formula = y ~ x)\n\nNumber of Observations: 25 \nEquivalent Number of Parameters: 4.66 \nResidual Standard Error: 3.639 \nTrace of smoother matrix: 5.12  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate\t  cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat <- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n### Spline Regression (Additive Models)\n\n### Splines\n\nWe can generalize this idea a bit using splines. Splines are functions that solve a penalized approximation problem: \n\n$$\\hat{f} = \\text{arg min}_{f} \\frac{1}{2n} \\|y - f(x)\\|_2^2 + \\lambda \\int |f''(x)|^2 \\text{d}x$$\n\nHere we are saying we'll take _any function_ (not just a polynomial) \nthat achieves the optimal trade-off between data fit (first / loss term)\nand not being too rough (second / penalty term). \n\nWhy is this the right penalty to use? In essence, we are trying to\npenalize 'deviation from linearity' and since linear functions have\nsecond derivative 0 (by definition) the integral of the second\nderivative gives us a measure of non-linearity. \n\nIn some remarkable work, [Grace Wahba](https://en.wikipedia.org/wiki/Grace_Wahba) \nand co-authors showed that the solutions to that optimization problem are\npiecewise polynomials with a few additional constraints - these functions are\n_splines_. In addition to piecewise polynomialness, splines also guarantee:\n\n- continuity of the function and its derivatives\n- linearity outside the range of data fit ('natural splines')\n\nBecause splines not only match the value of the function at the knots\n(places where the 'pieces' match up) but also the derivatives, they are very\nsmooth indeed. \n\nStepping back, splines are just a different sort of feature engineering.\nInstead of using polynomial basis functions ($x^1, x^2, \\dots, x^k$), splines\nuse a much smoother basis and hence give smoother results, avoiding the 'wiggles'\nproblem we saw above.\n\n`R` provides the basic tools for spline fitting in the `splines` package. For\nreal work, you almost surely want to use the advanced functionality of the `mgcv`\npackage. For more on splines and models using them ('additive models'), see \n[this online course](https://noamross.github.io/gams-in-r-course/).\n\nWe can see that individual splines are quite nice little functions and usually\nare only non-zero on small regions: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\n\nnsx <- ns(x_grid, df=5) # The natural spline basis\nplot(x_grid, apply(nsx, 1, max), ylim=range(nsx), type=\"n\")\n\nfor(i in 1:5){\n    lines(x_grid, nsx[,i], col=i)\n}\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nWe can use linear combinations of these 'bumps' to fit non-linear functions, \nincluding our example from above: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(y ~ ns(x, df=5))\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat <- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n(This basically matches what we had before with $\\text{DoF} \\approx 4.7$, but\nyou can get different answers by messing with the `df` parameter  here.)\n\nOur predicted response is actually the sum of the individual spline effects: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(y ~ ns(x, df=5))\ny_hat <- predict(m, data.frame(x = x_grid))\n\nplot(x, y, lwd=2)\nlines(x_grid, Ey_grid)\nfor(i in 1:5){\n    lines(x_grid, ns(x_grid, df=5)[,i] * coef(m)[-1][i], col=i+1)\n}\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nIf we were able to sum up the colored lines, we would get the black line back. \n\nWe can also see that natural splines give smooth predictions outside the range\nof the original data: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred <- predict(m, data.frame(x = x_pred))\n\nplot(x_pred, Ey_pred, type=\"l\")\nlines(x_pred, y_pred, col=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nAdmittedly, not ideal, but it will do. In particular, note that outside of the\n'data region', we fall back on a nice comfortable \n\n(You can repeat this process with the slightly more common $b$-splines, but the differences aren't huge.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(y ~ bs(x, df=5))\ny_hat <- predict(m, data.frame(x = x_grid))\n\nplot(x, y, xlim=range(x_grid))\nlines(x_grid, Ey_grid)\nfor(i in 1:5){\n    lines(x_grid, bs(x_grid, df=5)[,i] * coef(m)[-1][i], col=i+1)\n}\n\ny_pred <- predict(m, data.frame(x = x_grid))\n\nlines(x_grid, y_pred, col=\"red4\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\nFor a 'full-strength' version of spline fitting, you can use the `mgcv` package:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: nlme\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'nlme'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n```\n\n\n:::\n\n```{.r .cell-code}\nm <- gam(y ~ s(x))\n\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.8865     0.6353  -1.395    0.179\n\nApproximate significance of smooth terms:\n       edf Ref.df     F p-value    \ns(x) 5.582  6.664 30.78  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.896   Deviance explained =   92%\nGCV = 13.695  Scale est. = 10.089    n = 25\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- gam(y ~ s(x))\ny_hat <- predict(m, data.frame(x = x_grid))\n\nplot(x, y, xlim=range(x_grid))\nlines(x_grid, Ey_grid)\nlines(x_grid, y_hat, col=\"red4\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](notes04_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\nRun `?s` to see the _many_ features of the `mgcv` package. `mgcv` also includes\nseveral specialized splines (*e.g.* periodic) which you may find useful in some\nproblems. \n\nSplines are not normally consider a 'machine learning' tool but they are\n_incredibly_ useful in applied statistics work. (The two methods you will rely\non most from this class are spline regression and random forests.) \n\n### $\\ell_1$-Filtering\n\n## Kernel Methods\n\n### Manual feature expansion\n\nWe can think of feature expansion as mapping our data to a higher-dimensional space $\\Phi: \\R^p \\to \\R^P$ and fitting a linear model there. As we have seen above,\nsplines and their kin provide a useful way of constructing the mapping $\\Phi$, \nbut we are still constrained to work with a fairly restricted type of problem. \n\nCan we generalize this formally to more interesting maps $\\Phi(\\cdot)$? \nYes - via kernel methods!\n\n### Ridge without coefficients\n\nBefore introducing kernel methods formally, let's look back to ridge regression.\nWe showed that the ridge solution is given by \n\n$$\\hat{\\beta}_{\\text{Ridge}} = (\\bX^T\\bX + \\lambda \\bI)^{-1}(\\bX^T\\by)$$\n\nSome nifty linear algebra will let us rewrite this as \n\n$$\\hat{\\beta}_{\\text{Ridge}} = \\lambda^{-1}\\bX^T(\\bX\\bX^{\\top}/\\lambda + \\bI)^{-1}\\by = \\bX^{\\top}(\\bX\\bX^{\\top}+\\lambda \\bI)^{-1}\\by$$\n\nYou can prove this using the [Woodbury Matrix Identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity)\nor we can just check it for a single data set and trust that it generalizes: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100\np <- 5 \n\nX <- matrix(rnorm(n * p), nrow=n, ncol=p) \n\nEy <- X[,1] + sqrt(X[,2]^2 + 5) + 0.1 * X[,3]^3 + cos(abs(X[,4])) + 1/(abs(X[,5]) + 3)\ny <- Ey + rnorm(n, sd=sqrt(0.25))\n\nlambda <- 2\neye <- function(p) diag(1, p, p)\nbeta_hat_ridge <- solve(crossprod(X) + lambda * eye(p), crossprod(X, y))\n\nbeta_hat_ridge_alt <- t(X) %*% solve(tcrossprod(X) + lambda * eye(n)) %*% y\n\ncbind(beta_hat_ridge, beta_hat_ridge_alt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]        [,2]\n[1,]  1.43685611  1.43685611\n[2,] -0.50138191 -0.50138191\n[3,]  0.38435758  0.38435758\n[4,] -0.05680342 -0.05680342\n[5,]  0.05131597  0.05131597\n```\n\n\n:::\n:::\n\n\n\n\nSo they are the same! But why did we do this? \n\nIt is sometimes a bit more computationally efficient if we have to invert an \n$n \\times n$ matrix instead of a $p \\times p$ matrix, but there are faster \nmethods if we're really interested in speed. \n\nNote that, if we want to make a ridge regression _prediction_ now for a new\n$\\tilde{\\bx}$, we just need to compute: \n\n$$\\newcommand{\\bx}{\\mathbf{x}}\\begin{align*}\n\\hat{\\beta}_{\\text{Ridge}}^T\\tilde{\\bx} &= \\left[\\bX^{\\top}(\\bX\\bX^{\\top}+\\lambda \\bI)^{-1}\\by\\right]^T\\tilde{\\bx} \\\\\n&= \\left[\\by^T(\\bX\\bX^{\\top}+\\lambda \\bI)^{-T}\\bX\\right]\\tilde{\\bx} \\\\\n&= \\by^T(\\bX\\bX^{\\top}+\\lambda \\bI)^{-1} \\bX\\tilde{\\bx}\n\\end{align*}$$\n\n\nIntuitively, recall that the inner product of two vectors measures the angle\nbetween them and can, up to some scaling, be used as a 'similarity' measure. \nThis result essentially says that our prediction on a new data set is a weighted\naverage of the training data, with weights based on similarity of the new point\nwith the training data. This is not a crazy structure...\n\nIf we look closer, we see that we **only** need to compute products of the form\n$\\bx_1\\bx_2$ to make this work. This is where the magic of kernels lies.\n\n### Kernel Trick\n\nEarlier, we considered non-linear regression by _feature expansion_, where we\nreplaced $\\bX$ by $\\Phi(\\bX)$, where $\\Phi: \\R^p \\to \\R^P$ maps to a\nhigher-dimensional space (applied row-wise). We can use this mapping in our new\nridge formula to get our non-linear predictions as \n\n$$\\begin{align*}\n\\hat{y}(\\tilde{\\bx}) &= \\by^T(\\Phi(\\bX)\\Phi(\\bX)^{\\top}+\\lambda \\bI)^{-1} \\Phi(\\bX)\\Phi(\\tilde{\\bx})\n\\end{align*}$$\n\nLonger, but not necessarily better. If $P$ is very large, computing with \n$\\Phi(X)$ can be *incredibly* expensive.\n\nIt turns out, however, that we never need to actually form $\\Phi(X)$, we only \nneed $\\kappa(\\bx_1, \\bx_2) = \\Phi(\\bx_1)^T\\Phi(\\bx_2)$. If we can compute \n$\\kappa$ directly, we never need to work with $\\Phi$. \n\nFunctions that allow this are called (Mercer) kernels and they are just a \nlittle bit magical.\n\nIf we let $\\K = \\Phi(X)\\Phi(X)^{\\top}$ be defined by\n$\\K_{ij} = \\kappa(\\bx_i, \\bx_j)$, then we can write our feature-augmented ridge regression as: \n\n$$\\hat{y}(\\tilde{\\bx}) = \\by^T(\\K+\\lambda \\bI)^{-1} \\kappa(\\bX, \\tilde{\\bx})$$\n\nIf we break this apart, we see that our predictions at the new point $\\tilde{\\bx}$ are essentially just weighted averages of our original observations $\\by$, weighted by the similarity between the new point and our training points $\\kappa(\\bX, \\cdot)$. This intuition is super important and we'll dig into it further below. \n\nLet's try out _Kernel(ized) Ridge Regression_:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(kernlab)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'kernlab'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    cross\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n```\n\n\n:::\n\n```{.r .cell-code}\n## Define a kernel function - we'll spell this out below.\nrbf <- rbfdot(sigma = 0.05)\n\n## calculate kernel matrix\nK <- kernelMatrix(rbf, X)\ndim(K)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100 100\n```\n\n\n:::\n:::\n\n\n\nNote that this is a $n \\times n$ matrix - not a $p \\times p$ matrix!\n\nWe can now use this to make predictions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkrr <- function(x, lambda=1) {\n    crossprod(y, solve(K + lambda * eye(n), kernelMatrix(rbf, X, x)))\n}\n\nkrr_MSE <- mean((y - krr(X))^2)\nprint(krr_MSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4519902\n```\n\n\n:::\n:::\n\n\n\nThis is _better_ than the OLS MSE: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(resid(lm(y ~ X))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4985457\n```\n\n\n:::\n:::\n\n\n\nWhy? How is this not a contradiction of everything said before. \n\nBut how do we actually _do_ kernel multiplication? \n\nThere are _many_ kernel functions in the world. The most common are defined by: \n\n- $\\kappa(\\bx_1, \\bx_2) = \\bx_1^{\\top}\\bx_2$. This is the linear kernel, \n  equivalent to non-kernel methods\n- $\\kappa(\\bx_1, \\bx_2) = (\\bx_1^{\\top}\\bx_2 + c)^d$. This is the _polynomial\n  kernel_, equivalent to fitting polynomial regression up to degree $d$ with all\n  cross products. (The role of $c$ is tricky, but it essentially controls the\n  relative weight of the higher and lower order terms)\n- $\\kappa(\\bx_1, \\bx_2) = e^{-\\sigma^2\\|\\bx_1 - \\bx_2\\|^2}$. This is the \n  _squared exponential_ or _radial basis function_ (RBF) kernel; it is very \n  popular in spatial statistics, where it is closely related to a method known\n  as [kriging](https://en.wikipedia.org/wiki/Kriging).\n\nWhile the first two kernels give us a natural $\\Phi(\\cdot)$ mapping, the\n$\\Phi(\\cdot)$ for the RBF kernel is actually _infinite dimensional_: it lets \nus fit function classes we could not fit without the kernel trick.\n\nFor even more kernels, see: \n\n- [David Duvenaud's Kernel Cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/)\n- [GPML Section 4.2.1](http://gaussianprocess.org/gpml/chapters/RW.pdf)\n\nMuch of our current understanding of deep learning is based on specialized\nkernel methods: \n\n- The [Neural Tangent Kernel](https://en.wikipedia.org/wiki/Neural_tangent_kernel)\n- [Kernel Methods for Deep Learning](https://papers.nips.cc/paper_files/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html)\n- [Gradient Descent as a Kernel Machine](https://arxiv.org/abs/2012.00152) (This paper is a bit controversial)\n\n[This blog](https://rajatvd.github.io/NTK/) is a particularly nice \n(but still somewhat technical) introduction to the Neural Tangent Kernel and \nits relationship to neural networks.\n\nSpline theory has also found use to explain ReLU (piecewise linear activation)\nneural networks: \n\n- [Mad Max: Spline Insights into Deep Learning](https://arxiv.org/abs/1805.06576)\n- [Representer Theorems for Neural Networks and Ridge Splines](https://jmlr.csail.mit.edu/papers/v22/20-583.html)\n\nWe can also _kernelize_ some of the other methods we've studied in this course. \n\nNotably, we can kernelize $K$-Nearest Neighbors to get \"KKNN\" if we modify the\ndefinition of distance used to be \"kernel-ish.\" We first recall that we can\nwrite distances solely in terms of inner products:  \n\n$$\\begin{align*}\n\\|\\bx_1 - \\bx_2\\|_2^2 &= (\\bx_1 - \\bx_2)^T(\\bx_1 - \\bx_2) \\\\\n&= \\bx_1^T\\bx_1 - \\bx_2^T\\bx_1 - \\bx_1^T\\bx_2 + \\bx_2^T\\bx_2 \\\\\n&= \\bx_1^T\\bx_1  - 2\\bx_1^T\\bx_2 + \\bx_2^T\\bx_2 \\\\\n\\implies \\text{Kernel Distances} &= \\kappa(\\bx_1, \\bx_1) - 2\\kappa(\\bx_1, \\bx_2) + \\kappa(\\bx_2, \\bx_2)\n\\end{align*}$$\n\nIf we compute distances this way for a given $\\kappa$, we get kernel KNN. This\nis particularly nice when we have kernels that capture specific behaviors \n(*e.g.*, periodic or embedded categorical) that we can't really treat as \nEuclidean.\n\n## Trade-Offs between Linear and Non-Linear Methods\n\nHaving developed some tools for non-linear regression, let's step back and ask\nwhether they are worthwhile. Recall our error decomposition: \n\n$$\\text{MSE} = \\text{Irreducible Error} + \\text{Variance} + \\text{Model Bias}^2 + \\text{Estimation Bias}^2$$\n\nWe have already discussed how tools like ridge and lasso let us remove variance\nat the cost of (estimation) bias. \n\nWe argued that this was a worthwhile trade\nwhen the problem had a lot of 'innate' variance, either from having\nlarge noise variance ($\\V[\\epsilon] \\gg 0$) or from having many features. \nIn particular, use of some sort of shrinkage (ridge or lasso penalization) was\nessential in the 'high-dimensional' case ($p > n$) where OLS had so much\nvariance that it was not even uniquely defined. We also argued that, as we \ngot more and more data ($n \\to \\infty$), the variance took care of itself in \nthe usual statistical way. \n\nThese lessons generalize to the non-linear vs linear debate as well. Choosing\nto use a linear model is itself a variance reducing choice - there are 'more'\ncurves than lines in some sense. If we restrict our attention to linear models\nonly, we are potentially accepting some Model Bias, again with a hope of reducing\nvariance to get overall better performance. As such, the same intuition as above\napplies: linear models are preferred when variance is the primary concern, either\nfrom noisy data or from small data; as we get more data, variance decreases\nnaturally, so going to non-linear models reduces bias, giving overall \nsmaller error. \n\nIn the non-linear context, the 'estimation bias / variance' trade-off remains, \nbut modern tools like `mgcv` essentially handle this automatically for us. It\nis of course still there, but `mgcv` has some [nifty\nauto-tuning](https://doi.org/10.1080/01621459.2016.1180986) built-in.[^gambook]\n\n[^gambook]: If you want to go much deeper into the practical use of additive\n(spline) models, Simon Wood's book [Generalized Additive\nModels](https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331)\nis fantastic. \n\n## Other Topics in Regression\n\n### Alternative Loss Functions\n\nFor most of this class, we have used MSE as our loss function, building on a foundation of least squares. \n\nSquared ($\\ell_2^2$) error is a bit interesting: as an error gets _larger_, it counts even more. This makes OLS/MSE methods sensitive to outliers (in the same way the mean is), but it also implies that OLS won't work too hard to \"over-minimize\" small errors. \n\nWe can consider some other loss functions: \n- $\\ell_1$ error (absolute difference): robust to outliers, fits to the conditional _median_ instead of of the conditional _mean_. \n\n  The resulting _Median Absolute Deviation_ (MAD) regression no longer has a closed form, but can be solved quickly\n  using tools like `CVX.`\n  \n- Huber loss: a blend of $\\ell_2$ and $\\ell_1$ error, which uses $\\ell_2$ for small errors and $\\ell_1$ for big errors. \n$$\\text{HL}_{\\delta}(x) = \\begin{cases} \\frac{1}{2}x^2 & |x| < \\delta \\\\ \\delta * (|x| - \\delta/2) & |x| \\geq \\delta \\end{cases}$$\n\n  This is still convex, so we can solve it with `CVX.`\n  \n- $\\epsilon$-insensitive: \n  $$L_{\\epsilon}(x) = (|x| - \\epsilon)_+ = \\begin{cases} 0 & |x| < \\epsilon \\\\ |x| - \\epsilon & |x| \\geq \\epsilon \\end{cases}$$\n  This captures the idea of \"close enough is good enough\" with \"close enough\" being defined by $\\epsilon$. It's a bit unnatural statistically, but relates to an important classification method we'll cover in a few weeks. \n  \n  \n### Multi-Task Regression\n\nIn some contexts, we may want to perform _multiple regressions_ at the same time. That is, we have a _vector_ of responses for each observation. The OLS generalization is straightforward: \n$$\\newcommand{\\bbeta}{\\mathbf{\\beta}}\\newcommand{\\bB}{\\mathbf{B}}\\newcommand{\\bY}{\\mathbf{Y}}\\argmin_{\\bB} \\frac{1}{2}\\|\\bY - \\bX\\bB\\|_F^2 \\implies \\hat{\\bB} = (\\bX^T\\bX)^{-1}\\bX^T\\bY$$\nso we essentially just fit different OLS models for each element of the response. This is not super interesting. \n\nMulti-task regression becomes interesting if we want to add structure to $\\bB$: a particularly common requirement is to select the _same set of features_ for each of the regression targets. We do this using a group lasso on _each row_ of $\\bB$: under this structure, if we select a feature for one part of the response, we'll use it for every part. (Can you see why?)\n\n$$\\argmin_{\\bB} \\frac{1}{2}\\|\\bY - \\bX\\bB\\|_F^2 + \\lambda \\sum_{j=1}^p \\|\\bB_{j\\cdot}\\|_2$$\n\nAnother common requirement is for $\\bB$ to be \"low-rank\" but we won't consider that here. If you want to look into it, keywords are \"nuclear-norm regularized.\"\n\n#### Weights and generalization\n\n- WLS loss: $\\newcommand{\\bW}{\\mathbf{W}}(\\by - \\bX\\bbeta)^T\\bW(\\by - \\bX\\bbeta)$ for known diagonal $\\bW$. \n  Weight samples based on their variance (higher variance gets smaller weights)\n- Weighted lasso: $+\\lambda\\sum_{j=1}^p w_j|\\beta_j|$. \n  Higher weights are less likely to be selected\n  \n  Basis of 'adaptive' lasso methods (not discussed)\n- Generalized ridge: $+\\frac{\\lambda}{2}\\bbeta^T\\Omega\\beta$\n\n  Used to 'smooth' $\\hat{\\beta}$ - useful for ridge analogue of fused lasso\n\n",
    "supporting": [
      "notes04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}