[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "",
    "text": "In lieu of a final exam, STA 9890 has a prediction competition worth approximately one-third of your final grade. For this year’s competition, students will be predicting property valuations in a mid-sized city on the US West Coast.\nThe 200 total points associated with this competition will be apportioned as:"
  },
  {
    "objectID": "reports/report02.html",
    "href": "reports/report02.html",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "Released to Students: 2025-03-11\nSubmission: 2025-04-18 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report02.html#research-report",
    "href": "reports/report02.html#research-report",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "text": "Research Report #02: Ensemble Learning Techniques for Fair Classification\nIn this Research Report, you will explore concepts of Machine Learning Fairness and see how ensemble learning techniques can be used to create a “fair” ensemble from “unfair” base learners.1 Along the way, you will\n\nUse a black-box optimizer to implement a form of regularized logistic regression\nDesign and implement a custom ensemble learning strategy\nImplement and Assess Various ML Fairness Metrics\n\nProject Skeleton\nYour report should be divided into three sections, covering the following:\n\nBackground on ML Fairness\nComputation - Implementing Regularized Logistic Regression with CVXR or cvxpy\n\nImplementation and Assessment of the FairStacks Ensembling Method\n\nAt a minimum, these should include the following elements:\n\nBackground on ML Fairness\n\nSocial background, including two examples of bias in automated decision systems\nOverview of definitions used to measure ML Fairness\nIdentification and import of a real data set on which we can evaluate the proposed methodology. Be sure to describe both the “raw” data and the fairness metric you are interested in. If there is relevant social context, be sure to describe that as well.\n\n\nComputation\n\nIntroduction to the CVX optimization framework\nImplementation of the following classification methods as base learners to be used in construction of the FairStacks ensemble:\n\nNaive Bayes (no CVX)\nLinear Discriminant Analysis (no CVX)\nSupport Vector Machines (use CVX)\nLogistic Regression (use CVX): plain, ridge, and lasso variants\nDecision trees (you may use an existing implementation - use individual tree(s) here, not a full random forest)\n\n\n\n\nImplementation and Assessment of FairStacks\n\nDefine the FairStacks Ensembling Problem, taking care to describe both the choice of loss function (logistic loss)2, regularization and any constraints used\nImplement the FairStacks Problem using CVX\n\nImplement a full model building (train-validation/ensembling-test split) pipeline to implement FairStacks robustly\nApply the FairStacks pipeline to your real data set and chosen fairness metric\nCompute the fairness and accuracy obtained by the FairStacks ensemble and compare it with the fairness and accuracy of the individual base learners. Does FairStacks succeed in its goal of improving both accuracy and fairness?\n\n\nAdditional Background\nML Fairness\nAs ML systems continue to permate our society, increasing interest has been paid to their effects on society. ML systems are highly effective at replicating the characteristics of their training data - both good (accuracy) and bad (bias).3 The book BHN reviews this emerging area in some detail.\nFor our purposes, we can restrict our attention to ‘fair classification’, i.e., making sure that ML systems treat different groups ‘equally’ (BHN §3). There are many definitions of ‘equally’ that can be appropriate, depending on the problem. In the simplest, demographic parity, the ML system should give the same fraction of positive (+1) labels to all demographic groups; this is the type of fairness embodied by phrases like “looks like America”. We quantify this with measures like deviation from demographic parity. If we divide our population into two groups \\(\\mathcal{G}_1, \\mathcal{G}_2\\), the deviation from demographic parity associated with a classifier \\(f: \\mathbb{R}^p \\to \\{0, 1\\}\\) is given by:\n\\[\\text{DDP}(f) = \\left|\\frac{1}{|\\mathcal{G}_1|}\\sum_{i \\in \\mathcal{G}_1} f(\\mathbf{x}_i) - \\frac{1}{|\\mathcal{G}_2|}\\sum_{i \\in \\mathcal{G}_2} f(\\mathbf{x}_i)\\right| = \\left|\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{G}_1} f(\\mathbf{x}) - \\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{G}_2} f(\\mathbf{x}) \\right|\\]\nThe 2018 FAccT Tutorial “21 Fairness Definitions and Their Politics” covers this and other fairnesss metrics. For purposes of this project, you should consider the fairness metric that is most appropriate to the social context surrounding your dataset of interest. Note that, if your interest is in a non-US data set, the relevant subgroups and legal doctrines may be significantly different than those (implicitly) assumed by most of the FairML literature.\nFairStacks\nFairStacks is a novel ensemble learning method which attempts to create a fair ensemble from a set of unfair base learners. At the highest level, FairStacks generalizes the following idea: if Approach A is biased against a group and Approach B is biased in favor of the same group, then Approach (A + B) will be approximately unbiased.\nTo put FairStacks into practice, we modify our basic ensembling procedure: in standard ensemble learning, we create a “model of models”, finding weights \\(\\hat{\\mathbf{w}}\\) such that\n\\[ \\hat{\\mathbf{w}} = \\text{arg min}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\text{Loss}\\left(\\mathbf{y}, \\sum_j w_j \\hat{f}_j(\\mathbf{x}_i)\\right) \\]\nwhere the inner sum is taken over the base learners \\(\\{\\hat{f}_j\\}\\). We add ‘fairness’ in the form of a regularizer (penalty). If \\(\\hat{b}_j\\) is the bias of base learner \\(\\hat{f}_j\\), FairStacks solves\n\\[ \\hat{\\mathbf{w}} = \\text{arg min}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\text{Loss}\\left(\\mathbf{y}, \\sum_j w_j \\hat{f}_j(\\mathbf{x}_i)\\right) + \\underbrace{\\lambda \\left|\\sum_j w_j \\hat{b}_j\\right|}_{\\text{Fairness Penalty}}\\]\nBy setting \\(\\lambda\\) large, we force the ‘average bias’ down to zero, resulting in a fairer ensemble. See the paper for more details.\nCVX\nThe CVX family of libraries attempt to make convex optimization easy and accesible to a wide range of users. Implementations of CVX can be found in:\n\nMatlab: the original CVX\n\nPython: cvxpy\n\nR: CVXR\n\nJulia: Convex.jl\n\n\nand more. While the details vary according to the host language, the essential structure is unchanged. I demonstrate the use of CVXR here to implement Lasso regression.\n\nset.seed(100)  # For reproducibility\nlibrary(CVXR)  # Load CVXR\nn &lt;- 400       # Set up a moderately high-dimensional problem\np &lt;- 1000\ns &lt;- 5\nsigma &lt;- 2\n\n# Lasso works best for IID Gaussian data\nX &lt;- matrix(rnorm(n * p), \n            nrow=n, ncol=p)\n\n# 'True' coefficients are mainly sparse with 5 non-zero values\nbeta_star &lt;- matrix(rep(0, p), ncol=1)\nbeta_star[1:s] &lt;- 3\n\n# Generate observed response from OLS DGP\ny &lt;- X %*% beta_star + rnorm(n, sd=sigma)\n\n## We are now ready to apply CVXR\n####  Also see discussion at\n####  https://www.cvxpy.org/examples/machine_learning/lasso_regression.html\n\nbeta &lt;- Variable(p) # Create 'beta' as a CVX _variable_ to be optimized\n\n# Per theory, about the right level of regularization to be used here\nlambda  &lt;- sigma * sqrt(s * log(p) / n) \nloss    &lt;- 1/(2 * n) * sum((y - X %*% beta)^2) # MSE Loss\npenalty &lt;- lambda * sum(abs(beta))\n\nobjective &lt;- Problem(Minimize(loss + penalty))\nbeta_hat  &lt;- solve(objective)$getValue(beta)\n\nWe can see that this correctly finds the non-zero elements:\n\nplot(beta_hat, \n     xlab=\"Coefficient Number\", \n     ylab=\"Lasso Estimate\", \n     main=\"CVX Lasso Solution\", \n     col=\"red4\", \n     pch=16)\n\n\n\n\n\n\n\nIf we look more closely, we see two important properties of the solution:\n\n\nNon-specialized solvers like CVXR cannot achieve exact zeros, so it is useful to do a bit of post-processing (e.g., zapsmall()).\nIf we need exact zeros, other approaches should be used\n\nThe lasso solution exhibits shrinkage, as the estimate are systematically smaller than the true values\n\n\nhead(beta_hat, n=10)\n\n               [,1]\n [1,]  2.435849e+00\n [2,]  2.498692e+00\n [3,]  2.376080e+00\n [4,]  2.418007e+00\n [5,]  2.424720e+00\n [6,]  1.756240e-21\n [7,]  1.604964e-21\n [8,] -1.193091e-21\n [9,] -9.722483e-22\n[10,] -1.320523e-21\n\n\nCVX is never the optimal approach for a given problem, but it is an incredibly useful tool for prototyping and trying out interesting new approaches without putting in the effort to derive and implement a specialized algorithm. (Tools like tensorflow and pytorch take this idea even further, but require significantly more effort to learn. If you are interested in working in ML after this course ends, they are a great place to invest your time.)\nThe documentation for the CVX packages has several examples which will be of use for you.\nPossible Topic(s) for Additional Research\nAlgorithms for Regularized Logistic Regression\nFor this report, you are not required to fit the FairStacks problem “by hand” and can use CVX instead. While CVX is flexible and easy to use, it rarely performs as efficiently as a hand-coded tailored algorithm. You may want to explore the use of convex optimization algorithms for fitting logistic regression problems generally and the FairStacks problem specifically.\nWhile the gradient descent method you used in Research Report #01 can be applied here, it is likely to be unbearably slow. You may achieve better performance using a Newton or Quadratic Approximation approach. These approaches use a (second-order) Taylor expansion to approximate the objective function by a quadratic function; that quadratic function has a closed-form minimizer (essentially the OLS solution) which is used to update the iterate.4 A new approximation is created at the new iterate, solved again, etc. See BV §9.5 for details. This approach is widely used to fit generalized linear models such as logistic regression. In the statistics literature, you may see it described as Iteratively Reweighted Least Squares or Fisher Scoring; see, e.g., the output of summary on a glm fit in R.\nAlternative Fair ML Proposals\nThe FairStacks approach is far from the only Fair ML proposal in the literature. If this is a topic that interest you, you may want to compare FairStacks to other proposals and see which performs the best on your data set and which achieves an acceptable level of fairness. (Not all methods can achieve all levels of fairness). Even though FairStacks worked the best on the problems we considered, it may not be the best for your data set - there is no free lunch after all!\nAdditional Randomization\nAs discussed in class, ensemble methods work best when the base learners exhibit relatively low degrees of correlation. (If all base learners make the same prediction, it does not matter how they are weighted in the ensemble.) This is often particularly difficult to guarantee when applying powerful ML techniques to not-too-difficult prediction problems, as any technique worth knowing will perform reasonably well. In the FairStacks paper, we used a novel technique called mini-patching5 to increase the variance of the base learner ensemble: mini-patching works by taking small subsets of both rows (observations) and columns (features) and fitting base learners to these subsets. We then fit the mini-patched learners using the FairStacks ensembling approach.\nDoes this technique improve the performance of FairStacks on your data set? Are there other techniques you can use to create alternative / better base learners to use in your ensemble?\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.6 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report02.html#footnotes",
    "href": "reports/report02.html#footnotes",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis Research Report is adapted from my ongoing research collaboration with Camille Little (Rice University) and Genevera Allen (Columbia University). A now-somewhat-dated manuscript describing our work in this space can be found online at ArXiv 2206.00074.↩︎\nIn the FairStacks paper, we used OLS as a classification method for somewhat technical reasons. For this Research Report, you should use logistic loss as your primary loss function. It may be an interesting extension to compare against OLS.↩︎\nThe earliest entries in the modern era of chatbots were marked by several flaws reflective their training data. Early iterations of the chat engine built into the Bing search engine were often rather overly-sexual and not-so-subtly malicious, perhaps reflecting the darker corners of the internet on which it was trained.↩︎\nFor this approach, you should use the matrix-analytic closed-form OLS solution instead of using ‘internal’ gradient descent. The aim of Newton methods is to avoid gradient descent.↩︎\nMini-Patching is related to, but should not be confused with the more common technique of Mini-Batching.↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "",
    "text": "There are no required textbooks for this course. Supplemental readings and suggested practice will be assigned from the following readily-available resources:\n\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nConvex Optimization by Boyd and Vandenberghe (BV)\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (ISL)\nElements of Statistical Learning by Hastie Tibshirani, and Freedman (ESL)\nStatistical Learning with Sparsity by Hastie, Tibshirani, and Wainwright (SLS)\nPattern Recognition and Machine Learning by Bishop (PRML)\nProbabilistic Machine Learning: An Introduction by Murphy (PML-1)\nProbabilistic Machine Learning: Advanced Topics by Murphy (PML-2)\nGaussian Processes for Machine Learning by Rasmussen and Williams (GPML)\nReinforcement Learning by Sutton and Hutto (RL)\nBoosting: Foundations and Algorithms by Schapire and Freund (SF)\nFoundations of Machine Learning by Mohri, Rostamizadeh, and Talwalkar (FML)\nUnderstanding Machine Learning: From Theory to Algorithms by Shalev-Shwartz and Ben-David (UML)\nPatterns, Predictions, and Actions by Hardt and Recht (HR)\nFairness and Machine Learning by Barocas, Hardt, and Narayanan (BHN)\nLearning Theory from First Principles by Bach (LTFP)\n\n\n\nSTA 9890 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nThe instructor will provide a formula sheet for use during in-class exams. No alternative resources may be used during these activities.\nStudents may use online resources, such as blog posts, discussion fora, etc. for the Research Reports and Course Competition but Generative AI tools are specifically disallowed.\n\nException: Coding Assistance Tools, e.g., GitHub CoPilot, may be used for the Research Reports and Course Competition. Their usage is limited to coding your ideas. They may not be used as idea generation tools.\n\nThe era of Generative AI is challenging and fast-paced. If you are unsure whether a tool is allowed in this course, please reach out to the instructor before using it. In matters of Academic Integrity, it is far better to ask permission than forgiveness."
  },
  {
    "objectID": "resources.html#course-resources",
    "href": "resources.html#course-resources",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "",
    "text": "There are no required textbooks for this course. Supplemental readings and suggested practice will be assigned from the following readily-available resources:\n\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nConvex Optimization by Boyd and Vandenberghe (BV)\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (ISL)\nElements of Statistical Learning by Hastie Tibshirani, and Freedman (ESL)\nStatistical Learning with Sparsity by Hastie, Tibshirani, and Wainwright (SLS)\nPattern Recognition and Machine Learning by Bishop (PRML)\nProbabilistic Machine Learning: An Introduction by Murphy (PML-1)\nProbabilistic Machine Learning: Advanced Topics by Murphy (PML-2)\nGaussian Processes for Machine Learning by Rasmussen and Williams (GPML)\nReinforcement Learning by Sutton and Hutto (RL)\nBoosting: Foundations and Algorithms by Schapire and Freund (SF)\nFoundations of Machine Learning by Mohri, Rostamizadeh, and Talwalkar (FML)\nUnderstanding Machine Learning: From Theory to Algorithms by Shalev-Shwartz and Ben-David (UML)\nPatterns, Predictions, and Actions by Hardt and Recht (HR)\nFairness and Machine Learning by Barocas, Hardt, and Narayanan (BHN)\nLearning Theory from First Principles by Bach (LTFP)\n\n\n\nSTA 9890 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nThe instructor will provide a formula sheet for use during in-class exams. No alternative resources may be used during these activities.\nStudents may use online resources, such as blog posts, discussion fora, etc. for the Research Reports and Course Competition but Generative AI tools are specifically disallowed.\n\nException: Coding Assistance Tools, e.g., GitHub CoPilot, may be used for the Research Reports and Course Competition. Their usage is limited to coding your ideas. They may not be used as idea generation tools.\n\nThe era of Generative AI is challenging and fast-paced. If you are unsure whether a tool is allowed in this course, please reach out to the instructor before using it. In matters of Academic Integrity, it is far better to ask permission than forgiveness."
  },
  {
    "objectID": "resources.html#aipolicy",
    "href": "resources.html#aipolicy",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Academic Integrity Policy",
    "text": "Academic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nIn this course, expectations for academic integrity for in-class graded activities are straightforward: no use of unauthorized materials on weekly quizzes or mid-semester tests. Unless explicitly stated otherwise by the instructor in writing, the only authorized materials allowed during in class assessment are the instructor-provided formula sheets.\nThe Research Reports are “open peers”, “open book”, “open internet” but you may not use paid services or generative AI services. You must cite all sources used and include an acknowledgement section listing all peers with whom you collaborated. You do not need to acknowledge the instructor or any Piazza discussions, but you must acknowledge other faculty members or other Baruch students who help you complete your reports, even if they are not enrolled in this course.\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity."
  },
  {
    "objectID": "resources.html#course-accomodations",
    "href": "resources.html#course-accomodations",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Course Accomodations",
    "text": "Course Accomodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services (SDS). Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed."
  },
  {
    "objectID": "resources.html#personal-resources2",
    "href": "resources.html#personal-resources2",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Personal Resources1",
    "text": "Personal Resources1\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).2\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\nOn campus, you can also access the Bearcat Food Pantry.\n\n\nFinancial Security\nBaruch students experiencing heightened financial stress have access to Student Emergency Grants administered through the Office of the Dean of Students.\nNote that funds are also available for students experiencing immigration-related financial stress.\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during business hours for more information."
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎"
  },
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "",
    "text": "This course applies multiple regression techniques to the increasingly important study of very large data sets. Those techniques include linear and logistic model fitting, inference, and diagnostics. Methods with special applicability for Big Data will be emphasized, such as lasso and ridge regression. Issues of model complexity, the bias-variance tradeoff, and model validation will be studied in the context of large data sets. Methods that rely less on distributional assumptions are also introduced, including cross-validation, bootstrap resampling, and nonparametric methods. Students will learn dimension reduction methods, correlation analysis, and random forests."
  },
  {
    "objectID": "objectives.html#official-course-description--",
    "href": "objectives.html#official-course-description--",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "",
    "text": "This course applies multiple regression techniques to the increasingly important study of very large data sets. Those techniques include linear and logistic model fitting, inference, and diagnostics. Methods with special applicability for Big Data will be emphasized, such as lasso and ridge regression. Issues of model complexity, the bias-variance tradeoff, and model validation will be studied in the context of large data sets. Methods that rely less on distributional assumptions are also introduced, including cross-validation, bootstrap resampling, and nonparametric methods. Students will learn dimension reduction methods, correlation analysis, and random forests."
  },
  {
    "objectID": "objectives.html#course-learning-objectives",
    "href": "objectives.html#course-learning-objectives",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9890 will be able to:\n\nIdentify Key ML Tasks and Trade-Offs\nUse Regression and Classification Tools to Develop Interpretable and Accurate Predictive Models\nDevelop and Apply Ensemble Learning Strategies\nAccurately Assess Model Performance Using CV, Hold-Out, Stability, and Bootstrap Techniques\nUse Unsupervised Methods to Find Meaningful Structure in Data\nApply Statistical Machine Learning Methods to Novel Data Structures and Types\nGenerate and Communicate Insights via Statistical Machine Learning Methods\n\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\nLearning Goal 6\nLearning Goal 7\n\n\n\n\nTest #01\n✓\n✓\n\n✓\n\n\n\n\n\nResearch Report #01\n✓\n✓\n\n✓\n\n\n\n\n\nTest #02\n✓\n✓\n✓\n✓\n\n\n\n\n\nResearch Report #02\n✓\n✓\n✓\n✓\n\n\n\n\n\nTest #03\n✓\n\n✓\n✓\n✓\n\n\n\n\nResearch Report #03\n✓\n\n✓\n✓\n✓\n\n✓\n\n\nCourse Competition\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\nIn-Class Presentation\n\n\n\n\n✓\n✓\n✓"
  },
  {
    "objectID": "objectives.html#program-learning-goals",
    "href": "objectives.html#program-learning-goals",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course is intended to contribute to the following Program Learning Goals for the MS in Business Analytics:\n\n\n\n Learning Goal\nMS BA Learning Goal\nDescription\n\n\n\n\n\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n✓\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non-technical/lay audiences.\n\n\n✓\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course is intended to contribute to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\n\n\n Learning Goal\nMS QMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g., deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n✓\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course is intended to contribute to the following Program Learning Goals for the MS in Statistics:\n\n\n\n Learning Goal\nMS Statistics Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems form business and the other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n✓\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "STA 9890 - Handouts and Additional Notes",
    "section": "",
    "text": "Supplemental course notes and links to useful external resources will be posted here.\n\nSupplemental Lecture Notes\n\nWeek 1 - Course Overview & Introduction to ML (2025-01-28)\n\nInstructor Notes\n\n\n\nWeek 2 - Regression I (2025-02-04)\n\nInstructor Notes\n\nThe YouTube channel “3Blue1Brown” makes excellent videos explaining mathematical concepts. A recent entry discusses gradient descent in the context of Neural Networks. At this point in the course, our focus is still on simpler (convex) methods, so not all of this will be directly applicable, but it is still a useful summary and gives helpful background on how gradient methods remain at the heart of all modern ML.\n\nYou may also find value in 3Blue1Brown videos on Linear Algebra and on Probability.\nOf these, the following are likely to be particularly useful in this course:\n\nVectors\nLinear Transformations\nMatrix Multiplication\nInverses, Rank, and Null Space\nDot Products and Duality\nEigenvectors and Eigenvalues\nBayes’ Theorem\n\nthough you don’t need to watch all of these immediately.\nIn this course, we will apply calculus techniques (mainly differentiation) to functions \\(\\mathbb{R}^{p} \\to \\mathbb{R}\\). The website matrixcalculus.org/ is helpful for this work.\n\n\nWeek 3 - Regression II (2025-02-11)\n\nInstructor Notes\n\n\n\nWeek 4 - Regression III (2025-02-25)\n\nInstructor Notes\n\n\n\nWeek 5 - Introduction to Classification (2025-03-04)\n\nInstructor Notes\n\n\n\nWeek 6 - Classification I (2025-03-11)\n\nInstructor Notes\n\n\n\nWeek 7 - Classification II (2025-03-18)\n\nInstructor Notes\n\n\n\nWeek 8 - Ensemble Learning & Resampling Methods (2025-03-25)\n\nInstructor Notes\n\n\n\nWeek 9 - Tree-Based Methods (2025-04-01)\n\nInstructor Notes\n\n\n\nWeek 10 - Introduction to Unsupervised Learning (2025-04-08)\n\nInstructor Notes\n\n\n\nWeek 11 - Unsupervised Learning I (2025-04-22)\n\nInstructor Notes\n\n\n\nWeek 12 - Unsupervised Learning II (2025-04-29)\n\nInstructor Notes\n\n\n\nWeek 13 - Introduction to Generative Models (2025-05-06)\n\nInstructor Notes\n\n\n\n\nTests and Solutions\n\nTest 1: Regression\n\nExam Booklet\nSolutions\n\n\n\n\nAdditional Materials\nThe following homework problems and exams were used for the Spring 2024 offering of this course. You may find them useful as you study for the three in-class tests. Note, however, that these were 3-hour take-home cumulative exams, rather than the 1-hour in-class non-cumulative exams used this semester, so the questions are not exactly what you should expect. In particular, note that the exam boundaries do not exactly line up with our schedule this semester.\nSolutions will not be provided, but I am happy to discuss individual problems in office hours or on the course discussion board.\n\nHomeworks\n\nHomework #01\nHomework #02\nHomework #03\nHomework #04\n\nExams\n\nMidterm Exam\nFinal Exam"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "STA 9890 - Research Reports",
    "section": "",
    "text": "In lieu of traditional homework, STA 9890 has three “research reports” (one for each unit of the course). These research reports are intended to help you develop your skills in the computational and methodological aspects of Statistical Machine Learning. Specifically, these reports are designed to help you learn to deal with novel machine learning developments published as academic research papers, to assess whether so-called developments are actually able to do what they claim, to determine whether a proposed method can actually solve the problem you care about and, if appropriate, to call shenanigans on the ‘puffery’ that is pervasive in the ML literature.\nEach Research Report must be submitted as a fully-typed PDF using the course Brightspace. (No handwritten work will be graded.) Each report must include all code used and should have several figures. Reports should be 6-8 pages, double double or single spaced in a legible 10 to 12 point font. You may include appendices for things like code or longer proofs, but the ``main body’’ of your submission must cover the expected material.\n\nResearch Reports\n\nResearch Report #01: Bias and Variance in Linear Regression\nDue Dates:\n\nReleased to Students: 2025-02-04\nSubmission Deadline: 2025-03-07 11:45pm ET\n\nIn Research Report #01, you will dig into the oft-cited claim that Ordinary Least Squares is a Best Linear Unbiased Estimator (BLUE). In classical statistics, the BLUE property is often used as an argument of optimality, implying that we can’t beat OLS, so we shouldn’t even try. As you will see, this optimality of OLS is quite overstated: OLS can be beaten quite easily whenever its assumptions are violated, whenever non-linear estimators are allowed, or whenever bias is permitted (taking “Best” to mean “minimum MSE” instead of “minimum variance”).\nThese findings may seem a bit abstract, but they get at the heart of almost every method and principle we will cover in this course. In this project, in addition to getting a better understanding of what BLUE does and does not mean, you will learn to:\n\nimplement gradient descent methods\ndesign Monte Carlo simulations to assess bias and variance\nfind optimal values of tuning parameters using cross-validation\n\n\n\nResearch Report #02: Ensemble Learning Techniques for Fair Classification\nDue Dates:\n\nReleased to Students: 2025-03-11\nSubmission Deadline: 2025-04-18 11:45pm ET\n\nIn Research Report #02, you will apply some of the tools we have developed to the problem of fairness in machine learning (FairML). While not a core topic for this course, this exercise is useful to see how the core idea of this course–regularization, optimization, etc.–can be applied to interesting and novel questions. In this project, you will also engage critically with a newly proposed ML method and investigate i) whether it truly does what it claims to; ii) whether it can be efficiently and reliably implemented; and iii) the degree to which (if any!) it solves your problem of interest. As working Data Scientists and Business Analysts, you may not think of yourselves as researchers, but knowing how to read and critically evaluate cutting-edge work will let you maintain and enhance your skills throughout your career.\n\n\nResearch Report #03: Sparse Principal Components Analysis\nDue Dates:\n\nReleased to Students: 2025-04-22\nSubmission Deadline: 2025-05-09 11:45pm ET\n\nIn Research Report #03, you will explore sparse PCA and apply it to a data set of interest. As you do so, you will see how a modern machine learning principle (sparsity) can be used to improve a classical statistical technique like PCA to get ‘the best of both worlds.’ Because our focus here is on an unsupervised method, this report should be careful to consider interpretation and validation of the resulting PCs, as standard validation techniques for supervised methods cannot be applied."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 9890 - Statistical Learning for Data Mining",
    "section": "",
    "text": "Welcome to the course website for STA 9890 - Statistical Learning for Data Mining (Spring 2025)!\nSTA 9890 is a survey of Statistical Machine Learning targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives. Official copies of these documents can be found on CUNY Brightspace.\nThis site also hosts descriptions of the Course Competition and Research Reports (Homework), as well as selected Handouts and Additional Notes that will be useful.\nThere are quite a few moving parts to this course, so this key dates file or the list of upcoming course activities below may be useful:\n\n\n\n\n\n\n\nA CSV file suitable for import into Google Calendar with all assignment deadlines can be found here.\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 9890 - Course Syllabus",
    "section": "",
    "text": "All syllabus and course schedule provisions subject to change with suitable advance notice."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA 9890 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎\nThough ubiquitous, email is a remarkably ‘flaky’ service, providing the sender no way to guarantee their message arrives untampered and providing the recipient no way to guarantee the providence of a message received. (This is not quite true: there are tools for more secure email but they are somewhat more difficult to use and are not supported at CUNY.) Brightspace is integrated with CUNY’s Identity Verification Services and allows students to guarantee correct submission. Note that Brightspace does not, by default, send students an email confirming submission, but I believe this is an option that can be enabled on the student’s end.↩︎\nHaoyu Chen and Jiongjiong Yang. “Multiple Exposures Enhance Both Item Memory and Contextual Memory over Time”. Frontiers in Psychology 11. November 2020. DOI:10.3389/fpsyg.2020.565169↩︎\nFor this course, an average student is a student who enters the course with:\n\nFluency with statistical and numerical software at the level of (at least) STA 9750;\nFluency with univariate and multivariate regression at the level of (at least) STA 9700; and\nFamiliarity with probability and linear algebra;\n\nand is earning a B-range grade. If you have less background or are aiming for a higher grade, you should expect to commit proportionally more time to this course.\nIf you lack the prerequisite background listed above or simply wish to review it before the semester begins in earnest, please reach out to the instructor and I will be more than happy to provide supplementary readings.↩︎\nThe CUNY Graduate Center has a useful summary of these expectations. Baruch courses follow the same standards. See also CUNY Central Policy.↩︎\nAs a student, you have free access to GitHub CoPilot once you create a student GitHub account and register for the Student Developer Pack.↩︎\nIn theory, you can work with these frameworks at this sort of “low-level”, but they are likely to be far more cumbersome than using base R or standard numpy for these tasks. If you are interested in working with these frameworks in this manner in order to better familiarize yourself with the core computational abstractions, you may request instructor permission to do so.↩︎"
  },
  {
    "objectID": "reports/report03.html",
    "href": "reports/report03.html",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "",
    "text": "Released to Students: 2025-04-22\nSubmission: 2025-05-09 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report03.html#research-report",
    "href": "reports/report03.html#research-report",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "Research Report #03: Sparse Principal Components Analysis",
    "text": "Research Report #03: Sparse Principal Components Analysis\nIn the final Research Report of this course, you will investigate the intersection of sparsity, a form of interpretability, with unsupervised learning, the branch of ML focused on finding meaningful patterns in data.\nClassically, principal components analysis (PCA) is the cornerstone of dimension reduction methods. By reducing a large number of features to a smaller number of linearly independent (uncorrelated) components, PCA is often said to increase interpretability. This assumes a degree of “reification” - is the PC something meaningful on its own?\nIf the PC captures a true underlying factor, then perhaps we have found something worth interpreting; but PCA always finds some sort of factorization, even when there is no “true” underlying factor. In some circumstances, this PC becomes a thing in itself, e.g., IQ, but that cannot always be assumed. If we do not assume or define a meaning to the PC, it can only be understood as a combination of all of the features used to create it, hardly a paragon of interpretability.\nSparse PCA takes a different approach: rather than finding an interpretation of the linear combination of all features that captures the most variance, it seeks a linear combination of only a few features that explain nearly-the-most variance. Because only a few features are used, the resulting PC is more interpretable a priori than the output of classical PCA.\nAs background, you should read the paper by Weylandt and Swiler1 and review the Supporting Materials to see how Sparse PCA can be applied in a scientific setting. For more methodological background, see the references cited therein, especially the paper by Witten et al., as well as ISL §6.3.1 and §12.2 and SLS §8.1-8.2.2.\nProject Skeleton\nYour report should be divided into three sections, covering the following:\n\nBackground on PCA and Sparse PCA\nComputation - Implementing the Power Method and the Sparse Power Method for PCA\nImplementation and Assessment of Sparse PCA\n\nAt a minimum, these should include the following elements:\n\nBackground on PCA and Sparse PCA\n\nDerivation of PCA from Variance Maximization to Singular Value Decomposition\nProof of Convergence of the Power Method for SVD Computations\nModification of Classical Power Method for Sparsity\n\n\nComputation\n\nImplementation of Classical and Sparsified Power Method\nDiscussion of Convergence for Power Methods2\n\nHow might one tune the sparsity level used?\n\n\nImplementation and Assessment\n\nIn-Simulation: Construct simulations to compare the accuracy of classical and sparse PCA. Which does better when the “true” PCs are dense? When they are sparse? Does this depend on the sample size? How can we measure accuracy of the estimated PCs?\nOn Real Data: Identify a real data set where PCA can be applied. Apply both classical and sparse PCA and compare the results. Which PCA is better (in whatever sense)? Which PCA is more interepretable? Is there a way to validate your interpretation?\n\n\nAdditional Background\nThe Power Method is a classical approach to computing eigenvectors and, by extension, singular vectors. For this research report, you will build upon the singular vector variant, so I review the eigenvector variant here.\nSuppose \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{p \\times p}_{\\succ 0}\\) is a strictly positive definite \\(p \\times p\\) matrix. By the spectral theorem, it has an eigendecomposition:\n\\[\\mathbf{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\]\nwhere the eigenvalues \\(\\{\\lambda_i\\}\\) are decreasing and strictly positive: (\\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\\)). If the eigenvalues are distinct (as they are with probability 1 for continuous-valued data observed noisly), then this decomposition is unique up to sign.3\nThe power method for computing the leading (highest eigenvalue) eigenvector of \\(\\mathbf{\\Sigma}\\) proceeds as follows:\n\nInitialize: select \\(\\mathbf{v}^{(0)}\\) as a random unit vector\nRepeat Until Convergence:\n\nMultiply: \\(\\tilde{\\mathbf{v}}^{(k+1)} = \\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\)\n\nNormalize: \\(\\mathbf{v}^{(k+1)} = \\tilde{\\mathbf{v}}^{(k+1)} / \\|\\tilde{\\mathbf{v}}^{(k+1)}\\|_2\\)\n\nIterate: \\(k \\leftarrow k + 1\\)\n\n\n\nReturn: At convergence, return the eigenvector \\(\\mathbf{v}^{(k)}\\) and the eigenvalue \\(\\lambda = \\|\\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\|_2\\)\n\n\nAdditional eigenvectors can be found by “deflating” \\(\\mathbf{\\Sigma}\\) and applying the power method to \\[\\mathbf{\\Sigma}' = \\mathbf{\\Sigma} - \\lambda \\hat{\\mathbf{v}}\\hat{\\mathbf{v}}^{\\top}.\\]\nSo long as the eigenvalues of \\(\\mathbf{\\Sigma}\\) are distinct, the power method converges to the leading eigenvector as long as the initial guess is not perfectly orthogonal to the true eigenvector. If we pick our initial guess randomly, this is a very reasonable assumption. To see why this is the case, note that:\n\\[\\begin{align*}\n\\mathbf{v}^{(k)} &\\propto \\mathbf{\\Sigma}^k \\mathbf{v}^{(0)} \\\\\n                 &= \\left(\\sum_{i=1}^p \\lambda_i^k \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\right)\\mathbf{v}^{(0)} \\\\\n                 &= \\sum_{i=1}^p \\lambda_i^k \\langle\\mathbf{v}_i, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_i\n\\end{align*}\\]\nViewed here, you can see why the power method has its name. As \\(k \\to \\infty\\), the first term in this series becomes much much larger than the others, so we get\n\\[\\mathbf{v}^{(k)} \\buildrel \\sim \\over \\propto \\lambda_1^k \\langle\\mathbf{v}_1, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_1\\] After normalizing and dropping the scalar terms, we have\n\\[\\mathbf{v}^{(k)} \\to \\mathbf{v}_1\\]\nas desired. This argument also clarifies the two possible failure modes of the power method:\n\nIf the initial guess is unlucky and orthogonal to the true eigenvector, we have \\(\\langle \\mathbf{v}_1, \\mathbf{v}^{(0)} \\rangle = 0\\) so the \\(\\mathbf{v}_1\\) term drops out of the answer and we converge to the next eigenvector (unless we are orthogonal to that one as well).\nIf the eigenvalues are not distinct, then multiple terms become large and we get a strange ‘superposition’ of multiple eigenvectors. (Alternatively, there isn’t a ‘true’ right answer, but we get one possible eigenvector.)\nPotential Topics for Additional Research\nWhile the main focus of this report is on Sparse PCA, a suitably sparsified SVD can be used for a wide variety of multivariate analysis problems. You may wish to extend your analysis to other sparse multivariate methods, such as Sparse CCA or Sparse PLS.\nAlternatively, a huge number of approaches to Sparse PCA have been proposed, not all of which are based on the power method. You may wish to compare the approach I prefer (projected power method) with other approaches: be sure to consider i) accuracy; ii) interpretability; and iii) computational ease / efficiency in your comparison.\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.4 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report03.html#footnotes",
    "href": "reports/report03.html#footnotes",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nM. Weylandt and L.P. Swiler. “Beyond PCA: Additional Dimension Reduction Techniques to Consider in the Development of Climate Fingerprints”. Journal of Climate 37(5), p.1723-1735. 2024. DOI: 10.1175/JCLI-D-23-0267.1. Direct Link↩︎\nRecall that PCs are only defined “up to sign”. If \\(\\hat{\\mathbf{u}}\\) is a valid PC, so is \\(-\\hat{\\mathbf{u}}\\). You will need to modify your usual convergence checks to account for this.↩︎\nNote that we can substitute \\(\\mathbf{v}_i \\to -\\mathbf{v}_i\\) without changing the result since the minus signs will cancel in \\((-\\mathbf{v}_i)(-\\mathbf{v}_i)^{\\top} = \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\).↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "reports/report01.html",
    "href": "reports/report01.html",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "",
    "text": "Released to Students: 2025-02-04\nSubmission: 2025-03-07 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report01.html#research-report",
    "href": "reports/report01.html#research-report",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "Research Report #01: Bias and Variance in Linear Regression",
    "text": "Research Report #01: Bias and Variance in Linear Regression\nIn this research report, you will dive deeply into the bias-variance trade-off and the BLUE-ness (or lack thereof) of ordinary least squares.\nProject Skeleton\nYour report should be divided into four sections, covering the following:\n\nTheoretical background\nComputation - Gradient Descent and Weight Decay\nBias and Variance Under Linear Data Generating Processes (DGP)\nBias and Variance Under Non-Linear DGP\n\nAt a minimum, these should include the following elements:\n\nTheoretical Background\n\n\nRigorous Statement and Proof of the Bias-Variance Decomposition\n\\[\\mathbb{E}[\\text{MSE}] = \\text{Bias}^2 + \\text{Variance} (+ \\text{Irreducible Noise})\\]\nDiscuss how this statement is to be interpreted in the context of parameter estimation (when the underlying DGP is linear) and in the context of (possibly misspecified) prediction. Be sure to clarify when the “Irreducible Noise” term is needed.\n\nProof of the “BLUE” property of OLS with clear statement of the relevant assumptions. Take care to differentiate which assumptions are required for “B” and which are required for “U”.\n\n\nComputation\n\nDerivation and implementation of the ‘closed-form’ matrix expression for OLS\nDerivation and implementation of the ‘closed-form’ matrix expression for Ridge Regression\nDerivation and implementation of a gradient descent method for OLS\nDerivation and implementation of a ‘weight decay’ modification for OLS gradient descent.\nEmpirical demonstration of the equivalance of OLS+Weight Decay with Ridge Regression\n\n\nBias and Variance Under Linear DGP\n\nPosit a Linear DGP\nUsing Monte Carlo simulations:\n\nShow that OLS is unbiased under this DGP\nCompute the variance of a given regression coefficient\nCompute the in- and out-of-sample prediction MSE\nShow how the variance, in-sample, and out-of-sample MSE change with the sample size, \\(n\\)\n\n\n\nCompare against ridge regression\n\nShow that RR is biased under this DGP\nCompare the variance of OLS and RR\nDemonstrate the MSE Existence Theorem for Ridge Regression\nCompare the MSE of RR and OLS as a function of the sample size \\(n\\)\n\n\n\n\n\nBias and Variance Under Non-Linear DGP\n\nPosit a Non-Linear DGP\nUsing simulation, determine the “best approximate” linear regression coefficients: that is, find the OLS coefficients minimizing MSE\nShow that the estimated OLS coefficients converge to the “best approximate” coefficients as the sample size \\(n\\) increases\nCompute the MSE of RR in simulation and then use 5-fold cross-validation to determine the optimal regularization level\nCompare the MSE of RR and OLS in the non-linear setting\n\n\n\nFinally, note that because we are interested in bias, variance, and MSE – each of which is defined with respect to a (known) DGP – we only use statistical simulation for this project. In Report #02 and Report #03, you will apply methods to real (non-simulated) data.\nAdditional Background\nGradient Descent methods fit models by taking small steps in the direction opposite the gradient of the loss function. (Recall the gradient points in the direction of steepest increase, so moving in the opposite direction leads to a decrease in error.)\nAbstractly, given a differentiable loss function, \\(\\mathcal{L}(\\beta)\\), gradient descent proceeds as follows:\n\nSelect initial guess \\(\\beta^{(0)}\\) and set \\(k=0\\)\n\nRepeat until convergence:\n\n\nCompute gradient \\(\\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}\\)\n\nUpdate guess using a gradient step \\[\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}\\]\n\nIncrement \\(k := k+1\\)\n\nCheck convergence:\n\nParameter convergence: if \\(\\beta^{(k+1)} \\approx \\beta^{(k)}\\), we have converged.\nObjective convergence: if \\(\\mathcal{L}(\\beta^{(k+1)}) \\approx \\mathcal{L}(\\beta^{(k)})\\), we have converged\n\n\n\n\nAt convergence, return \\(\\beta^{(k+1)}\\)\n\n\nThis scheme is pretty easy to implement, but there are a few points where it behooves you to be careful:\n\nHow do we check the approximate equality in the convergence check?\nWe will almost never get exact equality, so you need to pick a norm and a tolerance.\nHow do we choose the step-size \\(c\\)? Much has been written about this; for our purposes it suffices to take a very small value of \\(c\\) and keep it constant.\nWhat if we never reach convergence? Should we stop after some (large) number of iterations?\n\nThe method of weight-decay modifies the gradient update as follows:\n\\[\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}} - \\omega \\beta^{(k)}\\]\nHere \\(\\omega\\) is another small constant scalar value. By subtracting off a fraction of \\(\\beta^{(k)}\\) at each step, weight decay prevents the estimate from ever getting to be too big (hence the name). In the case of OLS, this can be shown to be equivalent to a form of ridge regression.1\nThe MSE Existence Theorem for Ridge Regression comes in two forms:\n\n\nEstimation Variant. Let \\[\\text{MSE}_i(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}}[(\\hat{\\beta_i} - \\beta_i^*)^2]\\] denote the mean squared estimation error in the \\(i^\\text{th}\\) component of \\(\\hat{\\beta}\\) as an estimator of the true regression coefficients \\(\\beta^*\\). Here the expectation is taken over random realizations of both the design matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{y}\\), as well as the estimator \\(\\hat{\\beta}\\) which is a function of \\(\\mathbf{X}, \\mathbf{y}\\).\nThen, there exists some positive value \\(\\lambda &gt; 0\\) such that\n\\[\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) &gt; \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})\\]\nwhere \\(\\hat{\\beta}_{\\text{RR}, \\lambda}\\) is the ridge regression estimate with regularization parameter \\(\\lambda\\).\n\n\nPrediction Variant. Let \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}, \\mathbf{y}'}[\\|\\mathbf{X}\\hat{\\beta} - \\mathbf{y}'\\|_2^2\\] denote the mean squared prediction error associated with the estimator \\(\\hat{\\beta}\\). Here the expectation is taken over random realizations of both the design matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{y}\\) used for training, as well as the test data vector \\(\\mathbf{y}'\\) which must be drawn from the same DGP.\nThen, there exists some positive value \\(\\lambda &gt; 0\\) such that\n\\[\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) &gt; \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})\\]\nwhere \\(\\hat{\\beta}_{\\text{RR}, \\lambda}\\) is the ridge regression estimate with regularization parameter \\(\\lambda\\).\n\n\nIt is important to note that both of these are statements in expectation: OLS may perform better than RR on a single realization, but in average - over many realizations - suitably-tuned RR will perform better.\nAlso, we note that the MSE Existence Theorem does not give any practical advice on selecting \\(\\lambda\\); we have to fall back on our standard approaches, such as cross-validation, for doing so.\nFinally, note that the prediction version of the MSE Existence Theorem only holds when we look at test (out-of-sample) prediction accuracy. OLS is the optimal linear method for in-sample MSE always and forever.\nPossible Topic(s) for Additional Research\nRecent ML research has focused on the case of overparameterized models: that is, models with more parameters than data points used for training. We have already seen ridge regression as one way to deal with this problem. Another approach is to simply use an iterative method, such as gradient descent, and stop it when the training error reaches zero. (This is a non-unique global minimum for the convex OLS problem.) How does this procedure compare? Does it complicate our understanding of the bias-variance trade-off?\nAnother recent line of research generalizes the “BLUE” property, showing that OLS is in an appropriate sense “BUE” - it is still optimal even if we allow for (certain types of) non-linear estimators as well. Can you show this?\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.2 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report01.html#footnotes",
    "href": "reports/report01.html#footnotes",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nA somewhat remarkable finding of much recent research is that many ‘new’ regularization methods pioneered for fitting neural networks have essentially the same effect as ridge (\\(\\ell_2^2\\) or Tikhonov) regularization: weight decay, drop out, early stopping, mini-batching and mini-patching. Sometimes it’s hard to beat the classics.↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "notes/notes01.html",
    "href": "notes/notes01.html",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "",
    "text": "Where classical statistics focuses on learning information about a population from a (representative) sample, Machine Learning focuses on out-of-sample prediction accuracy.\nFor example, given a large set of medical records, the natural instinct of a statistician is to find the indicators of cancer in order assess them via some sort of follow-on genetic study, while a ML practitioner will typically start by building a predictive algorithm to predict who will be diagnosed with cancer. The statistician will perform calculations with \\(p\\)-values, test statistics, and the like to make sure that any discovered relationship is accurate, while the ML practitioner will verify the performance by finding a new (hopefully similar) set of medical records to test algorithm performance.\nClearly, this difference is more one of style than substance: the statistician might see what features are important in the ML model to decide what to investigate, while the ML modeler will use statistical tools to make sure the model is finding something real and not just fitting to noise. In this course, the distinction may be even blurrier as our focus is statistical machine learning - that little niche right on the boundary between the two fields.\nIn brief, “statistics vs ML” is a bit of a meaningless distinction as both fields draw heavily from each other. I tend to say one is doing statistics whenever the end-goal is to better understand something about the real world (ie., the end product is knowledge), while one is doing ML whenever one is building a system to be used in an automated fashion (ie., the end product is software), but definitions vary.1\nIn this course we will use the following taxonomy borrowed from the ML literature:\n\nSupervised Learning: Tasks with a well-defined target variable (output) that we aim to predict\nExamples:\n\nGiven an image of a car running a red-light, read its license plate.\nGiven attendance records, predict which students will not pass a course.\nGiven a cancer patient’s medical records, predict whether a certain drug will have a beneficial impact on their health outcomes.\n\nUnsupervised Learning: Given a whole set of variables, none of which is considered an output, learn useful underling structure.\nExamples:\n\nGiven a set of climate simulations, identify the general trends of a climate intervention.\nGiven a set of students’ class schedules, group them into sets. (You might imagine these sets correspond to majors, but without that sort of “label” (output variable) this is only speculative, so we’re unsupervised)\nGiven a social network, identify the most influential users.\n\n\nThere are other types of learning tasks: e.g. semi-supervised, online, reinforcement, but the Supervised/Unsupervised distinction is the main one we will use in this course.\nWithin Supervised Learning, we can further subdivide into two major categories:\n\nRegression Problems: problems where the response (label) is a real-valued number\nClassification Problems: problems where the response (label) is a category label.\n\nBinary classification: there are only two categories\nMulti-way or Multinomial classification: multiple categories\n\n\nLinear Regression, which we will study more below, is the canonical example of a regression tool for supervised learning.\nAt this point, you can already foresee one of the (many) terminology inconsistencies will will encounter in this course: logistic regression is a tool for classification, not regression. As modern ML is the intersection of many distinct intellectual traditions, the terminology is rarely consistent.2"
  },
  {
    "objectID": "notes/notes01.html#a-taxonomy-of-machine-learning",
    "href": "notes/notes01.html#a-taxonomy-of-machine-learning",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "",
    "text": "Where classical statistics focuses on learning information about a population from a (representative) sample, Machine Learning focuses on out-of-sample prediction accuracy.\nFor example, given a large set of medical records, the natural instinct of a statistician is to find the indicators of cancer in order assess them via some sort of follow-on genetic study, while a ML practitioner will typically start by building a predictive algorithm to predict who will be diagnosed with cancer. The statistician will perform calculations with \\(p\\)-values, test statistics, and the like to make sure that any discovered relationship is accurate, while the ML practitioner will verify the performance by finding a new (hopefully similar) set of medical records to test algorithm performance.\nClearly, this difference is more one of style than substance: the statistician might see what features are important in the ML model to decide what to investigate, while the ML modeler will use statistical tools to make sure the model is finding something real and not just fitting to noise. In this course, the distinction may be even blurrier as our focus is statistical machine learning - that little niche right on the boundary between the two fields.\nIn brief, “statistics vs ML” is a bit of a meaningless distinction as both fields draw heavily from each other. I tend to say one is doing statistics whenever the end-goal is to better understand something about the real world (ie., the end product is knowledge), while one is doing ML whenever one is building a system to be used in an automated fashion (ie., the end product is software), but definitions vary.1\nIn this course we will use the following taxonomy borrowed from the ML literature:\n\nSupervised Learning: Tasks with a well-defined target variable (output) that we aim to predict\nExamples:\n\nGiven an image of a car running a red-light, read its license plate.\nGiven attendance records, predict which students will not pass a course.\nGiven a cancer patient’s medical records, predict whether a certain drug will have a beneficial impact on their health outcomes.\n\nUnsupervised Learning: Given a whole set of variables, none of which is considered an output, learn useful underling structure.\nExamples:\n\nGiven a set of climate simulations, identify the general trends of a climate intervention.\nGiven a set of students’ class schedules, group them into sets. (You might imagine these sets correspond to majors, but without that sort of “label” (output variable) this is only speculative, so we’re unsupervised)\nGiven a social network, identify the most influential users.\n\n\nThere are other types of learning tasks: e.g. semi-supervised, online, reinforcement, but the Supervised/Unsupervised distinction is the main one we will use in this course.\nWithin Supervised Learning, we can further subdivide into two major categories:\n\nRegression Problems: problems where the response (label) is a real-valued number\nClassification Problems: problems where the response (label) is a category label.\n\nBinary classification: there are only two categories\nMulti-way or Multinomial classification: multiple categories\n\n\nLinear Regression, which we will study more below, is the canonical example of a regression tool for supervised learning.\nAt this point, you can already foresee one of the (many) terminology inconsistencies will will encounter in this course: logistic regression is a tool for classification, not regression. As modern ML is the intersection of many distinct intellectual traditions, the terminology is rarely consistent.2"
  },
  {
    "objectID": "notes/notes01.html#footnotes",
    "href": "notes/notes01.html#footnotes",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some cultural differences that come from ML’s CS history vs Statistics’ “Science-Support” history: notably, ML folks think of (binary) classification while statisticians think of regression as the first task to teach.↩︎\nYou may recall the famous quip about the US and the UK: “Two countries, separated by a common language.”↩︎\nYou might still worry about the randomness of this comparison process: if we had a slightly different sample of our new data, would the better model still look better? You aren’t wrong to worry about this, but we have at a minimum made the problem much easier: now we are just comparing the means of two error distributions - classic \\(t\\)-test stuff - as opposed to comparing two (potentially complex) models.↩︎\nAnecdotally, a Google Data Scientist once mentioned to me that they rarely bother doing \\(p\\)-value or significance calculations. At the scale of Google’s A/B testing on hundreds of billions of ad impressions, everything is statistically significant.↩︎\nI’m being a bit sloppy here. This is more precisely the population test error, not the test set test error.↩︎\nThere are details here about how we measure similarity, but for now we will restrict our attention to simple Euclidean distance.↩︎\nKNN is a non-parameteric method, but not all non-parametric methods require access to the training data at test time. We’ll cover some of those later in the course.↩︎\nIt is easier to understand this ‘in reverse’: as \\(K \\downarrow 1\\), training error decreases to 0, so as \\(K \\uparrow n\\), training error increases.↩︎\nIn OLS, the complexity doesn’t make the model ‘wigglier’ in a normal sense - it’s still linear after all - but you can think of it as the additional complexity of a 3D ‘map’ (i.e., a to-scale model) vs a standard 2D map.↩︎\nFigures from HR Chapter 6, Generalization.↩︎\nFor details, see O. Bousquet and A. Elisseeff, Stability and Generalization in Journal of Machine Learning Research 2, pp.499-526.↩︎\nSome regression textbooks advocate a rule that you have 30 data points for every variable in your model. This is essentially guaranteeing that the \\(p/n\\) ratio that controls the generalization of OLS (see above!) stays quite small.↩︎"
  },
  {
    "objectID": "notes/notes01.html#test-and-training-error",
    "href": "notes/notes01.html#test-and-training-error",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Test and Training Error",
    "text": "Test and Training Error\nAs we think about measuring a model’s predictive performance, it becomes increasingly important to distinguish between in-sample and out-of-sample performance, also called training (in-sample) and testing (out-of-sample) performance.\nIn previous courses, you likely have assessed model fit by seeing how well your model fits the data it was trained on: statistics like \\(R^2\\), SSE, SSR in regression or \\(F\\)-tests for model comparison do just this. As you have used them, they are primarily useful for comparison of similar models, e.g., OLS with different numbers of predictor variables. But it’s worth reflecting on this process a bit more: didn’t the model with more predictors always fit the data better? If so, why don’t we always just include all of our predictors?\nOf course you know, the answer is that we want to avoid overfitting. Just because a model fit the data a bit better doesn’t mean it is actually better. If you need 1,000 variables to get a 0.1% reduction in MSE, do you really believe those features are doing much? No!\nYou likely have a sense that a feature needs to “earn its keep” to be worth including in a model. Statisticians have formalized this idea very well in some contexts: quantities like degrees of freedom or adjusted \\(R^2\\) attemps to measure whether a variable provides a statistically significant improvement in performance. These calculations typically rely on subtle calculations involving nice properties of the multivariate normal distribution and ordinary least squares, or things that can be (asymptotically) considered essentially equivalent.\nIn this class, we don’t want to make those sorts of strong distributional and modeling assumptions. So what can we do instead? Well, if we want to see if Model A predicts more accurately than Model B on new data, why don’t we just do that? Let’s get some new data and compare the MSEs of Model A and Model B: whichever one does better is the one that does better.3\nThis is a pretty obvious idea, so it’s worth asking why it’s not the baseline and why statisticians bothered with all the degrees of freedom business to start with. As always, you have to know your history: statistics comes from a lineage of scientific experimentation where data is limited and often quite expensive to get. If you are running a medical trial, you can’t just toss a few hundred extra participants in - this costs money! If you are doing an agricultural experiment, it may take several years to see whether a new seed type actually has higher yield than the previous version. It’s also not clear how one should separate data into training and test sets: if you are studying, e.g., friendship dynamics on Facebook, you don’t have an (obvious) “second Facebook” that you can use to assess model accuracy.\nBy contrast, CS-tradition Machine Learning comes from a world of “internet-scale” where data is plentiful, cheap, and is continuously being collected.4 Not all problems fall in this regime but, as we will see, enough do that it’s worth thinking about what we should do in this scenario. Excitingly, if we don’t demand a full and exhaustive mathematical characterization of a method before we actually apply it, we can begin to explore much more complex and interesting models.\n\n\n\n\n\n\nAdvice\n\n\n\nA good rule of thumb for applied statistical and data science work: begin by asking yourself what you would do if you had access to the whole population (or an infinitely large sample) and then adapt that answer to the limited data you actually have. You always want to make sure you are asking the right question, even if you are only able to give an approximate finite-data answer, rather than giving an ‘optimal’ answer to a question you don’t actually care about.\n\n\nSo, for the first two units of this course, we will put this idea front and center: we will fit our models to a training set and then see how well they perform on a test set. Our goal is to not to find find models which perform well on the test set per se: we really want to find models that perform well on the all future data, not just one test set. But this training/test split will certainly get us going in the right direction.\nLooking ahead, let’s note some of the key questions we will come back to again and again:\n\nIf we don’t have an explicit test set, where can we get one? Can we ‘fake it to make it’?\nWhat types of models have small “test-training” gap, i.e. do about as well on the test and training sets, and what models have a large gap?"
  },
  {
    "objectID": "notes/notes01.html#model-complexity",
    "href": "notes/notes01.html#model-complexity",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Model Complexity",
    "text": "Model Complexity"
  },
  {
    "objectID": "notes/notes01.html#nearest-neighbor-methods",
    "href": "notes/notes01.html#nearest-neighbor-methods",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Nearest Neighbor Methods",
    "text": "Nearest Neighbor Methods\nLet’s now see how complexity plays out for a very simple classifier, \\(K\\)-Nearest Neighbors (KNN). KNN formalizes the intuition of “similar inputs -&gt; similar outputs.” KNN looks at the \\(K\\) most similar points in its training data (“nearest neighbors” if you were to plot the data) and takes the average label to make its prediction.6\n\nlibrary(class) # Provides a KNN function for classification\nargs(knn)\n\nfunction (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) \nNULL\n\n\nYou can see here that KNN requires access to the full training set at prediction time: this is different than something like OLS where we reduce our data to a set of regression coefficients (parameters).7\nWe’ll also need some data to play with. For now, we’ll use synthetic data:\n\n#' Make two interleaving half-circles\n#'\n#' @param n_samples Number of points (will be divided equally among the circles)\n#' @param shuffle Whether to randomize the sequence\n#' @param noise Standard deviation of Gaussian noise applied to point positions\n#'\n#' @description Imitation of the Python \\code{sklearn.datasets.make_moons} function.\n#' @return a \\code{list} containining \\code{samples}, a matrix of points, and \\code{labels}, which identifies the circle from which each point came.\n#' @export\nmake_moons &lt;- function(n_samples=100, shuffle=TRUE, noise=0.25) {\n  n_samples_out = trunc(n_samples / 2)\n  n_samples_in = n_samples - n_samples_out\n  \n  points &lt;- matrix( c(\n    cos(seq(from=0, to=pi, length.out=n_samples_out)),  # Outer circle x\n    1 - cos(seq(from=0, to=pi, length.out=n_samples_in)), # Inner circle x\n    sin(seq(from=0, to=pi, length.out=n_samples_out)), # Outer circle y\n    1 - sin(seq(from=0, to=pi, length.out=n_samples_in)) - 0.5 # Inner circle y \n  ), ncol=2) \n  \n  if (!is.na(noise)) points &lt;- points + rnorm(length(points), sd=noise)\n  \n  labels &lt;- c(rep(1, n_samples_out), rep(2, n_samples_in))\n  \n  if (!shuffle) {\n    list(\n      samples=points, \n      labels=labels\n    )\n  } else {\n    order &lt;- sample(x = n_samples, size = n_samples, replace = F)\n    list(\n      samples=points[order,],\n      labels=as.factor(ifelse(labels[order] == 1, \"A\", \"B\"))\n    )\n  }\n}\n\nThis function comes from the clusteringdatasets R package, but the underlying idea comes from a function in sklearn, a popular Python ML library.\nLet’s take a look at this sort of data:\n\nTRAINING_DATA &lt;- make_moons()\n\n\nlibrary(ggplot2)\nlibrary(tidyverse)\ndata.frame(TRAINING_DATA$samples, \n           labels=TRAINING_DATA$labels) |&gt;\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=3)\n\n\n\n\n\n\n\n\nWe can make this a bit more attractive:\n\nMY_THEME &lt;- theme_bw(base_size=20) + theme(legend.position=\"bottom\")\ntheme_set(MY_THEME)\n\n\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %&gt;%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=2) + \n  ggtitle(\"Sample Realization of the Moons Dataset\")\n\n\n\n\n\n\n\n\nMuch better!\nLet’s try making a simple prediction at the point (0, 0):\n\nknn(TRAINING_DATA$samples, \n    cl=TRAINING_DATA$labels, \n    test=data.frame(X1=0, X2=0), k=3)\n\n[1] B\nLevels: A B\n\n\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nDoes this match what you expect from the plot above? Why or why not? The following image might help:\n\nlibrary(ggforce)\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %&gt;%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point() + \n  ggtitle(\"Sample Realization of the Moons Dataset\") + \n  geom_circle(aes(x0=0, y0=0, r=0.2), linetype=2, color=\"red4\")\n\n\n\n\n\n\n\n\nWhy is R returning a factor response here? What does that tell us about the type of ML we are doing?\n\n\n\nWe can also visualize the output of KNN at every point in space:\n\nvisualize_knn_boundaries &lt;- function(training_data, k=NULL){\n    xrng &lt;- c(min(training_data$samples[,1]), max(training_data$samples[,1]))\n    yrng &lt;- c(min(training_data$samples[,2]), max(training_data$samples[,2]))\n    \n    xtest &lt;- seq(xrng[1], xrng[2], length.out=101)\n    ytest &lt;- seq(yrng[1], yrng[2], length.out=101)\n    \n    test_grid &lt;- expand.grid(xtest, ytest)\n    colnames(test_grid) &lt;- c(\"X1\", \"X2\")\n    \n    pred_labels = knn(training_data$samples, \n                      cl=training_data$labels, \n                      test_grid, \n                      k=k)\n    \n  ggplot() + \n  geom_point(data=data.frame(TRAINING_DATA$samples, \n                             labels=TRAINING_DATA$labels), \n             aes(x=X1, y=X2, color=labels), \n             size=3) + \n  geom_point(data=data.frame(test_grid, pred_labels=pred_labels), \n             aes(x=X1, y=X2, color=pred_labels), \n             size=0.5) + \n  ggtitle(paste0(\"KNN Prediction Boundaries with K=\", k))\n}\n\nvisualize_knn_boundaries(TRAINING_DATA, k=1)\n\n\n\n\n\n\n\n\nIf we raise \\(K\\), we get smoother boundaries:\n\nvisualize_knn_boundaries(TRAINING_DATA, k=5)\n\n\n\n\n\n\n\n\nAnd if we go all the way to \\(K\\) near to the size of the training data, we get very boring boundaries indeed:\n\nvisualize_knn_boundaries(TRAINING_DATA, k=NROW(TRAINING_DATA$samples)-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhat does this tell us about the complexity of KNN as a function of \\(K\\)?\n\n\n\nIn the terminology we introduced above, we see that increasing \\(K\\) decreases model complexity (wiggliness).\nLet’s now see how training error differs as we change \\(K\\):\n\nTEST_DATA &lt;- make_moons()\n\nTRAINING_ERRORS &lt;- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train &lt;- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TRAINING_DATA$samples, k=k)\n    true_labels_train &lt;- TRAINING_DATA$labels\n    err &lt;- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the training (in-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TRAINING_ERRORS &lt;- rbind(TRAINING_ERRORS, data.frame(k=k, training_error=err))\n}\n\nAt k = 1, the training (in-sample) error of KNN is 0%\nAt k = 2, the training (in-sample) error of KNN is 9%\nAt k = 3, the training (in-sample) error of KNN is 6%\nAt k = 4, the training (in-sample) error of KNN is 4%\nAt k = 5, the training (in-sample) error of KNN is 5%\nAt k = 6, the training (in-sample) error of KNN is 9%\nAt k = 7, the training (in-sample) error of KNN is 5%\nAt k = 8, the training (in-sample) error of KNN is 6%\nAt k = 9, the training (in-sample) error of KNN is 7%\nAt k = 10, the training (in-sample) error of KNN is 4%\nAt k = 11, the training (in-sample) error of KNN is 6%\nAt k = 12, the training (in-sample) error of KNN is 5%\nAt k = 13, the training (in-sample) error of KNN is 6%\nAt k = 14, the training (in-sample) error of KNN is 6%\nAt k = 15, the training (in-sample) error of KNN is 7%\nAt k = 16, the training (in-sample) error of KNN is 9%\nAt k = 17, the training (in-sample) error of KNN is 8%\nAt k = 18, the training (in-sample) error of KNN is 8%\nAt k = 19, the training (in-sample) error of KNN is 9%\nAt k = 20, the training (in-sample) error of KNN is 8%\n\nggplot(TRAINING_ERRORS, aes(x=k, y=training_error)) + \n    geom_point() + \n    ggtitle(\"Training (In-Sample) Error of KNN\")\n\n\n\n\n\n\n\n\nCompare this to the test error:\n\nTESTING_ERRORS &lt;- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train &lt;- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TEST_DATA$samples, k=k)\n    true_labels_train &lt;- TEST_DATA$labels\n    err &lt;- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the test (out-of-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TESTING_ERRORS &lt;- rbind(TESTING_ERRORS, data.frame(k=k, test_error=err))\n}\n\nAt k = 1, the test (out-of-sample) error of KNN is 9%\nAt k = 2, the test (out-of-sample) error of KNN is 10%\nAt k = 3, the test (out-of-sample) error of KNN is 8%\nAt k = 4, the test (out-of-sample) error of KNN is 6%\nAt k = 5, the test (out-of-sample) error of KNN is 8%\nAt k = 6, the test (out-of-sample) error of KNN is 7%\nAt k = 7, the test (out-of-sample) error of KNN is 7%\nAt k = 8, the test (out-of-sample) error of KNN is 6%\nAt k = 9, the test (out-of-sample) error of KNN is 5%\nAt k = 10, the test (out-of-sample) error of KNN is 5%\nAt k = 11, the test (out-of-sample) error of KNN is 5%\nAt k = 12, the test (out-of-sample) error of KNN is 5%\nAt k = 13, the test (out-of-sample) error of KNN is 5%\nAt k = 14, the test (out-of-sample) error of KNN is 5%\nAt k = 15, the test (out-of-sample) error of KNN is 5%\nAt k = 16, the test (out-of-sample) error of KNN is 6%\nAt k = 17, the test (out-of-sample) error of KNN is 5%\nAt k = 18, the test (out-of-sample) error of KNN is 5%\nAt k = 19, the test (out-of-sample) error of KNN is 5%\nAt k = 20, the test (out-of-sample) error of KNN is 6%\n\nggplot(TESTING_ERRORS, aes(x=k, y=test_error)) + \n    geom_point() + \n    ggtitle(\"Test (Out-of-Sample) Error of KNN\")\n\n\n\n\n\n\n\n\nThe difference between the two is clearer if we put them on the same figure:\n\nERRS &lt;- inner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |&gt;\n        pivot_longer(-k) |&gt;\n        rename(Error=value, Type=name)\nggplot(ERRS, aes(x=k, y=Error, color=Type)) + \n  geom_point() + geom_line()\n\n\n\n\n\n\n\n\nWe notice a few things here:\n\nTraining Error increases in \\(K\\)8, with 0 training error at \\(K=1\\). (Why?)\nTest Error is basically always higher than test error\nThe best training error does not have the best test error\n\nWe can also look at the gap between training and test error: this is called generalization error or optimism:\n\ninner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |&gt;\n    mutate(optimism=test_error - training_error) |&gt;\n    ggplot(aes(x=k, y=optimism)) + \n    geom_point() + \n    geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nConsider the following questions:\n\nWhat is the relationship between optimism and model complexity\nWhat is the best value of \\(K\\) for this data set?\nHow should we pick the best value of \\(K\\)?\nHow might that change if we increase the number of training samples?\nHow might that change if we increase the number of test samples?"
  },
  {
    "objectID": "notes/notes01.html#generalization-and-model-complexity",
    "href": "notes/notes01.html#generalization-and-model-complexity",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Generalization and Model Complexity",
    "text": "Generalization and Model Complexity\nSo far, we have two useful concepts:\n\nTraining Error\nTest Error\n\nwith a third we can set up a trivial, but surprisingly useful, inequality:\n\\[\\text{Test Error} = \\text{Training Error} + \\text{Generalization Gap}\\]\nHere, the “Generalization Gap” is defined as the difference between the training error and the test error.5 Essentially, the Generalization Gap measures the “optimism” bias obtained by measuring accuracy on the original training data. If the Generalization Gap is large, the model will look much better on the training data then when we actually go to put it into practice. Conversely, if the Generalization Gap is small, the performance we estimate from the training data will continue when we deploy our model.\nThis is a all a bit circular, but it lets us break our overarching goal (small test error) into two parts:\n\nWe want a model with small training error; and\nWe want a model with small generalization gap.\n\nWe can only be confident that we’ll have a small test error when these both of these are true. Again - and just to be clear - having a small training error is important, but it is only necessary and not sufficient to have a small test error.\nWe can simply observe training error, so much of our theoretical analysis focuses on understanding the generalization gap. For now, let’s think about the generalization gap of plain linear regression (OLS). It is not hard to show (and we might show in class next week) that, if the OLS model is true, the expected training MSE is:\n\\[\\mathbb{E}[\\text{Training MSE}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n(y_i - \\sum_{j=1}^p x_{ij}\\hat{\\beta}_j)^2\\right] = \\frac{\\sigma^2(n-p)}{n} = \\sigma^2\\left(1-\\frac{p}{n}\\right)\\]\nHere \\(\\sigma^2\\) is the ‘noise variance’ of the OLS model. (We will review the OLS model in much more detail next week.)\nThis is somewhat remarkable: if we knew the exact true model \\(\\beta_*\\), our MSE would be \\(\\sigma^2\\), but our training MSE is less than that. How can we do better than the optimal and exactly correct model? Overfitting - our OLS fits our training data a bit ‘too well’ and it manages to capture the true signal and the noise. Whatever noise is in the data doesn’t carry into the test set, so we get a bit of overfitting.\nEven a simple model like OLS is vulnerable to a bit of overfitting. It isn’t too big in this context and our formula above actually lets us see how it behaves:\n\nAs \\(n\\to\\infty\\), the amount of overfitting goes down. This is of course the behavior we want: as we get more data, we should stop fitting noise and only fit signal.\nAs \\(p \\to n\\), the amount of overfitting goes up. That is, as we add more features (covariates) to our regression, we expect more over fitting. When we supply OLS with more ‘degrees of freedom’, some of those wind up fitting noise.\n\nThe \\(n\\to\\infty\\) behavior shouldn’t surprise you: the theme of “more data yields better estimation and smaller error” is ubiquitous in statistics. The behavior as \\(p\\) increases may not be something you have seen before. Later in this course, we will actually ask what happens if \\(p &gt; n\\). Clearly, our formula from above can’t hold as it predicts negative MSE! But we’ll get to that later…\nAs \\(p\\) gets larger, OLS is more prone to overfitting. It turns out that this is not a special property of OLS - basically all methods will have this property to one degree or another. While there are many ways to justify this, perhaps the simplest is a story about “complexity”: with more features, and hence more coefficients, OLS becomes a more complex model and more able to fit both the signal and the noise in the training data. Clearly, complexity is not necessarily bad - we want to be able to capture all of the signal in our data - but it is dangerous.\nThis complexity story is one we will follow through the rest of this course. A more complex model is one which is able to fit its training data easily. Mathematically, we actually measure complexity by seeing how well a model fits pure noise: if it doesn’t fit it well at all (because there is no signal!), we can usually assume it won’t overfit on signal + noise. But if it fits pure noise perfectly, it has by definition overfit the training data.\nI like to think of complexity as “superstitious” or “gullibility”: the model will believe (fit to) anything we tell it (training data), whether it is true (signal) or not (noise).\nWe don’t want a model that is too complex, but we also don’t want a model that is too simple. If it can’t fit signal, it is essentially useless. In our metaphor, an overly simple (low complexity) model is like a person who simply doesn’t believe anything at all: they are never tricked, but they also can’t really understand the world.\nLet us now take a quick detour into a flexible family of models and see how performance relates to the complexity story."
  },
  {
    "objectID": "notes/notes01.html#model-complexity-redux",
    "href": "notes/notes01.html#model-complexity-redux",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Model Complexity (Redux)",
    "text": "Model Complexity (Redux)\nLet us understand this using the tools of complexity we discussed above.\n\nFor large \\(K\\), the model became quite simple, barely fit the training data, but did just as well on the training data as the test data (poorly on both). This is indicative of a small generalization gap and a low complexity model.\nFor python \\(K\\), the model became rather complex, fit the training data well, but didn’t perform as well as we would like on the test data. This is indicative of a large generalization gap and a high complexity model.\n\nIn this case, we could see the complexity visually by looking at the decision boundaries (the lines separating predictions of class “A” from class “B”). This isn’t universally true9, but the intuition of “high complexity = more wiggles” is usually pretty good.\nThe classical story of ML and complexity is given by something like this:10\n\nFor now, you can interpret “risk” as “test error” and “empirical risk” as “training error”.\n\n\n\n\n\n\nA more complex story of complexity\n\n\n\n\n\nRecent research has suggested that the story is not quite so simple. Some methods exhibit a “single descent” curve\n\nand some even have a “double descent” curve:\n\nThe story of these curves is still an active topic of research, but it’s pretty clear that very large and very deep neural networks exhibit something like a double descent curve. My own guess is that we’re not quite measuring ‘complexity’ correctly for these incredibly complex models and that the classical story holds if we go measure complexity in the right way, but this is far from a universally held belief.\n\n\n\nSo how do we measure complexity? For OLS, it seems to be proportional to \\(p\\), while for KNN it seems to be inverse to \\(K\\). In this class, we won’t really focus too much on the actual measurements. For most of the methods we study, it is usually pretty clear what drives complexity up or down, even if we can’t quite put a number on it.\n\nStability and Generalization\nIt turns out there is a deep connection between generalization and a suitable notion of “stability”. A model is said to be relatively stable if changes to an input point do not change the output predictions significantly.11\nAt an intuitive level, this makes sense: if the model is super sensitive to individual inputs, it must be very flexible and hence quite complex. (A very simple model cannot be sensitive to all of its inputs.)\nWe can apply this idea to understand some rules-of-thumb and informal practices you might have seen in previous statistics courses:\n\nRegression leverage: Leverage measures how much a single data point can change regression coefficients. This is stability!\nRemoving outliers: we often define outliers as observations which have a major (and assumed corrupting) influence on our inferences. By removing outliers, we guarantee that the resulting inference is not too sensitive to any of the remaining data points. Here, the ‘pre-step’ of outlier removal increases stability (by changing sensitivity of those observations to zero) and hopefully makes our inferences more accurate (better generalization)\nUse of robust statistics, e.g. medians instead of means. These explicitly control the stability of our process.\n\nPerhaps most importantly: this justifies why the large \\(n\\) (big sample) limit seems to avoid overfitting. If our model is fit to many distinct data points, it can’t be too sensitive to any of them. At least, that’s the hope…\nThe sort of statistical models you have seen to date – so-called parametric models – have a ‘stabilizing’ effect. By reducing lots of data to only a few parameters, those parameters (and hence the model output) can’t depend too much on any individual input point.12 This ‘bottleneck’ in the parameter space seems to improve performance.\nOther methods, like \\(1\\)-Nearest-Neighbor, become increasingly more complex as we get more data and do not benefit from this sort of ‘bottleneck’ effect.\nAt this point, you might think that stability and variance are closely related concepts - you are not wrong and we will explore the connection in more detail next week."
  },
  {
    "objectID": "notes/notes01.html#key-terms-and-concepts",
    "href": "notes/notes01.html#key-terms-and-concepts",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Key Terms and Concepts",
    "text": "Key Terms and Concepts\n\nSupervised vs Unsupervised\nRegression vs Classification\nTraining Error\nTest Error\nGeneralization Gap\nComplexity\nOverfitting\nStability\n\\(K\\)-Nearest Neighbors\nDecision Boundaries"
  },
  {
    "objectID": "notes/notes02.html#bias-variance-tradeoff",
    "href": "notes/notes02.html#bias-variance-tradeoff",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff"
  },
  {
    "objectID": "notes/notes02.html#bias-variance-decomposition",
    "href": "notes/notes02.html#bias-variance-decomposition",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Bias-Variance Decomposition",
    "text": "Bias-Variance Decomposition\nIn Report #01, you will show that, under MSE loss, our expected test error can be decomposed as\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\]\nLet’s show how we can analyze these quantities for a KNN regression problem. Here, we’re using the ‘regression’ version of KNN since it plays nicely with MSE.1\n\n# Don't attach the package to avoid weird aliasing issues\nknn.reg &lt;- FNN::knn.reg \nargs(knn.reg)\n\nfunction (train, test = NULL, y, k = 3, algorithm = c(\"kd_tree\", \n    \"cover_tree\", \"brute\")) \nNULL\n\n\nWe also need a ‘true’ function which we’re trying to estimate. Let’s use the following model:\n\\[\\begin{align*}\nX &\\sim \\mathcal{U}([0, 1]) \\\\\nY &\\sim \\mathcal{N}(4\\sqrt{X} + 0.5 * \\sin(4\\pi * X), 0.25)\n\\end{align*}\\]\nThat is, \\(X\\) is uniform on the unit interval and \\(Y\\) is a non-linear function of \\(X\\) plus some Gaussian noise.\nFirst let’s plot \\(X\\) vs \\(\\E[X]\\) - under MSE loss this is our ‘best’ (Bayes-optimal) possible guess.\n\nyfun &lt;- function(x) 4 * sqrt(x) + 0.5 * sinpi(4 * x)\n\nx &lt;- seq(0, 1, length.out=101)\ny_mean &lt;- yfun(x)\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     main=\"True Regression Function\", cex.lab=1.5)\n\n\n\n\n\n\n\n\nTo generate training data from this model, we simply implement the PRNG components:\n\nx_train &lt;- runif(25, 0, 1) # 25 training points\ny_train &lt;- rnorm(25, mean=yfun(x_train), sd=0.5)\n\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\npoints(x_train, y_train)\n\n\n\n\n\n\n\n\nWe have some variance of course, but we can still “squint” to get the right shape of our function. Let’s see how KNN looks on this data.\nWe start with \\(K=1\\):\n\nX_train &lt;- matrix(x_train, ncol=1)\nplot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\nY_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=plot_grid)$pred\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\npoints(x_train, y_train)\n\nlines(plot_grid, Y_hat, col=\"red4\", lwd=2)\n\n\n\n\n\n\n\n\nThis is not a great fit - what happens if we repeat this process may times?\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\nfor(i in seq(1, 20)){\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    test_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    Y_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=test_grid)$pred\n    lines(test_grid, Y_hat, col=\"#FFAA0099\", lwd=0.5)\n}\n\n\n\n\n\n\n\n\nClearly we have some variance!\nIf we repeat with a higher value of \\(K\\), we see far less variance:\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\nfor(i in seq(1, 20)){\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    Y_hat &lt;- knn.reg(train=X_train, y=y_train, k=10, test=test_grid)$pred\n    lines(test_grid, Y_hat, col=\"#FFAA0099\", lwd=0.5)\n}\n\n\n\n\n\n\n\n\nHow well does KNN do on average?\nThat is, if we could repeat this process (infinitely) many times, how well would it recover the true regression function? Let’s try \\(K=1\\) and \\(K=10\\):\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\n\n\nKNN_AVERAGE_PRED_K1 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=1, test=test_grid)$pred\n}))\n\nlines(test_grid, KNN_AVERAGE_PRED_K1, col=\"red4\")\n\n\nKNN_AVERAGE_PRED_K10 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=10, test=test_grid)$pred\n}))\n\nlines(test_grid, KNN_AVERAGE_PRED_K10, col=\"blue4\")\n\n\n\n\n\n\n\n\nWe see here that, on average, KNN with \\(K=1\\) (red) basically gets the function just right - no bias!\nOn the other hand, because KNN with \\(K=10\\) smooths out the function, we see systematic errors (here oversmoothing). That’s some bias.\nSo which is better? - \\(K=1\\) - High variance, but low bias - \\(K=10\\) - Low variance, but high bias\nWe’ll have to look at some test error to see. For now, we’ll generate our test data exactly the same way as we generate our training data:\n\nKNN_K1_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    \n    # Generate from same model as before\n    X_test &lt;- matrix(runif(25, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K1_MSE &lt;- mean(rowMeans(KNN_K1_ERROR^2))\n\nKNN_K10_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    \n    # Generate from same model as before\n    X_test &lt;- matrix(runif(25, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=10, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K10_MSE &lt;- mean(rowMeans(KNN_K10_ERROR^2))\n\ncbind(K1_MSE = KNN_K1_MSE, \n      K10_MSE = KNN_K10_MSE)\n\n       K1_MSE K10_MSE\n[1,] 1.954541 1.46504\n\n\n\\(K=10\\) does better overall!\nBut does it do better everywhere or are some parts of the problem better for \\(K=1\\)?\nNow we’ll be systematic in our test data - spacing it equally on the grid and computing ‘pointwise’ MSE:\n\nKNN_K1_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    # Generate from same model as before\n    test_grid &lt;- seq(0, 1, length.out=101)\n    X_test &lt;- matrix(runif(test_grid, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(test_grid, mean=yfun(X_test), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K1_MSE &lt;- rowMeans(KNN_K1_ERROR^2)\n\nKNN_K10_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    test_grid &lt;- seq(0, 1, length.out=101)\n    X_test &lt;- matrix(runif(test_grid, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(101, mean=yfun(X_test), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=10, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K10_MSE &lt;- rowMeans(KNN_K10_ERROR^2)\n\nplot(KNN_K1_MSE, col=\"blue4\", pch=16, ylim=c(0, 1))\npoints(KNN_K10_MSE, col=\"red4\", pch=16)\n\n\n\n\n\n\n\n\nIt looks like - for this set up at least - \\(K=10\\) is better everywhere but that’s not always the case.\nPlay around with the size of the training data, the noise in the samples, and the data generating function (yfun) to see if you can get different behavior.\nAlso - is \\(K=10\\) really the optimal choice here? What would happen if we changed \\(n\\)?\nSo, now that we have a good sense of (average) test error, can we verify our MSE decomposition?\nRecall \\[\\begin{align*}\n\\E[\\text{MSE}] &= \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} \\\\\n\\text{ where } \\text{Bias}^2 &= \\E\\left[\\left(\\E[\\hat{y}] - \\E[y]\\right)^2\\right] \\\\\n&= \\left(\\E[\\hat{y}] - \\E[y]\\right)^2 \\text{ (Why can I drop the outer expectation?)} \\\\\n\\text{Variance} &= \\E\\left[\\left(\\hat{y} - \\E[\\hat{y}]\\right)^2\\right] \\\\\n\\text{Irreducible Error} &= \\E\\left[\\left(y - \\E[y]\\right)^2\\right]\n\\end{align*}\\]\n(Make sure you understand these definitions and how they work together!)\nLet’s work these out using all the tools we built before.\nFirst, for the bias: - we already have \\(\\E[y]\\) - this is just the yfun we selected - we can compute \\(\\E[\\hat{y}]\\) by running KNN many times and averaging the result\n\nsample_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\nKNN_AVERAGE_PRED_K1 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=1, test=sample_grid)$pred\n}))\n\nKNN_BIAS_K1 &lt;- KNN_AVERAGE_PRED_K1 - yfun(sample_grid)\nplot(sample_grid, KNN_BIAS_K1^2, col=\"red4\", \n     type=\"l\", main=\"Squared Bias of KNN with K=1\")\n\n\n\n\n\n\n\n\nNot too much bias, but things do go a bit off the rails near the end points.\nNext, we can compute variance pointwise:\n\nsample_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\nKNN_VARIANCE_K1 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    (knn.reg(train=X_train, y=y_train, k=1, test=sample_grid)$pred - KNN_AVERAGE_PRED_K1)^2\n}))\n\nplot(sample_grid, KNN_VARIANCE_K1, col=\"red4\", \n     type=\"l\", main=\"Variance of KNN with K=1\")\n\n\n\n\n\n\n\n\nFor this data at least, the variance term is generally larger than the bias term: this is what we expect with a very flexible (high variance + low bias) model like \\(1\\)-NN.\nFinally, irreducible error is just 0.25 everywhere (recall \\(y \\sim \\mathcal{N}(\\E[y], 0.25)\\)).\n\nKNN_IE &lt;- rowMeans(replicate(500, {\n    sample_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    X_test &lt;- matrix(sample_grid, ncol=1)\n    y_test &lt;- matrix(rnorm(sample_grid, mean=yfun(X_test), sd=0.5), ncol=1)\n    y_best_pred &lt;- matrix(yfun(X_test), ncol=1)\n    (as.vector(y_best_pred - y_test))^2\n}))\n\nplot(KNN_IE)\n\n\n\n\n\n\n\n\nPut these together and we see the decomposition in action:\n\nmean(KNN_BIAS_K1)^2 + mean(KNN_VARIANCE_K1) + mean(KNN_IE)\n\n[1] 0.5290861\n\n\nas compared to\n\nmean(KNN_K1_MSE)\n\n[1] 0.5242858\n\n\nVisually,\n\nlibrary(dplyr)\nlibrary(tidyverse)\nDECOMP_DATA &lt;- data.frame(\n   sample_grid = sample_grid, \n   KNN_K1_BIAS2 = KNN_BIAS_K1^2, \n   KNN_K1_VARIANCE=KNN_VARIANCE_K1, \n   KNN_K1_IE = KNN_IE, \n   KNN_K1_MSE = KNN_K1_MSE) |&gt; \n   pivot_longer(-sample_grid) |&gt;\n   mutate(Error=value,\n          Type=case_when(\n            name==\"KNN_K1_BIAS2\" ~ \"Bias^2\", \n            name==\"KNN_K1_IE\" ~ \"Irreducible Error\", \n            name==\"KNN_K1_VARIANCE\" ~ \"Variance\", \n            name==\"KNN_K1_MSE\" ~ \"Total Error\")) \n\n\nggplot() + \n  geom_bar(data=DECOMP_DATA |&gt; filter(Type != \"Total Error\"), \n           mapping=aes(x=sample_grid, y=Error, color=Type), \n           stat=\"identity\") + \n  geom_line(data=DECOMP_DATA |&gt; filter(Type == \"Total Error\"), \n            mapping=aes(x=sample_grid, y=Error), \n            color=\"red4\", linewidth=2) + \n  xlab(\"X\") + ylab(\"Test Error\")\n\n\n\n\n\n\n\n\nSo a bit of weirdness at the left end point - but holds up as well as we might expect for \\(N=25\\) samples.\nHow do the relative magnitudes of these terms change as you adjust the parameters of the simulation?"
  },
  {
    "objectID": "notes/notes02.html#linear-algebra-review",
    "href": "notes/notes02.html#linear-algebra-review",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Linear Algebra Review",
    "text": "Linear Algebra Review\nIn mathematics, a vector - random or otherwise - is a fixed-length ordered collection of numbers. When we want to be precise about the size of a vector, we often call it a “tuple”, e.g., a length-three vector is a “triple”, a length-four vector is a “4-tuple”, a length-five vector is a “5-tuple” etc..\nSo, these are all vectors:\n\n\\((3, 4)\\)\n\\((1, 1, 1)\\)\n\\((1, 5, 6, 10)\\)\n\nWhen we want to talk about the set of vectors of a given size, we use the Cartesian product of sets. For two sets, \\(A, B\\), the product set \\(A \\times B\\) is the set of all pairs, with the first element from \\(A\\) and the second from \\(B\\). In mathematical notation,\n\\[A \\times B = \\left\\{(a, b): a \\in A, b \\in B\\right\\} \\]\nThis set-builder notation is read as follows: “The Cartesian Product of \\(A\\) and \\(B\\) is the set of all pairs \\((a, b)\\) such that \\(a\\) is in \\(A\\) and \\(b\\) is in \\(B\\).”\nIf \\(A\\) and \\(B\\) are the same set, we define a Cartesian power as follows:\n\\[A^2 = A \\times A = \\left\\{(a_1, a_2): a_1 \\in A, a_2 \\in A\\right\\}\\]\nNote that even though the sets \\(A\\) and \\(A\\) in this product are the same, the elements in each pair may vary. For example, if \\(A = \\{1, 2, 3\\}\\), we have\n\\[A^2 = \\left\\{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3,2), (3, 3)\\right\\}\\]\nNote that vectors are ordered pairs so \\((2, 1) \\neq (1, 2)\\). From here, it should be pretty easy to convince yourself that set sizes play nicely with Cartesian products:\n\n\\(|A \\times B| = |A| |B|\\)\n\\(|A^k| = |A|^k\\)\n\nThe most common set of vectors we use are those where each element is an arbitrary real number. The set of vectors of length \\(n\\) (\\(n\\)-tuples) is thus \\(\\R^n\\). We rarely mix vectors of different lengths, so we don’t really have a name or notation for the “combo pack” \\(\\R^2 \\cup \\R^3 \\cup \\R^4\\).\nConventionally, vectors are written in bold (if on a computer) or with a little arrow on top (hand written): so a vector called “x” would be denoted \\(\\mathbf{x}\\) or \\(\\vec{x}\\). The elements of \\(\\bx\\) are denoted by subscripts \\(\\bx = (x_1, x_2, \\dots, x_n)\\).\n\nVector Arithmetic\nWe have three arithmetic operations we can perform on general vectors. The simplest is scalar multiplication. A scalar is a non-vector number, i.e., a ‘regular’ number. Scalar multiplication consists of applying the scalar independently to each element of a vector.\n\\[\\alpha \\bx = \\alpha(x_1, x_2, \\dots, x_n) = (\\alpha x_1, \\alpha x_2, \\dots, \\alpha x_n)\\]\nFor example, if \\(\\bx = (3, 4)\\) and \\(\\alpha = 2\\), we have \\[\\alpha \\bx = (6, 8)\\]\nNote that the output of scalar multiplication is always a vector of the same length as the input.\nWe also have the ability to add vectors. This again is performed element-wise.\n\\[\\bx + \\by = (x_1, \\dots, x_n) + (y_1, \\dots, y_n) = (x_1 + y_1, \\dots, x_n + y_n) \\]\nNote that we can’t add vectors of different lengths (recall our “no mixing” rule) and the output length is always the same as the input lengths.\nFinally, we have the vector inner product, defined as:\n\\[\\langle \\bx, \\by \\rangle = x_1y_1 + x_2y_2 + \\dots + x_ny_n \\]\nYou might have seen this previously as the “dot” product. The inner product takes two length-\\(n\\) vectors and gives back a scalar. This structure might seem a bit funny, but as we’ll see below, it’s actually quite useful.\nYou might ask if there’s a “vector-out” product: there is one, with the fancy name “Hadamard product”, but it doesn’t play nicely with other tools, so we don’t use it very much.\nThese tools play nicely together:\n\n\\(\\alpha(\\bx + \\by) = \\alpha \\bx + \\alpha \\by\\) (Distributive)\n\\(\\langle \\alpha \\bx, \\by \\rangle = \\alpha \\langle \\bx, \\by \\rangle\\) (Associative)\n\\(\\langle \\bx, \\by \\rangle = \\langle \\by, \\bx \\rangle\\) (Commutative)\n\n\n\nVector Length and Angle\nWe sometimes want to think about the “size” of a vector, analogous to the absolute value of a scalar. In scalar-world, we say “drop the sign” but there’s not an obvious analogue to a sign for a vector. For instance, if \\(\\bx = (3, -4)\\) is \\(\\bx\\) “positive”, “negative” or somewhere in beetween?\nWe note a trick from scalar-land: \\(|x| = \\sqrt{x^2}\\). We can use the same idea for vectors:\n\\[ \\|\\bx\\| = \\sqrt{\\langle \\bx, \\bx\\rangle} = \\sqrt{\\sum_{i=1}^n x_i^2}\\]\nThis quantity, \\(\\|\\bx\\|\\), is called the norm or length of a vector. We use the double bars to distinguish it from the absolute value of a scalar, but it’s fundamentally the same idea.\nIn \\(\\R^2\\), we recognize this formula for length as the Pythagorean theorem:\n\\[ \\|(3, 4)\\| = \\sqrt{3^2 + 4^2} = \\sqrt{25} = 5 \\]\nWe also sometimes want to define the angle between two vectors. We can define this as:\n\\[ \\cos \\angle(\\bx, \\by) = \\frac{\\langle \\bx, \\by\\rangle}{\\|\\bx\\|\\|\\by\\|} \\Leftrightarrow \\angle(\\bx, \\by) = \\cos^{-1}\\left(\\frac{\\langle \\bx, \\by\\rangle}{\\|\\bx\\|\\|\\by\\|}\\right)\\]\nWe won’t use this formula too often for implementation, but it’s good to have it for intuition. In particular, we note that angle is a proxy for sample correlation, justifying the common vernacular of “orthogonal”, meaning “at right angles”, for “uncorrelated” or “unrelated.”\n\n\nMatrices\nAn \\(n\\)-by-\\(p\\) array of numbers is called a matrix; here the first dimension is the number of rows while the second is the number of columns. So \\[\\bA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix}\\] is a 3-by-2 matrix. We denote the set of matrices of a given size as \\(\\R^{n \\times p}\\), extending slightly the notation we use for vectors.\nIn this course, we will use matrices for two closely-related reasons:\n\nTo organize our data\nTo specify and manipulate linear models\n\nSpecifically, if we have \\(n\\) training samples, each of \\(p\\) covariates (predictors), we will arrange them in a matrix traditionally called \\(\\bX \\in \\R^{n \\times p}\\). Here, each row of \\(\\bX\\) corresponds to an observation. Statisticians tend to call this matrix a design matrix because (historically) it was something designed as part of an experiment; the name got carried forward into the observational (un-designed) setting. You may also hear it called the ‘data matrix’.\nSuppose we have a design matrix \\(\\bX \\in \\R^{n \\times p}\\) and a vector of regression coefficients \\(\\mathbf{\\beta} \\in \\R^p\\). We can use matrix multiplication to make predictions about all observations simultaneously.\nSpecifically, recall that the standard (multivariate) linear model looks like:\n\\[\\hat{y} = \\sum_{i=1}^p x_i\\beta_i\\]\nFor a single observation, this can be written in vector notation as\n\\[\\hat{y} = \\bx^{\\top}\\bbeta\\]\nIf we have \\(n\\) observations, we can stack them in a vector as:\n\\[\\begin{pmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{pmatrix} =\n\\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\\\bx_n^{\\top}\\bbeta \\end{pmatrix}\\]\nWe can connect this with our design matrix \\(\\bX\\) by using the above as a definition of matrix-vector multiplication:\n\\[\\hat{\\by} = \\bX\\bbeta =  \n\\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\\\bx_n^{\\top}\\bbeta \\end{pmatrix}\\]\nHere, matrix multiplication proceeds by taking the inner product of each row with the (column) vector \\(\\bbeta\\). This may feel a bit unnatural at first, but with a bit of practice, it will become second nature. Note that we don’t need a transpose on \\(\\bX\\): the multiplication ‘auto-transposes’ in some sense.\nIt is always helpful to keep track of dimensions when doing matrix multiplication: checking that matrices and vectors have the right size is a useful way to make sure you haven’t done anything too wrong. In general, we can only multiply a \\(m\\)-by-\\(n\\) matrix with a \\(n\\)-by-\\(p\\) matrix and the result is a \\(m\\)-by-\\(p\\) matrix (the \\(n\\)-dimension gets reduced to a scalar by the inner product). Formally, we have something like\n\\[ \\R^{m \\times n} \\times \\R^{n \\times p} \\to \\R^{m \\times p}\\]\nFor purposes of this, you can always think of an \\(n\\)-vector as a \\(n\\)-by-1 “column” matrix, giving us:\n\\[\\underbrace{\\bX}_{\\R^{n \\times p}} \\underbrace{\\bbeta}_{\\R^p} = \\underbrace{\\hat{\\by}}_{\\R^{n \\times 1}}\\]\n\n\nSpectral Properties of Symmetric Matrices\nAn important class of matrices we will consider are symmetric matrices, which are just what the name sounds like. These come up in several key places in statistics, none more important than the covariance matrix, typically denoted \\(\\Sigma\\). Recall that the covariance operator \\(\\mathbb{C}\\) is symmetric (\\(\\mathbb{C}[X, Y] = \\mathbb{C}[Y, X]\\)) so the covariance matrix of a random vector turns out to be symmetric as well.\nAnother common source of symmetric matrices is when a matrix is multiplied by its transpose: you should convince yourself that \\(\\bX^{\\top}\\bX \\in \\R^{p \\times p}\\) is a symmetric matrix."
  },
  {
    "objectID": "notes/notes02.html#introduction-to-convex-optimization",
    "href": "notes/notes02.html#introduction-to-convex-optimization",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Introduction to Convex Optimization",
    "text": "Introduction to Convex Optimization\nIn this course, we will frequently want to find the minimizer of certain functions. Typically, these will arise as ERMs and the function will take a \\(p\\)-vector of inputs, e.g. regression coefficients, and produce a scalar loss output. Let us generally call this function \\(f: \\R^p \\to \\R\\).\nIn some circumstances, we can take the derivative of \\(f\\), typically called the gradient in this context, and set it equal to zero. For example, suppose we want to minimize an expression of the form:\n\\[f(\\bx) = \\frac{1}{2}\\bx^{\\top}\\bA\\bx + \\bb^{\\top}\\bx + c\\]\nwhere \\(\\bA\\) is a symmetric strictly positive-definite matrix, \\(\\bb\\) is an arbitrary \\(p\\)-vector, and \\(c\\) is a constant. Taking the gradient, we find\n\\[ \\frac{\\partial f}{\\partial \\bx} = \\bA\\bx + \\bb\\]\nWe set this to zero and find a crticial point at:\n\\[\\begin{align*}\n\\mathbf{0} &= \\bA\\bx + \\bb \\\\\n- \\bb &= \\bA\\bx \\\\\n\\implies \\bx &= -\\bA^{-1}\\bb\n\\end{align*}\\]\nassuming that \\(\\bA\\) is invertible. (Here, invertibility is implied by assuming \\(\\bA\\) is strictly positive-definite.) As usual, we are not quite done here, as we must also check that this is a minimizer and not a maximizer or a saddle point. To do so, we take the second derivative to find\n\\[ \\frac{\\partial}{\\partial \\bx}\\frac{\\partial f}{\\partial \\bx} = \\bA\\]\nIn this multivariate context, we need the second derivative, a.k.a the Hessian, to be strictly positive-definite to guarantee that we have found a minimizer and this is indeed exactly what we assumed above. As discussed above, ‘definiteness’ plays the role of sign for many applications of matrix-ness. Here, by assuming strictly positive definite, we are essentially treating the matrix as strictly positive (\\(&gt;0\\)), which is exactly the condition we need to guarantee a minimizer in the scalar case as well.\nHence, we have that the one minimizer of \\(f\\) is found at \\(\\bx_* = \\bA^{-1}\\bb\\). Compare this to the scalar case of minimizing a quadratic \\(\\frac{1}{2}ax^2 + bx + c\\) with minimizer at \\(x = -b/a\\).2\nThe above analysis works well, but it is essentially the only minimization we will be able to do in closed form in this course.3\nFor other functions, we will need to apply optimization: the mathematical toolkit for finding minimizers (or maximizers) of functions. Fortunately for us, many of the methods we begin this course with fall in the realm of convex optimization, a particularly nice branch of optimization.\nConvex Optimization refers to the problem of minimizing convex functions over convex sets. Let’s define both of these:\n\nA convex function is one which satisfies this inequality at all points: \\[f(\\lambda \\bx + (1-\\lambda)\\by) \\leq \\lambda f(\\bx) + (1-\\lambda) f(\\by)\\]\nfor any \\(\\bx, \\by\\) and any \\(\\lambda \\in [0, 1]\\).\nThis definition is a bit non-intuitive, but it basically implies that we have a “bowl-like” function. This definition captures the idea that the actual function value is always less than we might get from linear interpolation. A picture is helpful here:\n\n\n\n\n\n\n\n\n\n\nHere, we see that the actual value of the function (blue dot) is less than what we would get if we interpolated the two red points (red line).\nAn alternative definition is that if \\(f\\) is twice-differentiable, its Hessian (2nd derivative matrix) is positive semi-definite.\n\nA convex set is one that allows us to look “between” points. Specifically, a set \\(\\mathcal{C} \\subseteq \\R^p\\) is convex if \\[\\bx, \\by \\in \\mathcal{C} \\implies \\lambda \\bx + (1-\\lambda)\\by \\in \\mathcal{C}\\]\nfor all \\(\\bx, \\by \\in \\mathcal{C}\\) and any \\(\\lambda \\in [0, 1]\\).\n\nClearly these are related by the idea of looking “between” two points:\n\nConvex functions guarantee that this will produce something lower than naive interpolation; and\nConvex sets guarantee that the midpoint will be “allowable”.\n\nTo tie these together, note another alternative characterization of a convex function: one whose epigraph (the area above the curve in the plot) is a convex set.\nFor optimization purposes, these two properties imply a rather remarkable fact:\nIf \\(\\bx_0\\) is a local minimum of \\[\\min_{\\bx \\in \\mathcal{C}} f(\\bx)\\], then it is a global minimum.\nThis is quite shocking: if we find a point where we can’t improve by going in any direction, we are guaranteed to have found a global minimum and no point could be better. (It is possible to have multiple minimizers however: consider \\(f(\\bx) = 0\\). Any choice of \\(\\bx\\) is a global minimizer.) This lets us turn the “global” search problem into a “local” one.\nOf course, this only helps us if we can find a local minimizer of \\(f\\). How might we do so? Let’s recall a basic calculus idea: the gradient of a function is a vector that points in the direction of greatest increase (the “steepest” uphill). So if we go in the opposite direction of the gradient, we actually will go “downhill”. In fact, this is basically all we need to start applying gradient descent.\nGradient Descent: Given a convex function \\(f\\):\n\nInitialize at (arbitrary) starting point \\(\\bx_0\\).\nInitialize step counter \\(k=0\\).\nRepeat until convergence:\n\nCompute the gradient of \\(f\\) at \\(\\bx_k\\): \\(\\left.\\nabla f\\right.|_{\\bx=\\bx_k}\\)\nSet \\(\\bx_{k+1} = \\bx_k - c \\left.\\nabla f\\right.|_{\\bx=\\bx_k}\\)\n\n\nRepeated infinitely many times, this will converge to a local, and hence global, minimizer of \\(f\\). There are many variants of this basic idea4, mostly related to selecting the optimal step size \\(c\\), but this is the most important algorithm in convex optimization and it is important to understand it deeply.\nWe can apply it here to the 3D function \\[f(\\bx) = (x_1-2)^2 + (x_2-3)^2 + (x_3 - 4)^2.\\] Clearly, we can see that the minimizer has to be at \\((2, 3, 4)\\), but our algorithm won’t use that fact.\nBefore we can implement this algorithm, we need the gradient, which is given by\n\\[f(\\bx) = \\left\\|\\bx - (2,3,4)^{\\top}\\right\\|_2^2 \\implies \\nabla f = 2[\\bx - (2, 3, 4)^{\\top}]\\]\n\nx &lt;- matrix(c(0, 0, 0), ncol=1) # We want to work exclusively with column vecs\nconverged &lt;- FALSE\nc &lt;- 0.001 # Small step size\nf &lt;- function(x) sum((x - c(2, 3, 4))^2)\ngrad &lt;- function(x) 2 * (x - matrix(c(2, 3, 4), ncol=1))\n\nwhile(!converged){\n    g &lt;- grad(x)\n    x_new &lt;- x - c * g\n    \n    if(sum(abs(x - x_new)) &lt; 1e-5){\n        converged &lt;- TRUE\n    }\n    \n    x &lt;- x_new\n}\n\nx\n\n         [,1]\n[1,] 1.998893\n[2,] 2.998340\n[3,] 3.997787\n\n\nWe don’t get the exact minimizer, but we certainly get something ‘close enough’ for our purposes. In Report #01, you will use gradient descent and some variants to study the least squares problem.\nOften, we will want to see how the value of \\(f(\\bx_k)\\) changes over the course of the optimization. We expect that it will go down monotonically, but it may not be worth continuing the optimization if we have reached a point of ‘diminishing returns.’ You can do this by hand (evaluating \\(f\\) after each update and storing the results), but many optimizers will often track this automatically for you: e.g. TensorBoard.5"
  },
  {
    "objectID": "notes/notes02.html#footnotes",
    "href": "notes/notes02.html#footnotes",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Bias-Variance decomposition (and tradeoff) holds approximately for other loss functions, though the math is only this nice for MSE.↩︎\nNote that we can only minimize this quadratic in the case where \\(a\\) is strictly positive so the parabola is upward facing. This is the scalar equivalent of the strict positive-definiteness condition we put on \\(\\bA\\).↩︎\nOLS and some basic variants (ridge regression) can be written in this ‘quadratic’ style. Can you see why?↩︎\nFor instance, of the 14 optimization algorithms included in base pytorch, all but one are advanced versions of gradient descent. The exception is LBFGS which attempts to (approximately) use both the gradient and the Hessian (second derivative); computing the Hessian is normally quite expensive, so LBFGS uses some clever tricks to approximate the Hessian.↩︎\nModern ML toolkits like pytorch or TensorFlow are (at heart) fancy systems to do two things automatically that we did ‘by hand’ in this example:\n\nCompute gradients via a process known as ‘back-propogation’ or ‘autodiff’\nImplement (fancy) gradient descent\n\n↩︎"
  },
  {
    "objectID": "notes/notes02.html",
    "href": "notes/notes02.html",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "",
    "text": "\\[\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}}\\]"
  },
  {
    "objectID": "notes/notes02.html#ordinary-least-squares",
    "href": "notes/notes02.html#ordinary-least-squares",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nSuppose we want to fit a linear model to a given data set (for any of the reasons we discuss in more detail below): how can we choose which line to fit? (There are infinitely many!)\nSince our goal is minimizing test error, and we hope training error is at least a somewhat helpful proxy for training error, we can pick the line that minimizes training error. To do this, we need to commit to a specific measure of error. As the name Ordinary Least Squares suggests, OLS uses (mean) squared error as its target.\nWhy is MSE the right choice here? It turns out that MSE is very nice computationally, but the reason is actually more fundamental: given a random variable \\(Z\\), suppose we want to minimize the \\((Z - \\mu)\\) for some \\(\\mu\\): it can be shown that\n\\[ \\E[Z] = \\text{argmin}_{\\mu \\in \\R} \\E[(Z - \\mu)^2]\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nProve this for yourself.\nHint: Differentiate with respect to \\(\\mu\\).\n\n\n\nThat is, the quantity that minimizes the MSE is the mean. So when we fit a line to some data by OLS, we are implicitly trying to fit to \\(\\E[y]\\) - a very reasonable thing to do!\nSpecifically, given training data \\(\\mathcal{D} = \\{(\\bx_i, y_i)\\}_{i=1}^n\\) where each \\(\\bx_i \\in \\R^p\\), OLS finds \\(\\bbeta \\in \\R^p\\) such that \\[ \\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^p \\beta_jx_j\\right)^2 = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\bx^{\\top}\\bbeta\\right)^2\\]\nSome possibly new notation here:\n\n\\(\\bx^{\\top}\\bbeta\\) is the (inner) product of two vectors: defined as the sum of their elementwise products.\nOptimization problems:\n\n\\[\\hat{x} = \\text{argmin}_{x \\in \\mathcal{C}} f(x)\\]\n\\[f_* = \\text{min}_{x \\in \\mathcal{C}} f(x)\\]\n\nThese problems say: find the value of \\(x\\) in the set \\(\\mathcal{C}\\) that minimizes the function \\(f\\). \\(\\text{argmin}\\) says ‘give me the minimizer’ while \\(\\min\\) says give me the minimum value’. These are related by \\(f_* = f(\\hat{x})\\)\nThe function \\(f\\) is called the objective; the set \\(\\mathcal{C}\\) is called the constraint set.\n\nOrdinary Least Squares refers to the use of an MSE objective without any additional constraints.\nNote the general structure of our approach here:\n\nDefine the loss we care about\nSet up an optimization problem to minimize loss on the training set\nSolve optimization problem\n\nML folk call this empirical risk minimization (ERM) since we’re minimizing the risk (average loss) on the data we can see (the training data). Statisticians call this \\(M\\)-estimation, since it defines an estimator by Minimization of a measure of ‘fit’. Whatever you call it, it’s a very useful ‘meta-method’ for coming up with ML methods.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\n\nHow does this compare with Maximum Likelihood Estimation?\nWe set up this ERM method using mean squared error - what happens with other errors? Specifically, formulate this ERM for\n\nmean absolute error\nmean percent error\n\nand compare to OLS.\n\n\n\n\nSo far we’ve set up OLS as \\[ \\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\bx^{\\top}\\bbeta\\right)^2\\]\nWe can clean this up to make additional analysis easier:\n\nLet \\[\\by = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\\] be the (vertically stacked) vector of responses.\nNext look at our predictions: \\[\\hat{\\by} = \\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\bx_n^{\\top}\\bbeta \\end{pmatrix} = \\begin{pmatrix} \\bx_1^{\\top} \\\\ \\bx_2^{\\top} \\\\ \\vdots \\\\ \\bx_n^{\\top} \\end{pmatrix}\\bbeta = \\bX\\bbeta\\]\n\nHence, OLS is just \\[\\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n} \\|\\by - \\bX\\bbeta\\|_2^2\\] Here \\(\\|\\cdot\\|_2^2\\) is the (squared Euclidean or \\(L_2\\)) norm of a vector: defined by \\(\\|\\bz\\|_2^2 = \\sum z_i^2\\)\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\n\nDo we actually need the \\(1/n\\) part?\nWhy is this called linear?\n\n\n\n\nWe’ve formulated OLS as \\[\\hat{\\beta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n} \\|\\by - \\bX\\bbeta\\|_2^2.\\] In order to solve this, it will be useful to modify it slightly to \\[\\hat{\\beta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{2} \\|\\by - \\bX\\bbeta\\|_2^2\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhy is this ok to do?\n\n\n\nYou will show in Report #01 that the solution is given by\n\\[\\hat{\\beta} = (\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\by\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nProve this for yourself. What conditions are required on \\(\\bX\\) for the inverse in the above expression to exist?\n\n\n\nWith this estimate, our in-sample predictions are given by:\n\\[\\hat{\\by} = \\bX\\hat{\\bbeta} = \\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\by \\]\nThis says that our predictions are, in some sense, just a linear function of the original data \\(\\by\\). The matrix\n\\[\\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top} = \\bH\\]\nis sometimes called the ‘hat’ matrix because it puts a hat on \\(\\by\\) (\\(\\hat{\\by}=\\bH\\by\\)).\n\\(\\bH\\) can be shown to be a special type of matrix called a projector, meaning that it has eigenvalues all 0 or 1. For the hat matrix specifically, we can show that eigenvalues of \\(\\bH\\) are \\(p\\) zeros and \\(n - p\\) ones.\nThis in turn implies that it is idempotent, meaning \\(\\bH^2 = \\bH\\bH = \\bH\\). (To show this, simply express \\(\\bH\\) in terms of its eigendecomposition.) We can use this property to finally justify the in-sample MSE of OLS we have cited several times.\nThe in-sample MSE is given by:\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{n}\\|\\by - \\hat{\\by}\\|_2^2 \\\\\n           &= \\frac{1}{n}\\left\\|\\by - \\bH\\by\\right\\|_2^2 \\\\\n           &= \\frac{1}{n}(\\by - \\bH\\by)^{\\top}(\\by - \\bH\\by) \\\\\n           &= \\frac{1}{n}(\\by^{\\top} - \\by^{\\top}\\bH)(\\by - \\bH\\by) \\\\\n           &= \\frac{\\by^{\\top}\\by - \\by^{\\top}\\bH\\by - \\by^{\\top}\\bH\\by + \\by^{\\top}\\bH\\bH\\by}{n} \\\\\n           &= \\frac{\\by^{\\top}\\by - \\by^{\\top}\\bH\\by}{n} \\\\\n           &= \\frac{\\by^{\\top}(\\bI - \\bH)\\by}{n} \\\\\n\\implies \\E[\\text{MSE}] &= \\E\\left[\\frac{\\by^{\\top}(\\bI - \\bH)\\by}{n}\\right] \\\\\n&= \\frac{1}{n}\\E\\left[\\by^{\\top}(\\bI - \\bH)\\by\\right]\n\\end{align*}\\]\nTo finish this, we need to know that the expectation of a symmetric quadratic form satisfies\n\\[\\bx \\sim (\\mu, \\Sigma) \\implies \\E[\\bx^{\\top}\\bA\\bx] = \\text{Tr}(\\bA\\Sigma) + \\mu^{\\top}\\bA\\mu\\]\nfor any random vector \\(\\bx\\) with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\).\nTo apply this above, we note \\(\\by \\sim (\\bX\\beta_*, \\sigma^2 \\bI)\\), so we get\n\\[\\begin{align*}\n\\E\\left[\\by^{\\top}(\\bI - \\bH)\\by\\right] &= \\text{Tr}((\\bI - \\bH) \\sigma^2 \\bI) + (\\bX\\bbeta_*)^{\\top}(\\bI - \\bH)(\\bX\\bbeta_*) \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}\\bX^{\\top}(\\bI - \\bH)\\bX\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bH\\bX)\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\bX)\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bX)\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top} \\mathbf{0}\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) \\\\\n&= \\sigma^2 (\\text{Tr}(\\bI) - \\text{Tr}(\\bH))\n\\end{align*}\\]\nRecall that the trace is simply the sum of the eigenvalues, so this last term becomes \\(\\sigma^2(n - p)\\), finally giving us:\n\\[\\E[\\text{MSE}] = \\frac{\\sigma^2(n-p)}{n}  = \\sigma^2\\left(1 - \\frac{p}{n}\\right)\\]\nWhew! That was a lot of work! But can you imagine how much more work this would have been without all of these matrix tools?\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nMake sure you can justify every step in the derivation above. This is a particularly long computation, but we will use the individual steps many more times in this course."
  },
  {
    "objectID": "notes/notes03.html",
    "href": "notes/notes03.html",
    "title": "STA 9890 - Regularized Regression",
    "section": "",
    "text": "\\[\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}} \\newcommand{\\V}{\\mathbb{V}} \\newcommand{\\argmin}{\\text{arg min}}\\]\nLast week we showed the following:\nWe begin this week by asking if we can do better than OLS. To keep things simple, we begin by assuming we are under a linear DGP (so no ‘model error’) but that’s only a mathematical niceity. It’s not something you should always assume - in fact, it is really more important to think about how models do on non-linear DGPs. As we will see, it may still be useful to use linear models…\nBecause OLS is BLUE under our assumptions, we know that we need to relax one or more of our assumptions to beat it. For now, we will focus on relaxing the U - unbiasedness; non-linear methods come later in this course.\nRecalling our decomposition:\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance}\\]\nOur gambit is that we can find an alternative estimator with a bit more bias, but far less variance. Before we attempt to do so for linear regression, let’s convince ourselves this is possible for a much simpler problem - estimating means."
  },
  {
    "objectID": "notes/notes03.html#estimating-normal-means",
    "href": "notes/notes03.html#estimating-normal-means",
    "title": "STA 9890 - Regularized Regression",
    "section": "Estimating Normal Means",
    "text": "Estimating Normal Means\nSuppose we have data from a distribution \\[X_i \\buildrel \\text{iid} \\over \\sim\n\\mathcal{N}(\\mu, 1)\\] for some unknown \\(\\mu\\) that we seek to estimate. Quite reasonably, we might use the sample mean \\[\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\] to estimate \\(\\mu\\). Clearly, this is an unbiased estimator and it has variance given by \\(1/n\\), which isn’t bad. In general, it’s pretty hard to top this.\nWe can verify all of this empirically:\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(dplyr)\n\n\ncompute_mse_sample_mean &lt;- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R &lt;- replicate(1000, {\n        X &lt;- rnorm(n, mean=mu, sd=1)\n        mean(X)\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nMU_GRID &lt;- seq(-5, 5, length.out=501)\nN &lt;- 10\n\nSIMRES &lt;- map(MU_GRID, compute_mse_sample_mean, n=N) |&gt; list_rbind()\n\nOur bias is essentially always zero:\n\nggplot(SIMRES, aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(slope=0, \n                intercept=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Constant Zero Bias of Sample Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nSimilarly, our bias is small, and constant. Specifically, it is around \\(1/n\\) as predicted by standard theory:\n\nggplot(SIMRES, aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Variance of Sample Mean\") + \n    ggtitle(\"Constant Variance of Sample Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nAs expected, the MSE is the sum of bias and variance, so it’s basically just variance here:\n\nggplot(SIMRES, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Sample Mean MSE \") + \n    ggtitle(\"Constant MSE of Sample Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nSo far, it looks like the sample mean is hard to beat. In particular, this curve is But… what if we know, e.g., that \\(\\mu\\) is positive. We might still use the sample mean, but with the additional step that we set it to zero if the sample mean looks negative. That is, our new estimator is \\[\\hat{\\mu} = (\\overline{X}_n)_+ \\text{ where } z_+ = \\begin{cases} z & z &gt; 0 \\\\ 0 & z \\leq 0 \\end{cases}\\]\nThe \\((\\cdot)_+\\) operator is known as the positive-part. How does this \\(\\hat{\\mu}\\) do?\n\npospart &lt;- function(x) ifelse(x &gt; 0, x, 0)\ncompute_mse_positive_mean &lt;- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the positive part of the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R &lt;- replicate(1000, {\n        X &lt;- rnorm(n, mean=mu, sd=1)\n        pospart(mean(X))\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nSIMRES_POSPART &lt;- map(MU_GRID, compute_mse_positive_mean, n=N) |&gt; \n    list_rbind()\n\n\nggplot(SIMRES_POSPART, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nNot surprisingly, we do very poorly if we are estimating a negative \\(\\mu\\) but we assume it is positive. Let’s zoom in on the area near 0 however.\n\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= -0.5, \n           mu &lt;= 1) |&gt;\nggplot(aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nInteresting! For some of these values, we do better than the sample mean.\nIn particular, we do better in the following scenario:\n\nTrue mean is positive\nSample mean is negative\nPositive part of sample mean is zero, so closer than pure sample mean\n\nThe probability of step 2 (sample mean is negative) is near zero for large \\(\\mu\\), but for \\(\\mu\\) in the neighborhood of zero, it can happen.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nReview Question: As a function of \\(\\mu\\), what is the probability that \\(\\overline{X}_n\\) is negative? You can leave your answer in terms of the standard normal CDF \\(\\Phi(\\cdot)\\).\n\n\n\nThis is pretty cool. We have made an additional assumption and, when that assumption holds, it helps us or, worst case, doesn’t really hurt us much. Of course, when the assumption is wrong (\\(\\mu &lt; 0\\)), we do much worse, but we can’t really hold that against \\((\\overline{X}_n)_+\\).\nLooking more closely, we can look at the bias of \\((\\overline{X}_n)_+\\):\n\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= -0.5, \n           mu &lt;= 1) |&gt;\nggplot(aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Bias of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nWe see here that our improvement came at the cost of some bias, particularly in the \\(\\mu \\in [0, 1]\\) range. But for that bias, we see a good reduction in variance:\n\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= -0.5, \n           mu &lt;= 1) |&gt;\nggplot(aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(Variance^2)) + \n    ggtitle(\"Non-Constant Variance of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nHere, we see that the variance is less than \\(1/n\\) from \\(\\mu \\approx 0.5\\) and down. Let’s plot variance and bias against each other:\n\nlibrary(geomtextpath)\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= 0, \n           mu &lt;= 1) |&gt;\n    ggplot(aes(x=bias, y=variance)) + \n       geom_point() + \n       geom_textline(aes(x=bias, y=1/n - bias),\n                     lty=2, color=\"red4\", \n                     label=\"Breakeven\") + \n       ylim(c(0, 0.1)) + \n       theme_bw() + \n       xlab(expression(\"Bias\"^2)) + \n       ylab(\"Variance\") + \n       ggtitle(\"Bias-Variance Tradeoff for Positive Part Sample Mean\")\n\n\n\n\n\n\n\n\nHere, all of the values of \\(\\mu\\) corresponding to points below this line are points where the positive part estimator does better than the standard sample mean.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nSee if you can compute the bias and variance of \\((\\overline{X}_n)_+\\) in closed form. The moments of the Rectified Normal Distribution may be of use.\n\n\n\nOk - now let’s start to generalize. Clearly, the first step is to change the ‘positive’ assumption. The easiest generalization is to restrict \\(\\mu\\) to an interval \\([a, b]\\). In this case, it makes sense to replace the positive part operator with a ‘clamp’ operator:\n\\[(x)_{[a, b]} = \\begin{cases} a & x \\leq a \\\\ x & x\\in(a, b) \\\\ b & x \\geq b \\end{cases}\\]\nThe positive part operator we applied before is \\((x)_+ = (x)_{[0, \\infty)}\\).\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nExtend the simulation above to characterize the estimation performance (bias and variance) of \\((\\overline{X}_n)_[a, b]\\).\n\n\n\nA particularly useful version of this bound is taking \\((\\overline{X}_n)[-\\beta, +\\beta]\\); that is, we don’t know the sign of \\(\\mu\\), but we know it is less than \\(\\beta\\) in magnitude. This is not an improbable assumption - we often have a good sense of the plausible magnitude of a parameter (Bayesian priors anyone?) - but it feels a bit ‘firm’. Can we relax this sort of assumption? We want \\(\\mu\\) to be ‘not too big’, but we’re willing to go big if the data takes us there.\nWe can implement this as follows:\n\\[\\hat{\\mu}_{\\alpha} = \\frac{\\overline{X}_n}{1+\\alpha}\\]\nClearly, setting \\(\\alpha = 0\\) gets us back to the standard sample mean. Can this be better than the sample mean? Let’s do the calculations by hand. First we note that \\(\\E[\\hat{\\mu}_{\\alpha}] = \\frac{\\mu}{1+\\alpha}\\) giving a bias of \\[\\text{Bias} = \\E[\\hat{\\mu}_{\\alpha}] - \\mu = \\mu\\left(1 - \\frac{1}{1+\\alpha}\\right) \\implies \\text{Bias}^2 = \\mu^2\\left(1 - \\frac{1}{1+\\alpha}\\right)^2\\] and\n\\[\\text{Variance} = \\V[\\hat{\\mu}_{\\alpha}] = \\frac{1}{(1+\\alpha)^2}\\V[\\overline{X}_n] = \\frac{1}{n(1+\\alpha)^2}\\]\nso the total MSE is given by\n\\[\\begin{align*}\n\\text{MSE} &= \\E[(\\hat{\\mu}_{\\alpha} - \\mu)^2] \\\\\n           &= \\text{Bias}^2 + \\text{Variance} \\\\\n           &= \\mu^2\\left(1 - \\frac{1}{1+\\alpha}\\right)^2 + \\frac{1}{n(1+\\alpha)^2}\n\\end{align*}\\]\nFor suitable \\(\\alpha, n\\) this can be less than the standard MSE of \\(1/n\\). For instance, at \\(\\mu = 5\\) and \\(n = 10\\),\n\nshrunk_mean_mse &lt;- function(mu, n, alpha){\n    mu^2 * (1 - 1/(1+alpha))^2 + 1/(n * (1+alpha)^2)\n}\n\nshrunk_mean_mse(5, 10, 1e-4)\n\n[1] 0.09998025\n\n\nNot great - but an improvement! It’s actually pretty hard to beat the sample mean with an estimator of this form in the univariate case, but it can be incredibly useful in more general settings."
  },
  {
    "objectID": "notes/notes03.html#james-stein-estimation-of-multivariate-normal-means",
    "href": "notes/notes03.html#james-stein-estimation-of-multivariate-normal-means",
    "title": "STA 9890 - Regularized Regression",
    "section": "James-Stein Estimation of Multivariate Normal Means",
    "text": "James-Stein Estimation of Multivariate Normal Means\nTODO"
  },
  {
    "objectID": "notes/notes03.html#ridge-regression",
    "href": "notes/notes03.html#ridge-regression",
    "title": "STA 9890 - Regularized Regression",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nAbove, we saw that \\((\\overline{X}_n)_{[-\\beta, \\beta]}\\) could outperform \\(\\overline{X}_n\\) if the true parameter is ‘not too big’. Can we extend this idea to regression?\nRecall that we formulated OLS as:\n\\[\\hat{\\bbeta}_{\\text{OLS}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|\\]\nWe can apply a ‘clamp’ as:\n\\[\\hat{\\bbeta}_{\\tau-\\text{Clamped}} = \\argmin_{\\bbeta \\in \\R: \\|\\bbeta\\| &lt; \\tau} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\| \\leq {\\tau}\\]\nHere, we have some choice in measuring the ‘size’ of \\(\\hat{\\bbeta}\\): in fact, we can theoretically use any of our \\(\\ell_p\\)-norms. As with the squared loss, it turns out to be mathematically easiest to start with the \\(\\ell_2\\) (Euclidean) norm. This gives us the ridge regression problem:\n\\[\\hat{\\bbeta}_{\\text{Ridge}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2 \\leq {\\tau} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2^2 \\leq {\\tau}^2\\]\nBefore we discuss solving this, let’s confirm it is convex. Recall that for a problem to be convex, it needs to have a convex objective and a convex feasible set. Clearly the objective here is convex - we still have the MSE loss from OLS unchanged. So now we need to convince ourselves that \\(\\|\\bbeta\\|_2^2 \\leq \\tau^2\\) defines a convex set. We can look at this in 2D first:\n\\[\\|\\bbeta\\|_2^2 \\leq \\tau^2 \\implies \\beta_1^2 + \\beta_2^2 \\leq \\tau\\]\nBut this is just the equation defining a circle and its interior (in math speak a 2D ‘ball’) so it has to be convex!\nIn fact, constraints of the form\n\\[\\|\\bbeta\\|_p \\leq \\tau \\Leftrightarrow \\|\\bbeta\\|_p^2 \\leq \\tau^p\\]\nare convex for all \\(\\ell_p\\) norms (\\(p \\geq 1\\)). We will use this fact many times in this course. The sets that this constraint defines are called \\(\\ell_p\\) ‘balls’ by analogy with the \\(\\ell_2\\) figure.\n\n\n\n\n\n\n\n\n\nSo our ridge problem\n\\[\\hat{\\bbeta}_{\\text{Ridge}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2^2 \\leq {\\tau}^2\\] is indeed convex. While we can solve this without too much trouble, it turns out to be easier to use an optimization trick known as Lagrange Multipliers to change this to something easier to solve.\nThe Method of Lagrange Multipliers lets us turn constrained optimization problems that look like:\n\\[\\argmin_x f(x) \\text{ such that } g(x) \\leq \\tau\\]\ninto unconstrained problems like:\n\\[\\argmin_x f(x) + \\lambda g(x)\\]\nHere, the constant \\(\\lambda\\) is known as the Lagrange multiplier. Instead of forcing \\(g(x)\\) to be bounded, we simply penalize any \\(x\\) that makes \\(g(\\cdot)\\) large. For large enough penalties (large \\(\\lambda\\)), we will eventually force \\(g(x)\\) to be small enough that we satisfy the original constraint. In fact, there is a one-to-one relationship between \\(\\tau\\) and \\(\\lambda\\), but it’s usually not possible to work it out in any meaningful useful manner; all we can say is that larger \\(\\lambda\\) correspond to smaller \\(\\tau\\).\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhy is this true? Why does small \\(\\tau\\) imply large \\(\\lambda\\) and vice versa?\n\n\n\nIn this form, it’s straightforward to pose ridge regression as:\n\\[\\argmin_{\\beta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 + \\frac{\\lambda}{2}\\|\\bbeta\\|_2^2\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhy were we allowed to put an extra \\(\\frac{1}{2}\\) in front of the penalty term?\n\n\n\nYou will show in Report #01 that the solution to this problem is given by:\n\\[\\hat{\\bbeta}_{\\lambda} = (\\bX^{\\top}\\bX + \\lambda \\bI)^{-1}\\bX^{\\top}\\by\\]\nThere are several important things to note about this expression:\n\nThere are actually many ridge regression solutions, one for each value of \\(\\lambda\\). It is a bit improper to refer to “the” ridge regression solution. We will discuss selecting \\(\\lambda\\) below.\nIf you are a bit sloppy, this looks something like:\n\\[\\hat{\\beta} \\approx \\frac{SS_{XY}}{SS_{XX} + \\lambda}\\]\nso we ‘shrink’ the standard OLS solution towards zero, by an amount depending on \\(\\lambda\\).\nThe penalized form (as opposed to the constraint form) allows for \\(\\hat{\\beta}\\) to be arbitrarily large, if the data supports it.\nIf we set \\(\\lambda = 0\\), we recover standard OLS.\nUnlike the OLS solution, the ridge regression exists even when \\(p &gt; n\\).\n\nWhile this isn’t as simple to analyze as some of the expressions we considered above, this looks similar enough to our ‘shrunk’ mean estimator that it’s plausible it will improve on OLS. In fact, you will show in Report #01 that there is always some value of \\(\\lambda\\) that guarantees improvement over OLS.\nTo wit,\n\nn &lt;- 100\np &lt;- 80\nZ &lt;- matrix(rnorm(n * p), nrow=n)\nL &lt;- chol(toeplitz(0.6^(1:p))) # Add 'AR(1)' correlation\nX &lt;- Z %*% L\nbeta &lt;- runif(p, 2, 3)\n\neye &lt;- function(n) diag(1, n, n)\ncalculate_ridge_mse &lt;- function(lambda, nreps = 1000){\n    MSE &lt;- mean(replicate(nreps, {\n        y &lt;- X %*% beta + rnorm(n, sd=1)\n        beta_hat &lt;- solve(crossprod(X) + lambda * eye(p), crossprod(X, y))\n        sum((beta - beta_hat)^2)\n    }))\n    \n    data.frame(lambda=lambda, MSE=MSE)\n}\n\nlambda_grid &lt;- 10^(seq(-2, 2, length.out=41))\n\nRIDGE_MSE &lt;- map(lambda_grid, calculate_ridge_mse) |&gt; list_rbind()\nOLS_MSE &lt;- calculate_ridge_mse(0)$MSE # Ridge at lambda = 0 =&gt; OLS\n\nggplot(RIDGE_MSE, aes(x=lambda, y=MSE)) + \n    geom_point() + \n    geom_line() + \n    geom_abline(intercept=OLS_MSE, slope=0, lwd=2, lty=2) + \n    xlab(expression(lambda)) + \n    ylab(\"Estimation Error (MSE)\") + \n    ggtitle(\"Ridge Regularization Improves on OLS\") + \n    theme_bw() + \n    scale_x_log10()\n\n\n\n\n\n\n\n\nClearly, for smallish \\(\\lambda\\), we are actually outperforming OLS, sometimes by a significant amount! For this problem, the minimum MSE (best estimate) actually seems to occur near \\(\\lambda \\approx\\) 1.585\n\nModel Selection\nWhile theory and our experiment above tell us that there is some value of \\(\\lambda\\) that beats OLS, how can we actually find it? We can’t do a simulation like above as we had to know \\(\\bbeta_*\\) to compute the MSE and the theory is non-constructive. (In particular, we have to know \\(\\bbeta_*\\) to compute the bias.)\nWell, we can go back to our overarching goal: we want results that generalize to unseen data. Here, in particular, we want results that predict well on unseen data. So let’s just do that.\nWe can split our data into a training set and a validation set. The training set is like we have already seen, used to estimate the regression coefficients \\(\\bbeta_{\\text{Ridge}}\\); the validation set may be new. It is used to compute the MSE on ‘pseudo-new’ data and we then can select the best predicting value of \\(\\lambda\\). In this case, this looks something like the following:\n\n# Continuing with X, beta from above\ny &lt;- X %*% beta + rnorm(n, sd = 1)\n\nTRAIN_IX &lt;- sample(n, 0.8 * n)\nVALID_IX &lt;- setdiff(seq(n), TRAIN_IX) # Other elements\n\nTRAIN_X &lt;- X[TRAIN_IX, ]\nTRAIN_Y &lt;- y[TRAIN_IX]\n\nVALID_X &lt;- X[VALID_IX, ]\nVALID_Y &lt;- y[VALID_IX]\n\ncompute_validation_error &lt;- function(lambda){\n    beta_hat_rr &lt;- solve(crossprod(TRAIN_X) + lambda * eye(p), \n                         crossprod(TRAIN_X, TRAIN_Y))\n    \n    y_pred &lt;- VALID_X %*% beta_hat_rr\n    \n    data.frame(lambda = lambda, \n               validation_mse = mean((y_pred - VALID_Y)^2))\n}\n\nvalidation_error &lt;- map(lambda_grid, compute_validation_error) |&gt; list_rbind()\n\nggplot(validation_error, \n       aes(x = lambda, \n           y = validation_mse)) + \n    geom_point() + \n    xlab(expression(lambda)) + \n    ylab(\"MSE on Validation Set\") + \n    ggtitle(\"Hold-Out Tuning of Ridge Regression Regularization Parameter\") + \n    theme_bw() + \n    scale_x_log10()\n\n\n\n\n\n\n\n\nOur results here aren’t quite as definitive as when we knew the true \\(\\bbeta_*\\), but they suggest we want to take \\(\\lambda \\approx\\) 0.398 which isn’t too far from what we found above.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nDoes this procedure give an unbiased estimate of the true test error? Why or why not?\n\n\n\nThere are some limitations to this basic procedure - randomness and (arguably) inefficient data usage - that we will address later, but this gets us started."
  },
  {
    "objectID": "notes/notes03.html#lasso-regression",
    "href": "notes/notes03.html#lasso-regression",
    "title": "STA 9890 - Regularized Regression",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nRidge regression is a rather remarkable solution to the problem of ‘inflated’ regression coefficients. But let’s remind ourselves why we find inflated coefficients in the first place.\nA common cause of overly large regression coefficients is correlation among the columns of \\(\\bX\\). For example,\n\nmax_beta_sim &lt;- function(rho, n, p){\n    R &lt;- replicate(100, {\n        Z &lt;- matrix(rnorm(n * p), nrow=n)\n        L &lt;- chol(toeplitz(rho^(1:p)))\n        X &lt;- Z %*% L\n        beta &lt;- runif(p, 2, 3)\n        y &lt;- X %*% beta + rnorm(n)\n        \n        beta_hat &lt;- coef(lm(y ~ X))\n        max(abs(beta_hat))\n    })\n    data.frame(beta_max = max(R),\n               n = n, \n               p = p, \n               rho = rho)\n}\n\nRHO_GRID &lt;- seq(0.5, 0.999, length.out=101)\n\nBETAS &lt;- map(RHO_GRID, max_beta_sim, n = 40, p = 35) |&gt; list_rbind()\n\nggplot(BETAS, \n       aes(x = rho, \n           y = beta_max))+ \n    geom_point() + \n    scale_y_log10() + \n    theme_bw() + \n    xlab(\"Feature Correlation\") + \n    ylab(\"Maximum Observed Regression Coefficient\") + \n    geom_abline(intercept = 3, color=\"red4\", lwd=2, lty=2)\n\n\n\n\n\n\n\n\nThis simulation is maybe a bit unfair to OLS since we are taking maxima instead of averages or similar, but the message is certainly clear. As the columns of \\(\\bX\\) become highly correlated, we get larger values of \\(\\hat{\\beta}\\). Because OLS is unbiased here, this is fundamentally a story of variance: as we have more features,\n\\[\\V[\\hat{\\beta}] \\propto (\\bX^{\\top}\\bX)^{-1}\\]\nincreases, making it likely that we see increasingly wild values of \\(\\hat{\\beta}\\). (Recall that we generated our data so that \\(\\beta_*\\) was never larger than 3, as noted in red above.)\nWhy does this happen? It’s maybe a bit tricky to explain, but with highly correlated features the model can’t really distinguish them well, so it ‘overreacts’ to noise and produces very large swings (variance) in response to small changes in the training data. Like above, we can hope to tame some of this variance if we take on a bit of bias.\nAnother way we might want to ‘calm things down’ and improve on OLS is by having fewer features in our model. This reduces the chance of ‘accidental correlation’ (more features leads to more ‘just because’ correlations) and gives us a more interpretable model overall. (More about interpretability below)\n\nBest Subsets\nYou have likely already seen some sort of ‘variable selection’ procedures in prior courses: things like forward stepwise, backwards stepwise, or even hybrid stepwise. All of these are trying to get at the idea of only including variables that are ‘worth it’; in other settings, you may also see them used as finding the ‘most valuable’ variables. Let’s use our newfound knowledge of optimization to formalize this.\nThe best-subsets problem is defined by:\n\\[\\argmin_{\\beta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 \\text{ such that } \\|\\bbeta\\|_0 \\leq k\\]\nThis looks similar to our ridge regression problem, but now we’ve replaced the \\(\\ell_2\\)-norm constraint with the \\(\\ell_0\\)-‘norm’. Recall that the \\(\\ell_0\\)-‘norm’ is the number of non-zero elements in \\(\\bbeta\\) and that it’s not a real norm. So this says, find the minimum MSE \\(\\beta\\) with at most \\(k\\) non-zero elements. Since more features always reduces (or at least never increases) training MSE, we can assume that this problem will essentially always pick the best combination of \\(k\\) variables.\nAlternatively, in penalized form, we might write:\n\\[\\argmin_{\\beta} \\|\\by - \\bX\\bbeta\\|_2^2 + \\lambda \\|\\bbeta\\|_0\\]\nHere, we only introduce a new variable if including it lowers our training MSE by \\(\\lambda\\): we’ve implicitly set \\(\\lambda\\) as our ‘worth it’ threshold for variables.\nNote that these problems require finding the best combination of variables. The most individually useful variables might not be useful in combination: e.g., someone’s right shoe size might be very useful for predicting their height, and their left shoe size might be very useful for predicting their height, and their gender might be a little bit useful for predicting their height, but you’ll do better with right shoe and gender than with right and left shoe.\nBecause we’re checking all combinations of variables, this is a so-called ‘combinatorial’ optimization problem. These are generally quite hard to solve and require specialized algorithms (unlike our general purpose convex optimization algorithms). A naive approach of ‘try all models’ becomes impractical quickly. If we have \\(p=30\\) potential features, that’s \\(2^30\\), or just over a billion, possible models we would need to check. Even if we can check one model every minute, that takes about 2040 years to check them all; if we have a computer do the work for us and it’s faster, say 1000 models per minute, we still need over 2 years. And that’s just for 30 features! In realistic problems where \\(p\\) can be in the hundreds or thousands, we have no chance.\nThankfully, smart people have worked on this problem for us and found that for \\(p \\approx 100 - 1000\\), very fancy software can solve this problem.1 But that still leaves us very far we want to go…\nTODO:\n\nStepwise as approximate algorithm\nConvex relaxation"
  },
  {
    "objectID": "notes/notes03.html#model-selection",
    "href": "notes/notes03.html#model-selection",
    "title": "STA 9890 - Regularized Regression",
    "section": "Model Selection",
    "text": "Model Selection"
  },
  {
    "objectID": "notes/notes03.html#the-lasso",
    "href": "notes/notes03.html#the-lasso",
    "title": "STA 9890 - Regularized Regression",
    "section": "The Lasso",
    "text": "The Lasso\nTODO ### Model Selection TODO"
  },
  {
    "objectID": "notes/notes03.html#footnotes",
    "href": "notes/notes03.html#footnotes",
    "title": "STA 9890 - Regularized Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Best subset selection via a modern optimization lens” by Dimitris Bertsimas, Angela King, Rahul Mazumder Annals of Statististics 44(2): 813-852 (April 2016). DOI: 10.1214/15-AOS1388↩︎"
  },
  {
    "objectID": "resources.html#additional-baruch-and-cuny-benefits",
    "href": "resources.html#additional-baruch-and-cuny-benefits",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Additional Baruch and CUNY Benefits",
    "text": "Additional Baruch and CUNY Benefits\nAs Baruch students, you have access to a variety of cultural and education benefits including:\n\nFree New York Times subscription\nFree Wall Street Journal subscription\nFree Barron’s Subscription (Zicklin only)\nFree and Discounted Museum Admissions\nDiscounted Carnegie Hall performances\nGitHub Student Developer Pack\nFree and Discounted Amazon Prime\n\namong many others.\nMore will posted as I discover them."
  },
  {
    "objectID": "notes/notes04.html",
    "href": "notes/notes04.html",
    "title": "STA 9890 - Non-Linear Regression",
    "section": "",
    "text": "\\[\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}} \\newcommand{\\V}{\\mathbb{V}} \\newcommand{\\argmin}{\\text{arg min}} \\newcommand{\\K}{\\mathbb{K}}\\]\nThis week, we continue our discussion of ‘fancy’ regression from last week to consider non-linear models. While we introduce several methods for non-linear regression, we also pause to consider when non-linear methods can actually be expected to outperform linear methods.\nAlso note that we are arguably being a bit ‘sloppy’ about what we mean by ‘linear’ methods. For purposes of these notes, we define a ‘linear’ method as one where the fitted model has a linear response to changes in the inputs; formally, one where\n\\[\\frac{\\partial y}{\\partial x_i} = \\beta_i \\text{ (a constant) for all } i \\in \\{1, \\dots, p\\}\\]\nThis definition includes things like best-subsets and lasso regression, which are not linear functions of \\(\\by\\), but excludes things like polynomial regression.1 In the above definition, each \\(\\beta_i\\) is essentially a regression coefficient, and values of \\(i\\) where \\(\\beta_i = 0\\) are non-selected variables if we are using a sparse method."
  },
  {
    "objectID": "notes/notes04.html#footnotes",
    "href": "notes/notes04.html#footnotes",
    "title": "STA 9890 - Non-Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn other classes, you may have been taught that polynomial regression is a linear model. It is definitely linear in the sense that that class would have used (linear in \\(\\by\\)) but we’re using a different sense here. The core insight of that analysis - that a non-linear function can be expressed as a linear combination of non-linear parts - will appear several times in these notes.↩︎\nIn Python, the statsmodels package also implements LOESS fits.↩︎\nIf you want to go much deeper into the practical use of additive (spline) models, Simon Wood’s book Generalized Additive Models is fantastic.↩︎"
  },
  {
    "objectID": "notes/notes04.html#non-linearity-via-linear-combination-of-non-linear-parts",
    "href": "notes/notes04.html#non-linearity-via-linear-combination-of-non-linear-parts",
    "title": "STA 9890 - Non-Linear Regression",
    "section": "Non-Linearity via Linear Combination of Non-Linear Parts",
    "text": "Non-Linearity via Linear Combination of Non-Linear Parts\nWe have spent the previous few weeks developing a set of tools for building linear models of \\(\\by\\). Specifically, we have sought ways to approximate \\(\\E[y | \\bx]\\) as a weighted sum of each \\(x_i\\):\n\\[ \\E[y | \\bx] \\approx \\langle \\bx, \\hat{\\bbeta} \\rangle = \\sum_{i=1}^p x_i \\hat{\\beta}_i\\]\nFor situations where the relationship is nearly (or even truly) linear, this approximation can be quite useful and good estimates of the \\(\\beta_i\\) coefficients can lead to accurate estimates of \\(\\E[y | \\bx]\\), and ultimately accurate prediction of test set response \\(\\by\\). Using the bias-variance decomposition, we have seen that it is often worthwhile to accept a bit of bias in estimation of each \\(\\beta_i\\) if it is compensated by a worthwhile reduction in the variance of \\(\\beta_i\\).\nWe can further refine this equality by decomposing bias a bit further:\n\\[\\text{MSE} = \\text{Irreducible Error} + \\text{Variance} + \\text{Model Bias}^2 + \\text{Estimation Bias}^2\\]\nHere, we have decomposed \\(\\text{Bias}^2\\) into two terms:\n\n\\(\\text{Model Bias}^2\\): a measure of the systematic error arising from use of a linear model to predict a non-linear DGP\n\\(\\text{Estimation Bias}^2\\): a measure of the systematic error arising from use of a regularized estimation procedure which exhibits shrinkage\n\nPut another way, “Model Bias” results from use of a linear model when something non-linear should be used, while “Estimation Bias” arises from the use of a biased method (like ridge regression) instead of something nominally unbiased like OLS. For the previous two weeks, we have mainly focused on linear models for linear DGPs, so model bias has been zero. If we expand our gaze to non-linear DGPs, we have to deal with model bias a bit more directly. As with estimation bias, it is frequently worthwhile to accept model bias to reduce variance; see more discussion below.\n\nPolynomial Expansion\nYou have likely already seen polynomial expansion (or polynomial regression) in previous courses. Essentially, PE fits a low(-ish)-order polynomial instead of a line to data. More formally, let \\(f(\\bx) \\equiv \\E[y|\\bx]\\) be the regression function (best possible predictor) that week seek to approximate from noisy observations. Standard linear regression essentially seeks to fit a first-order Taylor approximation of \\(f\\) around some point \\(\\overline{\\bx}\\):\n\\[\\begin{align*}\nf(\\bx) &\\approx f(\\overline{\\bx}) + \\nabla_f(\\bx_0)^{\\top}(\\bx - \\overline{\\bx}) \\\\\n       &= f(\\overline{\\bx}) + \\sum_{i=1}^p \\left. \\frac{\\partial f}{\\partial x_i}\\right|_{x = \\overline{x}_i}(x_i - \\overline{x}_i)\n\\end{align*}\\]\nIf we rearrange this a bit, we get\n\\[\\begin{align*}\nf(\\bx) &\\approx f(\\overline{\\bx}) + \\sum_{i=1}^p \\left. \\frac{\\partial f}{\\partial x_i}\\right|_{x = \\overline{x}_i}(x_i - \\overline{x}_i) \\\\\n&= \\underbrace{\\left(f(\\overline{\\bx}) - \\sum_{i=1}^p \\left.\\frac{\\partial f}{\\partial x_i}\\right|_{x_i=\\overline{x}_i}\\overline{x}_i\\right)}_{=\\beta_0} + \\sum_{i=1}^p \\underbrace{\\left.\\frac{\\partial f}{\\partial x_i}\\right|_{x_i=\\overline{x}_i}}_{=\\beta_i} x_i\\\\\n&= \\beta_0 + \\sum_{i=1}^p x_i \\beta_i\n\\end{align*}\\]\nso we see that the regression coefficients are more-or-less the (suitably-averaged) partial derivatives of \\(f\\) while the intercept is the value of \\(f\\) at the center of our expansion (\\(f(\\overline{\\bx})\\)) plus some differential adjustments. Note that, if the variables \\(\\bX\\) are centered so that \\(\\E[\\bX] = \\mathbf{0}\\), the intercept is exactly the average value of \\(f\\), as we would expect.\nClearly, this linear approximation will do better in the situations where Taylor expansions generally perform better: when i) the derivative of \\(f\\) is roughly constant and ii) the higher order terms are small. This makes sense: if \\(f\\) is very-close-to-linear, there is minimal loss from fitting a linear approximation to \\(f\\); if \\(f\\) is very non-linear, however, a linear approximation can only be be so good, and we are saddled with significant model bias.\nIf we want to mitigate some of that model bias, we can choose to use a higher-order Taylor series. If we examine a second order Taylor series, we get a similar approximation as before, where \\(\\mathcal{H}\\) denotes the Hessian matrix of second derivatives:\n\\[\\begin{align*}\nf(\\bx) &\\approx f(\\overline{\\bx}) + \\nabla_f(\\bx_0)^{\\top}(\\bx - \\overline{\\bx}) + \\frac{1}{2}(\\bx - \\overline{\\bx})^{\\top}\\mathcal{H}_f(\\overline{\\bx})(\\bx - \\overline{\\bx})^{\\top}\n\\end{align*}\\]\nAfter some rearrangement this becomes:\n\\[f(\\bx) \\approx \\beta_0 + \\sum_{i=1}^p \\frac{\\partial f}{\\partial x_i} x_i +\\sum_{i=1}^p \\frac{\\partial^2 f}{\\partial x_i^2} x_i^2 +  \\sum_{\\substack{i, j = 1 \\\\i \\neq j}}^p \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} x_ix_j\\]\nWe recognize the constant term (intercept) and first order terms (linear slopes) from before, but now we have added two types:\n\nSecond order terms (\\(x_i^2\\)): these capture higher order non-linearities in a single variable. For example, if \\(y = x^2\\), then the ‘true fit’ really lies in the second order term \\(\\partial^2f / \\partial x^2 = 2\\) instead of any linear approximation\nCross terms (\\(x_ix_j\\)): these capture non-linear (non-additive) multi-variate relationships between features. You may have seen these before as interaction terms.\n\nRules of thumb vary as to which set of terms are more important. Historically, statisticians have tended to put in the interaction (cross) terms first, though this is far from universal practice. In particular, when dealing with ‘binary’ variables, the higher order term is unhelpful: if a feature is \\(0/1\\) (did the patient receive the drug or not?), adding a squared term has no effect since \\(0^2 = 0\\) and \\(1^2 = 1\\).\nRegardless, this is why we sometimes use ‘polynomial expansion’ of our original variables. We are trying to capture the higher-order (non-linear) terms of the Taylor approximation.\nIt is worth thinking about what happens when we add higher-order terms to our model. In particular, note that we i) pick up some correlation among features; and ii) we will generally have more variance since we have more features. In practice, the correlation isn’t too much of a problem and the actual polynomials fit by, e.g., poly in R are modified to remove correlation. The variance however can be a more significant problem.\nLet’s see this in action. Let’s first consider fitting polynomial regression to a simple (univariate) non-linear function: \\(f(x) = \\sqrt{|x|^3 + 5} * \\cos(x)\\)\n\ncurve(sqrt(abs(x)^3 + 5) * cos(x), from=1, to=10)\n\n\n\n\n\n\n\n\nThis is not too non-linear. It’s still very smooth (analytic in the jargon) and should be relatively easy to fit. Let’s also generate some noisy observations of this function.\n\nn &lt;- 25\n# Linear regression doesn't care about the order of our data, but plotting does\n# so sorting gives better visuals\nx &lt;- sort(runif(n, 2, 10)) \nEy &lt;- sqrt(abs(x)^3 + 5) * cos(x)\ny &lt;- Ey + rnorm(n, sd=3)\n\n\nx_grid &lt;- seq(min(x), max(x), length.out=501)\nEy_grid &lt;- sqrt(abs(x_grid)^3 + 5) * cos(x_grid)\n\nplot(x, y)\nlines(x_grid, Ey_grid)\n\n\n\n\n\n\n\n\nWe can now fit some linear models to this data (shown in color): we use R’s built-in poly function to automatically create the polynomials we seek. You can think of poly(x, k) as creating k features \\(x^1, x^2,\n\\dots, x^k\\) but it actually does something a bit more subtle under the hood to make model fitting a bit more numerically stable.\n\nplot(x, y)\nlines(x_grid, Ey_grid)\n\nfor(k in 1:5){\n    m &lt;- lm(y ~ poly(x, k))\n    \n    yhat &lt;- predict(m, data.frame(x = x_grid))\n    lines(x_grid, yhat, col=k+1)\n}\n\nlegend(\"bottomleft\",\n    c(\"True Regression Function\", \n         \"First Order (Linear) Fit\",\n         \"Second Order (Quadratic) Fit\", \n         \"Third Order (Cubic) Fit\", \n         \"Fourth Order (Quartic) Fit\",\n         \"Fifth Order (Quintic) Fit\"), \n       col=1:6, lty=1)\n\n\n\n\n\n\n\n\nClearly, as we increase the order of the polynomial, we get a better (in-sample) fit. This makes sense: we know that more features gives us a better fit all else being equal, so what’s the issue?\nAs usual, the issue occurs for extrapolation. You can actually see some issues already beginning to manifest at the ends of our prediction intervals: the higher-order polynomials go radically different directions as we extrapolate and even the ‘best fit’ (quintic) looks like it’s going to be too steep as we go further.\nLet’s expand the prediction region for these fits:\n\nx_pred &lt;- seq(-5, 15, length.out=501)\nEy_pred &lt;- sqrt(abs(x_pred)^3 + 5) * cos(x_pred)\nplot(x_pred, Ey_pred, type=\"l\")\npoints(x, y)\n\nfor(k in 1:5){\n    m &lt;- lm(y ~ poly(x, k))\n    yhat &lt;- predict(m, data.frame(x=x_pred))\n    lines(x_pred, yhat, col=k+1)\n}\n\nlegend(\"bottomleft\",\n    c(\"True Regression Function\", \n         \"First Order (Linear) Fit\",\n         \"Second Order (Quadratic) Fit\", \n         \"Third Order (Cubic) Fit\", \n         \"Fourth Order (Quartic) Fit\",\n         \"Fifth Order (Quintic) Fit\"), \n       col=1:6, lty=1)\n\n\n\n\n\n\n\n\nThere’s some problems!\nTo be fair, it is a bit unreasonable to expect these models to perform too well outside of our original sampling area. What’s more worrying is that the extrapolations don’t just miss the curves of the true regression function, they actually create their own even more intense wiggles.\nWe can also manifest this without extrapolation by fitting very high-order polynomials:\n\nk &lt;- 9\nm &lt;- lm(y ~ poly(x, degree=10 + k, raw=TRUE))\nyhat &lt;- predict(m, data.frame(x = x_grid))\n\n\nplot(x, y)\nlines(x_grid, Ey_grid)\n\nfor(k in 1:5){\n    m &lt;- lm(y ~ poly(x, degree=14 + k, raw=TRUE))\n    yhat &lt;- predict(m, data.frame(x = x_grid))\n    lines(x_grid, yhat, col=k)\n}\n\n\n\n\n\n\n\n\nNote that we have to use raw=TRUE here to avoid R complaining about having too-high a degree in the polynomial. (R gives an error about “unique points” but the real issue is one about matrix rank, not ties in x.) I’m also surpressing some warnings here about a sub-standard fit.\nSo how can we address this ‘over-wiggliness’?\nAs we discussed last time, we need to use a different set of functions: one that is smooth (like polynomials), but not too-high order. It would also be really nice if we could get something ‘adaptive’ - allowing for more wiggliness where we have more data (and we need more wiggliness) and less wiggliness where we don’t have enough data (fall-back to linearity). ### Local Linear Models\nOne way to do this is the idea of “local linear (polynomial) models.” Instead of fitting a single (global) line, we can fit different lines in different parts of our data: that way, we can get an upward line when the true (non-linear) relationship is increasing and a downward line when the true relationship is decreasing. Or at least that’s the hope!\n\nplot(x, y)\nlines(x_grid, Ey_grid)\n\n# Divide our data into five 'buckets' and fit sub-models\n# This works because we've already sorted our data\n# (What would happen if we hadn't?)\nfor(k in 1:5){\n    xk &lt;- x[(5 * k - 4):(5*k)]\n    yk &lt;- y[(5 * k - 4):(5*k)]\n    mk &lt;- lm(yk ~ xk)\n    y_hat &lt;- predict(mk)\n    lines(xk, y_hat, col=\"red4\")\n}\n\n\n\n\n\n\n\n\nDefinitely some rough edges here - particularly in the areas between our buckets, but we’re on a good path. The locfit package will help us with the details:\n\nlibrary(locfit)\n\nlocfit 1.5-9.11      2025-01-27\n\nm &lt;- (locfit(y ~ lp(x)))\nsummary(m)\n\nEstimation type: Local Regression \n\nCall:\nlocfit(formula = y ~ lp(x))\n\nNumber of data points:  25 \nIndependent variables:  x \nEvaluation structure: Rectangular Tree \nNumber of evaluation points:  7 \nDegree of fit:  2 \nFitted Degrees of Freedom:  5.194 \n\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat &lt;- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n\n\n\n\n\n\n\n\nNot perfect, but pretty nice compared to what we had before.\nYou can also see a discussion of the “degrees of freedom” in the model output. This is not exactly the DoF you learned in earlier classes, but it’s sort of “morally equivalent.” This local fit is about as flexible as a 4th degree polynomial would be for this problem. Even though this model is made out of quadratics, it’s more flexible than a single quadratic. But we avoid the extreme variability associated with a polynomial of that high order. Win-win!\nWe can tweak some parameters of the local fit to get different responses: the big ones are the degree (deg) and the number of neighbors to use.\n\n?lp\n\n\nm &lt;- (locfit(y ~ lp(x, deg=4)))\nsummary(m)\n\nEstimation type: Local Regression \n\nCall:\nlocfit(formula = y ~ lp(x, deg = 4))\n\nNumber of data points:  25 \nIndependent variables:  x \nEvaluation structure: Rectangular Tree \nNumber of evaluation points:  7 \nDegree of fit:  4 \nFitted Degrees of Freedom:  6.986 \n\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat &lt;- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n\n\n\n\n\n\n\n\nPretty nice. If we ‘turn down’ the number of neighbors used:\n\nm &lt;- (locfit(y ~ lp(x, nn=0.2, deg=4)))\n\nWarning in lfproc(x, y, weights = weights, cens = cens, base = base, geth =\ngeth, : Estimated rdf &lt; 1.0; not estimating variance\n\nsummary(m)\n\nEstimation type: Local Regression \n\nCall:\nlocfit(formula = y ~ lp(x, nn = 0.2, deg = 4))\n\nNumber of data points:  25 \nIndependent variables:  x \nEvaluation structure: Rectangular Tree \nNumber of evaluation points:  27 \nDegree of fit:  4 \nFitted Degrees of Freedom:  24.257 \n\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat &lt;- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n\n\n\n\n\n\n\n\nToo far - super crazy! But not entirely unexpected - we know that \\(K\\)-Nearest Neighbors for small \\(K\\) has a huge variance.\nToo far!\nThe loess function in base R does this very nicely as well without requiring additional packages. For practical work, it’s a very nice tool for univariate modeling.2\n\nm &lt;- loess(y ~ x)\nsummary(m)\n\nCall:\nloess(formula = y ~ x)\n\nNumber of Observations: 25 \nEquivalent Number of Parameters: 4.64 \nResidual Standard Error: 4.098 \nTrace of smoother matrix: 5.1  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat &lt;- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n\n\n\n\n\n\n\n\n\n\nSpline Regression (Additive Models)\n\n\nSplines\nWe can generalize this idea a bit using splines. Splines are functions that solve a penalized approximation problem:\n\\[\\hat{f} = \\text{arg min}_{f} \\frac{1}{2n} \\|y - f(x)\\|_2^2 + \\lambda \\int |f''(x)|^2 \\text{d}x\\]\nHere we are saying we’ll take any function (not just a polynomial) that achieves the optimal trade-off between data fit (first / loss term) and not being too rough (second / penalty term).\nWhy is this the right penalty to use? In essence, we are trying to penalize ‘deviation from linearity’ and since linear functions have second derivative 0 (by definition) the integral of the second derivative gives us a measure of non-linearity.\nIn some remarkable work, Grace Wahba and co-authors showed that the solutions to that optimization problem are piecewise polynomials with a few additional constraints - these functions are splines. In addition to piecewise polynomialness, splines also guarantee:\n\ncontinuity of the function and its derivatives\nlinearity outside the range of data fit (‘natural splines’)\n\nBecause splines not only match the value of the function at the knots (places where the ‘pieces’ match up) but also the derivatives, they are very smooth indeed.\nStepping back, splines are just a different sort of feature engineering. Instead of using polynomial basis functions (\\(x^1, x^2, \\dots, x^k\\)), splines use a much smoother basis and hence give smoother results, avoiding the ‘wiggles’ problem we saw above.\nR provides the basic tools for spline fitting in the splines package. For real work, you almost surely want to use the advanced functionality of the mgcv package. For more on splines and models using them (‘additive models’), see this online course.\nWe can see that individual splines are quite nice little functions and usually are only non-zero on small regions:\n\nlibrary(splines)\n\nnsx &lt;- ns(x_grid, df=5) # The natural spline basis\nplot(x_grid, apply(nsx, 1, max), ylim=range(nsx), type=\"n\")\n\nfor(i in 1:5){\n    lines(x_grid, nsx[,i], col=i)\n}\n\n\n\n\n\n\n\n\nWe can use linear combinations of these ‘bumps’ to fit non-linear functions, including our example from above:\n\nm &lt;- lm(y ~ ns(x, df=5))\nplot(x, y)\nlines(x_grid, Ey_grid)\ny_hat &lt;- predict(m, data.frame(x = x_grid))\nlines(x_grid, y_hat, col=\"red4\")\n\n\n\n\n\n\n\n\n(This basically matches what we had before with \\(\\text{DoF} \\approx 4.7\\), but you can get different answers by messing with the df parameter here.)\nOur predicted response is actually the sum of the individual spline effects:\n\nm &lt;- lm(y ~ ns(x, df=5))\ny_hat &lt;- predict(m, data.frame(x = x_grid))\n\nplot(x, y, lwd=2)\nlines(x_grid, Ey_grid)\nfor(i in 1:5){\n    lines(x_grid, ns(x_grid, df=5)[,i] * coef(m)[-1][i], col=i+1)\n}\n\n\n\n\n\n\n\n\nIf we were able to sum up the colored lines, we would get the black line back.\nWe can also see that natural splines give smooth predictions outside the range of the original data:\n\ny_pred &lt;- predict(m, data.frame(x = x_pred))\n\nplot(x_pred, Ey_pred, type=\"l\")\nlines(x_pred, y_pred, col=\"red4\")\n\n\n\n\n\n\n\n\nAdmittedly, not ideal, but it will do. In particular, note that outside of the ‘data region’, we fall back on a nice comfortable\n(You can repeat this process with the slightly more common \\(b\\)-splines, but the differences aren’t huge.)\n\nm &lt;- lm(y ~ bs(x, df=5))\ny_hat &lt;- predict(m, data.frame(x = x_grid))\n\nplot(x, y, xlim=range(x_grid))\nlines(x_grid, Ey_grid)\nfor(i in 1:5){\n    lines(x_grid, bs(x_grid, df=5)[,i] * coef(m)[-1][i], col=i+1)\n}\n\ny_pred &lt;- predict(m, data.frame(x = x_grid))\n\nlines(x_grid, y_pred, col=\"red4\", lwd=2)\n\n\n\n\n\n\n\n\nFor a ‘full-strength’ version of spline fitting, you can use the mgcv package:\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nm &lt;- gam(y ~ s(x))\n\nsummary(m)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -4.2852     0.5974  -7.173  1.1e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n       edf Ref.df     F p-value    \ns(x) 5.954  7.038 70.49  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.954   Deviance explained = 96.5%\nGCV = 12.359  Scale est. = 8.9212    n = 25\n\n\n\nm &lt;- gam(y ~ s(x))\ny_hat &lt;- predict(m, data.frame(x = x_grid))\n\nplot(x, y, xlim=range(x_grid))\nlines(x_grid, Ey_grid)\nlines(x_grid, y_hat, col=\"red4\", lwd=2)\n\n\n\n\n\n\n\n\nRun ?s to see the many features of the mgcv package. mgcv also includes several specialized splines (e.g. periodic) which you may find useful in some problems.\nSplines are not normally consider a ‘machine learning’ tool but they are incredibly useful in applied statistics work. (The two methods you will rely on most from this class are spline regression and random forests.)\n\n\n\\(\\ell_1\\)-Filtering"
  },
  {
    "objectID": "notes/notes04.html#kernel-methods",
    "href": "notes/notes04.html#kernel-methods",
    "title": "STA 9890 - Non-Linear Regression",
    "section": "Kernel Methods",
    "text": "Kernel Methods\n\nManual feature expansion\nWe can think of feature expansion as mapping our data to a higher-dimensional space \\(\\Phi: \\R^p \\to \\R^P\\) and fitting a linear model there. As we have seen above, splines and their kin provide a useful way of constructing the mapping \\(\\Phi\\), but we are still constrained to work with a fairly restricted type of problem.\nCan we generalize this formally to more interesting maps \\(\\Phi(\\cdot)\\)? Yes - via kernel methods!\n\n\nRidge without coefficients\nBefore introducing kernel methods formally, let’s look back to ridge regression. We showed that the ridge solution is given by\n\\[\\hat{\\beta}_{\\text{Ridge}} = (\\bX^T\\bX + \\lambda \\bI)^{-1}(\\bX^T\\by)\\]\nSome nifty linear algebra will let us rewrite this as\n\\[\\hat{\\beta}_{\\text{Ridge}} = \\lambda^{-1}\\bX^T(\\bX\\bX^{\\top}/\\lambda + \\bI)^{-1}\\by = \\bX^{\\top}(\\bX\\bX^{\\top}+\\lambda \\bI)^{-1}\\by\\]\nYou can prove this using the Woodbury Matrix Identity or we can just check it for a single data set and trust that it generalizes:\n\nn &lt;- 100\np &lt;- 5 \n\nX &lt;- matrix(rnorm(n * p), nrow=n, ncol=p) \n\nEy &lt;- X[,1] + sqrt(X[,2]^2 + 5) + 0.1 * X[,3]^3 + cos(abs(X[,4])) + 1/(abs(X[,5]) + 3)\ny &lt;- Ey + rnorm(n, sd=sqrt(0.25))\n\nlambda &lt;- 2\neye &lt;- function(p) diag(1, p, p)\nbeta_hat_ridge &lt;- solve(crossprod(X) + lambda * eye(p), crossprod(X, y))\n\nbeta_hat_ridge_alt &lt;- t(X) %*% solve(tcrossprod(X) + lambda * eye(n)) %*% y\n\ncbind(beta_hat_ridge, beta_hat_ridge_alt)\n\n           [,1]       [,2]\n[1,] 1.42746826 1.42746826\n[2,] 0.15151370 0.15151370\n[3,] 0.57121814 0.57121814\n[4,] 0.04425193 0.04425193\n[5,] 0.58742212 0.58742212\n\n\nSo they are the same! But why did we do this?\nIt is sometimes a bit more computationally efficient if we have to invert an \\(n \\times n\\) matrix instead of a \\(p \\times p\\) matrix, but there are faster methods if we’re really interested in speed.\nNote that, if we want to make a ridge regression prediction now for a new \\(\\tilde{\\bx}\\), we just need to compute:\n\\[\\newcommand{\\bx}{\\mathbf{x}}\\begin{align*}\n\\hat{\\beta}_{\\text{Ridge}}^T\\tilde{\\bx} &= \\left[\\bX^{\\top}(\\bX\\bX^{\\top}+\\lambda \\bI)^{-1}\\by\\right]^T\\tilde{\\bx} \\\\\n&= \\left[\\by^T(\\bX\\bX^{\\top}+\\lambda \\bI)^{-T}\\bX\\right]\\tilde{\\bx} \\\\\n&= \\by^T(\\bX\\bX^{\\top}+\\lambda \\bI)^{-1} \\bX\\tilde{\\bx}\n\\end{align*}\\]\nIntuitively, recall that the inner product of two vectors measures the angle between them and can, up to some scaling, be used as a ‘similarity’ measure. This result essentially says that our prediction on a new data set is a weighted average of the training data, with weights based on similarity of the new point with the training data. This is not a crazy structure…\nIf we look closer, we see that we only need to compute products of the form \\(\\bx_1\\bx_2\\) to make this work. This is where the magic of kernels lies.\n\n\nKernel Trick\nEarlier, we considered non-linear regression by feature expansion, where we replaced \\(\\bX\\) by \\(\\Phi(\\bX)\\), where \\(\\Phi: \\R^p \\to \\R^P\\) maps to a higher-dimensional space (applied row-wise). We can use this mapping in our new ridge formula to get our non-linear predictions as\n\\[\\begin{align*}\n\\hat{y}(\\tilde{\\bx}) &= \\by^T(\\Phi(\\bX)\\Phi(\\bX)^{\\top}+\\lambda \\bI)^{-1} \\Phi(\\bX)\\Phi(\\tilde{\\bx})\n\\end{align*}\\]\nLonger, but not necessarily better. If \\(P\\) is very large, computing with \\(\\Phi(X)\\) can be incredibly expensive.\nIt turns out, however, that we never need to actually form \\(\\Phi(X)\\), we only need \\(\\kappa(\\bx_1, \\bx_2) = \\Phi(\\bx_1)^T\\Phi(\\bx_2)\\). If we can compute \\(\\kappa\\) directly, we never need to work with \\(\\Phi\\).\nFunctions that allow this are called (Mercer) kernels and they are just a little bit magical.\nIf we let \\(\\K = \\Phi(X)\\Phi(X)^{\\top}\\) be defined by \\(\\K_{ij} = \\kappa(\\bx_i, \\bx_j)\\), then we can write our feature-augmented ridge regression as:\n\\[\\hat{y}(\\tilde{\\bx}) = \\by^T(\\K+\\lambda \\bI)^{-1} \\kappa(\\bX, \\tilde{\\bx})\\]\nIf we break this apart, we see that our predictions at the new point \\(\\tilde{\\bx}\\) are essentially just weighted averages of our original observations \\(\\by\\), weighted by the similarity between the new point and our training points \\(\\kappa(\\bX, \\cdot)\\). This intuition is super important and we’ll dig into it further below.\nLet’s try out Kernel(ized) Ridge Regression:\n\nlibrary(kernlab)\n\n## Define a kernel function - we'll spell this out below.\nrbf &lt;- rbfdot(sigma = 0.05)\n\n## calculate kernel matrix\nK &lt;- kernelMatrix(rbf, X)\ndim(K)\n\n[1] 100 100\n\n\nNote that this is a \\(n \\times n\\) matrix - not a \\(p \\times p\\) matrix!\nWe can now use this to make predictions:\n\nkrr &lt;- function(x, lambda=1) {\n    crossprod(y, solve(K + lambda * eye(n), kernelMatrix(rbf, X, x)))\n}\n\nkrr_MSE &lt;- mean((y - krr(X))^2)\nprint(krr_MSE)\n\n[1] 0.4916979\n\n\nThis is better than the OLS MSE:\n\nmean(resid(lm(y ~ X))^2)\n\n[1] 0.5484549\n\n\nWhy? How is this not a contradiction of everything said before.\nBut how do we actually do kernel multiplication?\nThere are many kernel functions in the world. The most common are defined by:\n\n\\(\\kappa(\\bx_1, \\bx_2) = \\bx_1^{\\top}\\bx_2\\). This is the linear kernel, equivalent to non-kernel methods\n\\(\\kappa(\\bx_1, \\bx_2) = (\\bx_1^{\\top}\\bx_2 + c)^d\\). This is the polynomial kernel, equivalent to fitting polynomial regression up to degree \\(d\\) with all cross products. (The role of \\(c\\) is tricky, but it essentially controls the relative weight of the higher and lower order terms)\n\\(\\kappa(\\bx_1, \\bx_2) = e^{-\\sigma^2\\|\\bx_1 - \\bx_2\\|^2}\\). This is the squared exponential or radial basis function (RBF) kernel; it is very popular in spatial statistics, where it is closely related to a method known as kriging.\n\nWhile the first two kernels give us a natural \\(\\Phi(\\cdot)\\) mapping, the \\(\\Phi(\\cdot)\\) for the RBF kernel is actually infinite dimensional: it lets us fit function classes we could not fit without the kernel trick.\nFor even more kernels, see:\n\nDavid Duvenaud’s Kernel Cookbook\nGPML Section 4.2.1\n\nMuch of our current understanding of deep learning is based on specialized kernel methods:\n\nThe Neural Tangent Kernel\nKernel Methods for Deep Learning\nGradient Descent as a Kernel Machine (This paper is a bit controversial)\n\nThis blog is a particularly nice (but still somewhat technical) introduction to the Neural Tangent Kernel and its relationship to neural networks.\nSpline theory has also found use to explain ReLU (piecewise linear activation) neural networks:\n\nMad Max: Spline Insights into Deep Learning\nRepresenter Theorems for Neural Networks and Ridge Splines\n\nWe can also kernelize some of the other methods we’ve studied in this course.\nNotably, we can kernelize \\(K\\)-Nearest Neighbors to get “KKNN” if we modify the definition of distance used to be “kernel-ish.” We first recall that we can write distances solely in terms of inner products:\n\\[\\begin{align*}\n\\|\\bx_1 - \\bx_2\\|_2^2 &= (\\bx_1 - \\bx_2)^T(\\bx_1 - \\bx_2) \\\\\n&= \\bx_1^T\\bx_1 - \\bx_2^T\\bx_1 - \\bx_1^T\\bx_2 + \\bx_2^T\\bx_2 \\\\\n&= \\bx_1^T\\bx_1  - 2\\bx_1^T\\bx_2 + \\bx_2^T\\bx_2 \\\\\n\\implies \\text{Kernel Distances} &= \\kappa(\\bx_1, \\bx_1) - 2\\kappa(\\bx_1, \\bx_2) + \\kappa(\\bx_2, \\bx_2)\n\\end{align*}\\]\nIf we compute distances this way for a given \\(\\kappa\\), we get kernel KNN. This is particularly nice when we have kernels that capture specific behaviors (e.g., periodic or embedded categorical) that we can’t really treat as Euclidean."
  },
  {
    "objectID": "notes/notes04.html#trade-offs-between-linear-and-non-linear-methods",
    "href": "notes/notes04.html#trade-offs-between-linear-and-non-linear-methods",
    "title": "STA 9890 - Non-Linear Regression",
    "section": "Trade-Offs between Linear and Non-Linear Methods",
    "text": "Trade-Offs between Linear and Non-Linear Methods\nHaving developed some tools for non-linear regression, let’s step back and ask whether they are worthwhile. Recall our error decomposition:\n\\[\\text{MSE} = \\text{Irreducible Error} + \\text{Variance} + \\text{Model Bias}^2 + \\text{Estimation Bias}^2\\]\nWe have already discussed how tools like ridge and lasso let us remove variance at the cost of (estimation) bias.\nWe argued that this was a worthwhile trade when the problem had a lot of ‘innate’ variance, either from having large noise variance (\\(\\V[\\epsilon] \\gg 0\\)) or from having many features. In particular, use of some sort of shrinkage (ridge or lasso penalization) was essential in the ‘high-dimensional’ case (\\(p &gt; n\\)) where OLS had so much variance that it was not even uniquely defined. We also argued that, as we got more and more data (\\(n \\to \\infty\\)), the variance took care of itself in the usual statistical way.\nThese lessons generalize to the non-linear vs linear debate as well. Choosing to use a linear model is itself a variance reducing choice - there are ‘more’ curves than lines in some sense. If we restrict our attention to linear models only, we are potentially accepting some Model Bias, again with a hope of reducing variance to get overall better performance. As such, the same intuition as above applies: linear models are preferred when variance is the primary concern, either from noisy data or from small data; as we get more data, variance decreases naturally, so going to non-linear models reduces bias, giving overall smaller error.\nIn the non-linear context, the ‘estimation bias / variance’ trade-off remains, but modern tools like mgcv essentially handle this automatically for us. It is of course still there, but mgcv has some nifty auto-tuning built-in.3"
  },
  {
    "objectID": "notes/notes04.html#other-topics-in-regression",
    "href": "notes/notes04.html#other-topics-in-regression",
    "title": "STA 9890 - Non-Linear Regression",
    "section": "Other Topics in Regression",
    "text": "Other Topics in Regression\n\nAlternative Loss Functions\nFor most of this class, we have used MSE as our loss function, building on a foundation of least squares.\nSquared (\\(\\ell_2^2\\)) error is a bit interesting: as an error gets larger, it counts even more. This makes OLS/MSE methods sensitive to outliers (in the same way the mean is), but it also implies that OLS won’t work too hard to “over-minimize” small errors.\nWe can consider some other loss functions: - \\(\\ell_1\\) error (absolute difference): robust to outliers, fits to the conditional median instead of of the conditional mean.\nThe resulting Median Absolute Deviation (MAD) regression no longer has a closed form, but can be solved quickly using tools like CVX.\n\nHuber loss: a blend of \\(\\ell_2\\) and \\(\\ell_1\\) error, which uses \\(\\ell_2\\) for small errors and \\(\\ell_1\\) for big errors. \\[\\text{HL}_{\\delta}(x) = \\begin{cases} \\frac{1}{2}x^2 & |x| &lt; \\delta \\\\ \\delta * (|x| - \\delta/2) & |x| \\geq \\delta \\end{cases}\\]\nThis is still convex, so we can solve it with CVX.\n\\(\\epsilon\\)-insensitive: \\[L_{\\epsilon}(x) = (|x| - \\epsilon)_+ = \\begin{cases} 0 & |x| &lt; \\epsilon \\\\ |x| - \\epsilon & |x| \\geq \\epsilon \\end{cases}\\] This captures the idea of “close enough is good enough” with “close enough” being defined by \\(\\epsilon\\). It’s a bit unnatural statistically, but relates to an important classification method we’ll cover in a few weeks.\n\n\n\nMulti-Task Regression\nIn some contexts, we may want to perform multiple regressions at the same time. That is, we have a vector of responses for each observation. The OLS generalization is straightforward: \\[\\newcommand{\\bbeta}{\\mathbf{\\beta}}\\newcommand{\\bB}{\\mathbf{B}}\\newcommand{\\bY}{\\mathbf{Y}}\\argmin_{\\bB} \\frac{1}{2}\\|\\bY - \\bX\\bB\\|_F^2 \\implies \\hat{\\bB} = (\\bX^T\\bX)^{-1}\\bX^T\\bY\\] so we essentially just fit different OLS models for each element of the response. This is not super interesting.\nMulti-task regression becomes interesting if we want to add structure to \\(\\bB\\): a particularly common requirement is to select the same set of features for each of the regression targets. We do this using a group lasso on each row of \\(\\bB\\): under this structure, if we select a feature for one part of the response, we’ll use it for every part. (Can you see why?)\n\\[\\argmin_{\\bB} \\frac{1}{2}\\|\\bY - \\bX\\bB\\|_F^2 + \\lambda \\sum_{j=1}^p \\|\\bB_{j\\cdot}\\|_2\\]\nAnother common requirement is for \\(\\bB\\) to be “low-rank” but we won’t consider that here. If you want to look into it, keywords are “nuclear-norm regularized.”\n\nWeights and generalization\n\nWLS loss: \\(\\newcommand{\\bW}{\\mathbf{W}}(\\by - \\bX\\bbeta)^T\\bW(\\by - \\bX\\bbeta)\\) for known diagonal \\(\\bW\\). Weight samples based on their variance (higher variance gets smaller weights)\nWeighted lasso: \\(+\\lambda\\sum_{j=1}^p w_j|\\beta_j|\\). Higher weights are less likely to be selected\nBasis of ‘adaptive’ lasso methods (not discussed)\nGeneralized ridge: \\(+\\frac{\\lambda}{2}\\bbeta^T\\Omega\\beta\\)\nUsed to ‘smooth’ \\(\\hat{\\beta}\\) - useful for ridge analogue of fused lasso"
  },
  {
    "objectID": "notes/notes05.html",
    "href": "notes/notes05.html",
    "title": "STA 9890 - Introduction to Classification",
    "section": "",
    "text": "\\[\\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\Ycal}{\\mathcal{Y}}\\]\nAs we move into Unit II of this course, we turn our focus to classification."
  },
  {
    "objectID": "notes/notes05.html#footnotes",
    "href": "notes/notes05.html#footnotes",
    "title": "STA 9890 - Introduction to Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nConsider, e.g., predicting someone’s annual tax liability. Technically, this is discrete since tax liabilities are rounded to the nearest dollar, but functionally it may as well be continuous.↩︎\nEven though it doesn’t make sense to average ordinal data, University administrators are notorious for averaging Likert data to evaluate faculty. This is a nigh-universal grievance of numerically-minded faculty, but getting universities to change their established procedures to something statistically reasonable is only slightly less impossible than drawing blood from a turnip.↩︎\nIn this case, the FPR is exactly equal to the negative rate in the population.↩︎\nDon’t confuse this with Rate of Change.↩︎"
  },
  {
    "objectID": "notes/notes05.html#a-taxonomy-of-classification",
    "href": "notes/notes05.html#a-taxonomy-of-classification",
    "title": "STA 9890 - Introduction to Classification",
    "section": "A Taxonomy of Classification",
    "text": "A Taxonomy of Classification\nRecall from before that we defined regression as the subset of supervised learning where the response (\\(\\by\\)) was a continuous variable, or at least one that is appropriate to model as such.1 By contrast, classification considers \\(\\by\\) to take values in a discrete set. Though this restriction may feel limiting, it is actually quite powerful. We consider several different forms of classification, based on the support of \\(\\by\\):\n\nBinary classification: in this simplest case, \\(\\by\\) takes one of two values, which we conventionally code as \\(\\{0, 1\\}\\) or \\(\\{-1, +1\\}\\), depending on which makes the math easier. Note that the actual response need not be these numbers; they are simply a mathematical convenience.\nExamples of binary classification include:\n\nDoes an incoming student graduate within the next 6 years or no?\nDoes a patient have a given disease?\nIs there a dog present in an image?\nIs this applicant qualified for a job?\n\nNote that, in at least two of these, the underlying question is (arguably) not quite so binary, but we may choose to treat it as such. In particular, when assessing onset of a disease, the actual impairment to the patient is continuous, but it is common in medical practice to “binarize” the outcome. Any of you with aging family members know that cognitive decline occurs long before it’s ‘officially’ Alzheimer’s. Similarly, qualification for a job is perhaps best understood as a continuous (or even purely comparative) quantity “Is A more qualified than B?” but we chooes to binarize it when deciding to whom a job offer should be made.\nRecalling your probability theory, there is (essentially) only one distribution on \\(\\{0, 1\\}\\), the Bernoulli, so this scenario typically has the cleanest and simplest mathematics.\nMulticlass classification: in this case, \\(\\by\\) takes values in a set of finite and known, but potentially large, set of outcomes, which we will call \\(\\Ycal\\). Before about 20 years ago, multiclass problems took \\(|\\Ycal|\\) to be not much larger than 2, e.g., which blood type does a sample match or what language is a text sample written in, but much of the modern practical success of machine learning comes from the ability to treat very large outcome sets \\(\\Ycal\\).\nFor instance, modern computer vision (CV) has moved beyond far simple binary classification:\n\nThe famedImageNet benchmark data set has over 20,000 classes (arranged with additional structure) and modern data sets include even more. This can be contrasted with the older MNIST data set which collects only ten handwritten digits.\nBeyond CV, large multiclass classification has also revolutionized text modeling: when a chatbot generates text, it selects the next word from a large but finite set of English words.\nJust like binary classification is rooted in the Bernoulli distribution, multiclass classification is rooted in the categorical distribution. When applied to a large class of outcomes, the categorical distribution becomes a multinomial, cf Bernoulli to binomial, so multiclass classification is also frequently called multinomial classification.\n\nWhile our main focus is on binary and multiclass problems, it is worth noting some additional “modes” of classification.\n\nOrdinal classification: in this case, \\(\\Ycal\\) is an ordered set. The most common form of ordinal classification is the ubiquitous Likert scale which you have certainly encountered any time you have filled out a customer survey:\n\nStrongly Agree\nAgree\nNeutral\nDisagree\nStrongly Disagree\n\nIn this case, the categories clearly have an order and a ranking, but we can’t quite treat them as ‘numerical’ in the usual sense: Strongly Agree is not just Agree + Disagree.\nProper analysis of ordinal data is tricky and often poorly done. Complexities include the inappropriateness of statisticians’ most beloved tool, averaging2, and a fundamental ‘subjectiveness.’ Another common ordinal classification problem is the assignment of grades (A &gt; B &gt; C …). In this case, students and professors may agree that one piece of work may be better than another, but a student may feel their assignment deserves an A while the professor may judge it B work. It is quite difficult to infer each individual’s personal threshold for each category without extensive analysis.\nWe will not discuss ordinal data much in this course as it requires specialized methods. Typically ordinal data is modeled by treating the response as a rounded version of some underlying continuous variable: e.g., when grading hurricanes the classes (Category 1, Category 2, …) are derived from the underlying continuous variable of wind speed. This ‘hidden regression’ structure makes analysis of ordinal classification much closer to regression than binary classification.\n\n\nProbabilistic classification: we noted above that binary classification can be thought of as modeling a Bernoulli random variable. If we shift our focus to modeling the Bernoulli parameter (the probability \\(p \\in [0, 1]\\)) than the outcome (\\(y \\in \\{0, 1\\}\\)), we have the paradigm of probabilistic classification.\nAs a general rule, statistically-grounded classification methods have a natural probabilistic output built-in while methods coming from the CS tradition are not inherently probabilistic. Because probabilistic estimates are often quite useful, there exist several useful methods to ‘bolt-on’ probabilistic outputs to pure binary classifiers."
  },
  {
    "objectID": "notes/notes05.html#metrics-of-classification-accuracy",
    "href": "notes/notes05.html#metrics-of-classification-accuracy",
    "title": "STA 9890 - Introduction to Classification",
    "section": "Metrics of Classification Accuracy",
    "text": "Metrics of Classification Accuracy\nIn the regression context, we had a relatively small number of loss functions:\n\nMean Squared Error (MSE): mathematically convenient, easy interpretation, natural for OLS, optimizes for mean\nMean Absolute Error (MAE): mathematically a bit tricky, most practical in many circumstances, gives rise to robust regression, optimizes for median\nMean Absolute Percent Error (MAPE)\nCheckmark/Pinball Loss: gives rise to quantile estimation\nHuber loss: interpolates MSE and MAE\n\nBy contrast, in the classification context, we have many loss functions to work with. To get into them, it’s worth thinking back to the formalism of statistical hypothesis testing. Recall that, in hypothesis testing, we have a 2-by-2 table of possible outcomes:\n\nHypothesis Testing Table\n\n\n\n\n\n\n\n\nTruth\n\n\n\n\nDecision\nNull\nAlternative\n\n\nRetain Null\nTrue Negative\nFalse Negative (Type II Error)\n\n\nReject Null\nFalse Positive (Type I Error)\nTrue Positive\n\n\n\nSuppose that we have a large number (\\(n\\)) of binary classification points. If we let \\(\\hat{y}_i\\) be our prediction and \\(y^*_i\\) be the actual outcome, we can construct a similar table:\n\nClassification Outcome Table\n\n\n\n\n\n\n\n\nTruth (Second index)\n\n\n\n\nPrediction (First Index)\n\\(y_i^* = 0\\)\n\\(y_i^* = 1\\)\n\n\n\\(\\hat{y}_i=0\\)\n\\(n_{00}\\) True Negatives\n\\(n_{01}\\) False Negative (Type II Errors)\n\n\n\\(\\hat{y}_i=1\\)\n\\(n_{10}\\) False Positives (Type I Errors)\n\\(n_{11}\\) True Positives\n\n\n\nFrom this structure, we get several useful ideas:\n\nA confusion matrix is a 2x2 (or \\(K\\)-by-\\(K\\) for \\(K\\)-multiclass problems) table comparing the right answer (here “Truth” on the columns) with our guess (here “Decision” on the rows). A good classifier will have large values “on diagonal” (\\(n_{00} + n_{11}\\)) and small values “off diagonal” (\\(n_{01} + n_{10}\\))\nThe notion of true/false positive/negatives. These can be a bit tricky to remember, but the convention is:\n\nThe noun (positive/negative) captures the prediction\nThe adjective (true/false) assesses the prediction\n\nSo a “false positive” means that we guessed the positive (+1) class, but that we were wrong and the truth was the negative (-1 or 0) class.\n\nFrom this simple count table, we can create many different ratios. I find the statistical terminology a bit more intuitive than the ML terminology:\n\nTrue Positive Rate: Of the actual positives (\\(y^*=1\\)), what fraction are predicted correctly (truly) positives (\\(\\hat{y} = 1\\))? \\(\\text{TPR} = n_{11}/(n_{01} + n_{11})\\)\nFalse Negative Rate: Of the actual positives (\\(y^*=1\\)), what fraction are predicted incorrectly (falsely) as negatives (\\(\\hat{y} = 0\\))? \\(\\text{FNR} = n_{01}/(n_{01} + n_{11})\\)\nTrue Negative Rate: Of the actual negatives (\\(y^*=0\\)), what fraction are predicted correctly (truly) as negatives (\\(\\hat{y} = 0\\))?\n\\(\\text{FNR} = n_{00}/(n_{00} + n_{10})\\)\nFalse Positive Rate: Of the actual negatives (\\(y^*=0\\)), what fraction are predicted incorrectly (falsely) as positives (\\(\\hat{y} = 1\\))? \\(\\text{FPR} = n_{10} / (n_{00} + n_{10})\\)\n\nThese clearly satisfy some useful relationships:\n\\[\\text{TPR} = 1 - \\text{FNR} \\Leftrightarrow \\text{FNR} = 1 - \\text{TPR}\\]\nand\n\\[\\text{FPR} = 1 - \\text{TNR} \\Leftrightarrow \\text{TNR} = 1 - \\text{FPR}\\]\nIn this scenario, we recognize the false positive rate as the size (or level) associated with a statistical test (5% for a 95% confidence test) and the true positive rate as the power. Again, the statistical definitions can help us navigate the confusing terminology here: the confidence of a test, and therefore its false positive rate, are defined assuming the null (i.e., in a “true” negative scenario).\nThe “other direction” of rates – where we condition on the prediction instead of the truth – are less common, except for the false discovery rate:\n\\[\\text{FDR} = \\frac{n_{10}}{n_{10} + n_{11}}\\]\nThe FDR is commonly used in scientific settings and it answers this question: if a scientist makes \\(K\\) discoveries, what fraction of those claimed discoveries are correct?\n\n\n\n\n\n\nWarning\n\n\n\nNote that the convention used to define rates can be quite confusing and is perhaps opposite the convention used to define counts.\nTo compute the number of true positives, we look at predicted positives and count the number of “truths”, while to compute the true positive rate, our denominator is the number of actual positives and we count the fraction of correct predictions.\nTo me, the FDR is actually the most clearly named rate; I try to remember the definition of FDR and ‘work backwards’ to FPR since it must be something different than FDR.\n\n\nIn ML context, you will commonly hear reference to precision and recall. With our definitions above, we have\n\nPrecision = \\(1 - \\text{FDR}\\)\nRecall = \\(\\text{FNR} = 1 - \\text{TPR}\\)\n\nso, by maximizing precision, we ensure that all of our predicted positives are true positives and, by maximizing recall, we ensure that we have no false negatives (that is, we truly identify all of the real positives).\nIn the medical context, you may encounter alternative metrics of\n\nSensitivity, equal to TPR\nSpecificity, equal to TNR\n\nThis zoo of terms is admittedly quite confusing, but the Wikipedia article on Precision and Recall has an excellent table.\n\nCombining Metrics\nIf you look at the 2x2 table above, you can convince yourself that it has two “degrees of freedom:” having a high TPR implies nothing about the TNR (or equivalent metrics). That is, it is possible to have a classifier with arbitrarily good TPR (equivalently, small FNR) and terrible TNR (equivalently, small FPR).\nIn fact, we can construct one trivially:\n\\[\\hat{f}(\\cdot) = 1\\]\nThat is, the function that predicts \\(1\\) always. This classifier has zero false negatives, because it has no negatives whatsoever, but it necessarily has many false positives.3\nConversely,\n\\[\\hat{f}(\\cdot) = 0\\]\nhas no false positives, but many false negatives.\nClearly, we need a ‘combination’ metric that combines both degrees of freedom into a single score. Popular choices include:\n\nAccuracy: the fraction of correct predictions (\\((n_{00}+n_{11})/n\\))\nBalanced Accuracy: \\(\\frac{\\text{TPR} + \\text{TNR}}{2}\\)\n\\(F_1\\) Score: \\(2 *(1 - \\text{FDR}) / (\\text{TPR} - \\text{FDR} +1)\\)\n\nWhile these choices are popular, none of them are actually all that suitable for a real application. To properly evaluate a classifier, we need to know the real cost of false positives and false negatives, as well as the population base rates, and balance things accordingly.\nFor instance, if we are designing a ‘baseline’ medical diagnostic to be used as part of annual physicals, we need to think about what happens with false negatives and false positives.\n\nIf we give a false positive, patients will undergo a better test that costs $1,000 but gives an accurate answer in all circumstances (a “gold standard”).\nIf we give a false negative, patients will leave a condition untreated until it becomes far worse, requiring expensive surgery costing $50,000.\n\nIn this case, we might desire a classifier that minimizes \\[n_{10} + 50 n_{01}\\], not any of the of the ‘standard’ metrics. Note also that the particular FPR, FNR of this classifier will depend on the rate of the condition in the population, so we can’t actually find the best procedure by looking at FPR, FNR in isolation.\nWhile this decision-theoretic approach is optimal, it is often quite difficult to characterize in practice. In the example above, for instance, we made the choice to only minimize cost, without seeking to minimize suffering. (What if the untreated condition is incredibly painful? Should we intervene more often?) This sort of decision-making is ultimately very problem-specific and too often ignored in the academic literature. In practice, designing (and optimizing) the correct loss function is far more relevant to business outcomes than finding the best classifier to optimize an irrelevant cost function. procedure requires\n\n\nTrade-Off of Recall and Precision\nOur discussion of ‘trivial’ classifiers that focus on one metric while completely ignoring the other may remind you of the way this is typically solved in statistical testing procedures. We specify a maximum acceptable false positive rate (level or 1 - confidence) and then minimize the false negative rate subject to that constraint; in statistical speak, we seek a test with maximal power.\nAs we have seen with ridge and lasso regression, there is typically a ‘knob’ (hyperparameter) we can pick to tune a particular classifier. It is common to apply the classifier at all values of this knob, measure the TPR and TNR at all levels, and plot them in a trade-off curve. This curve is, for historical reasons, known as the receiver operating characteristic (ROC)4 curve and it looks something like:\n\nlibrary(yardstick)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n# Example from ?roc_curve\nroc_curve(two_class_example, truth, Class1) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  theme_bw()\n\n\n\n\n\n\n\n\nHere we see that, for this particular classifier, increased specificity is associated with decreased sensitivity or, in the alternative terms, increased TNR is associated with decreased TPR. As illustrated by our example of constant predictors above, this is a quite typical trade-off.\nCurves like this can be used in a few ways, none of which are fully satisfying:\n\nIf we have two curves, we can plot their ROCs on the same set of axes. If one classifier is uniformly above the other, it is said to dominate the other and is better for all applications. Sadly, this is rarely the case.\nIf we have two curves and plot their ROCs, but the ROCs cross, we don’t have strong guidance on which model to prefer, but if one is almost always above the other, we will likely still use it.\n\nA commonly reported metric is the area under the ROC curve (AUC), which ranges from 0.0 to 1.0, with higher values being better. This is a bit unsatisfying compared to the decision theoretic analysis, but it is an improvement over some other metrics as it at least addresses the fact there is a tradeoff.\n\nWhat does an AUC less than 0.5 imply? If you get an AUC less than 0.5, what should you do? Why is AUC near 0 actually a good thing?\n\nWhile the ROC is typically motivated by presence of a ‘tuning knob’, even methods like logistic regression (which have no obvious knobs) have an implicit knob we can choose to tune: the classification threshold. There is no law of nature that says that a predicted probability of 55% has to correspond to a prediction of the positive class. If false positives are very expensive, we may want to reduce the fraction of positives predicted and only predict the positive class when the estimated probability is over 80%. Similarly, if false negatives are very bad, we may want to predict positive even when the probability of a positive is in the 40% range: this is particularly common in medical contexts where the impact of a missed diagnosis is quite substantial, so doctors order extra diagnostic work ‘just to be sure’ even if the test does not strongly suggest a disease.\n\n\nScoring Functions\nScoring functions are loss functions particularly suited for evaluation of probabilistic classifiers. Unlike our confusion-matrix set of metrics, it’s less clear how we should evaluate probabilistic classifiers.\nIt is clear that, if we predict something with 80% probability and it doesn’t happen, this is a ‘worse’ mistake than if we only had a 51% probability, but how can we measure this formally?\n\nCalibration and Tightness\nLog Scores"
  },
  {
    "objectID": "notes/notes05.html#types-of-classification-methods",
    "href": "notes/notes05.html#types-of-classification-methods",
    "title": "STA 9890 - Introduction to Classification",
    "section": "Types of Classification Methods",
    "text": "Types of Classification Methods\nIn this course, we will consider two main types of classifiers:\n\nGenerative classifiers, which attempt to jointly model \\((\\bX, \\by)\\) and use probabilistic mathematics (mainly Bayes’ Rule) to infer \\(\\by | \\bX\\) from the joint model.\nDiscriminative classifiers, which directly attempt to estimate the ‘boundary’ between the classes.\n\nAs you will see, the terminology around these methods is particularly terrible."
  },
  {
    "objectID": "notes/notes05.html#building-mutliclass-classifiers-from-binary-classifiers",
    "href": "notes/notes05.html#building-mutliclass-classifiers-from-binary-classifiers",
    "title": "STA 9890 - Introduction to Classification",
    "section": "Building Mutliclass Classifiers from Binary Classifiers",
    "text": "Building Mutliclass Classifiers from Binary Classifiers\nAs we introduce methods, we will typically derive them for the binary classification task. If we want to apply them to the multiclass task, how can we extend them? It would be quite cumbersome if we needed brand new methods each time.\nIn general, there are two main strategies we might use:\n\nOne vs Rest\nEach vs Each\n\nFor simplicity, assume we are predicting \\(K\\) classes. In the “one-vs-rest” strategy, we will build \\(K\\) binary classifiers, of the form:\n\nClass 1 vs Not Class 1 (i.e., Class 2, 3, 4, \\(\\dots\\), \\(K\\))\nClass 2 vs Not Class 2 (i.e., Class 1, 3, 4, \\(\\dots\\), \\(K\\))\netc.\nClass \\(K\\) vs Not Class \\(K\\) (i.e., Class 1, 2, 3, \\(\\dots\\), \\(K-1\\))\n\nTo make an actual prediction, we take the highest score of each of these classifiers. E.g., if \\(K=3\\) for categories “Red”, “Green” and “Blue”, we might predict:\n\n\\(\\mathbb{P}(\\text{Red}) = 0.8\\) and \\(\\mathbb{P}(\\text{Not Red}) = 0.2\\)\n\\(\\mathbb{P}(\\text{Green}) = 0.4\\) and \\(\\mathbb{P}(\\text{Not Red}) = 0.6\\)\n\\(\\mathbb{P}(\\text{Blue}) = 0.2\\) and \\(\\mathbb{P}(\\text{Not Red}) = 0.8\\)\n\nOur final prediction would then be the maximizer, “Red.”\nThis strategy is relatively simple and intuitive, but for very large \\(K\\), the maximum can be far less than 50%, so it may feel a bit funny. (Why is this not an issue for \\(K=2\\)?)\nConversely, in the “each vs each” (sometimes called “one vs one”) setting, we train \\(\\binom{K}{2} = K(K-1)/2\\) classifiers for every possible pair of classes. To get the prediction, we then take a ‘majority vote’ of the classifiers.\nIn our \\(K=3\\) example above, we would have three classifiers:\n\nRed vs Blue, which we fit after throwing out the “Green” points\nRed vs Green, which we fit after throwing out the “Blue” points\nGreen vs Blue, which we fit after throwing out the “Red” points\n\nIf we make predictions for a single point, our binary classifications might be\n\nRed vs Blue: Red\nRed vs Green: Red\nGreen vs Blue: Blue\n\nIn this case, we would take a majority vote to get “Red” as our final prediction.\nI find this strategy a bit counterintuitive, but it is sometimes faster than the one-vs-rest strategy since each individual model is fit to a subset of the entire data set.\nCertain modern ML strategies, in particular deep neural networks, predict the probability of all \\(K\\) classes from a single model. In this case, we typically select the single most probable class as our prediction.\nIn applications where there are multiple contenders and the difference between them is small, it is common to randomly select a class in proportion to its predicted probability. This strategy is one of the things that gives ChatGPT its randomness; it usually predicts the most likely next word, but sometimes it does something a little bit weird.)\nNote that it is actually usually pretty hard to make a model that predicts a large set of probabilities simultaneously: in particular, it’s hard to get a vector that sums to one (like probabilities should). To address this, a softmax normalization is applied. This is essentially nothing more than i) making sure each predicted value is positive; and ii) dividing them all by the sum so they add to 1."
  },
  {
    "objectID": "notes/tests/test1.html",
    "href": "notes/tests/test1.html",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "",
    "text": "The original test booklet can be found here. \\[\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\by}{\\mathbf{y}}\\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\bbeta}{\\mathbf{\\beta}}\\newcommand{\\argmin}{\\text{arg min}}\\newcommand{\\bD}{\\mathbf{D}}\\newcommand{\\bzero}{\\mathbf{0}}\\newcommand{\\bI}{\\mathbf{I}}\\newcommand{\\bz}{\\mathbf{z}}\\]"
  },
  {
    "objectID": "notes/tests/test1.html#true-or-false",
    "href": "notes/tests/test1.html#true-or-false",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "True or False",
    "text": "True or False\n\nTEST\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAnswer\n\n\n\nTEXT\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAnswer"
  },
  {
    "objectID": "notes/tests/test1.html#short-answer",
    "href": "notes/tests/test1.html#short-answer",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "Short Answer",
    "text": "Short Answer\n\nQUESTION\n\n\n\n\n\n\nSolution\n\n\n\n\n\nANSWER"
  },
  {
    "objectID": "notes/tests/test1.html#mathematics-of-machine-learning",
    "href": "notes/tests/test1.html#mathematics-of-machine-learning",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "Mathematics of Machine Learning",
    "text": "Mathematics of Machine Learning"
  },
  {
    "objectID": "notes/tests/test1.html#true-or-false-30-points-at-3-points-each",
    "href": "notes/tests/test1.html#true-or-false-30-points-at-3-points-each",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "True or False (30 points at 3 points each)",
    "text": "True or False (30 points at 3 points each)\n\nLinear regression is a supervised learning method because it has a matrix of features \\(\\bX \\in \\R^{n \\times p}\\) and a vector of responses \\(\\by \\in \\R^{n}\\) which we attempt to predict using \\(\\bX\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. Linear regression methods are supervised learning methods because they attempt to predict (continuous) responses \\(y\\) using a set of covariates \\(\\bx\\).\n\n\n\nModels with higher training error always have higher test error.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. If this were true, we could solve all of life’s problems by minimizing training error with no regard for overfitting.\nAside: This story is made somewhat more difficult by recent studies in ML practice where we have learned to fit rather complex models in a way that keeps both training and test error small. We will return to this later in this course, but it is still by no means an always relationship.\n\n\n\nFor the same level of sparsity, best subsets provides smaller training error than the lasso.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. Because best subsets does not apply shrinkage, it achieves smaller training error than the lasso. But this is not necessarily a good thing: see, e.g., T. Hastie, R. Tibshirani, R. Tibshirani. “Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons. Statistical Science 35(4), p.579-592. 2020. DOI:10.1214/19-STS733.\n\n\n\nBecause kernel methods are more flexible than pure linear models, they always provide in-sample (training) error improvements.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. Linear functions are contained in (essentially?) all kernel spaces, so the kernel finds the best training fit over linear and non-linear functions, allowing improvements over pure linear fits.\n\n\n\nOLS finds the linear model with the lowest test MSE.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. OLS identifes the linear model with the lowest training MSE but it does not guarantee optimal test MSE (no method can do that). In fact, we know that suitably-tuned ridge regression will always achieve lower test MSE than OLS (in expectation).\n\n\n\nWhen cross-validation is used to select the optimal value of \\(\\lambda\\) in lasso regression, the CV estimate of the out-of-sample (test) error of the selected model is unbiased because the cross-validation error is computed on an unseen ‘hold-out’ set and not on the training data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. Whenever we use an error estimate to select a parameter or a hyperparameter, that error is no longer an unbiased answer for the eventual test error. There is no difference between optimizing \\(\\hat{\\beta}\\) and optimizing \\(\\lambda\\) in this regard.\n\n\n\nOrdinary least squares is BLUE when applied to a VAR1 model if the underlying data generating process is truly linear, the errors are mean zero and have constant variance (no `heteroscedasticity’).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. BLUE-ness of OLS presumes independent errors, which is not stated in the above question and is generally not true for time series models.\n\n\n\nReducing variance always increases bias.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. While this is often true, it is not always so. Ensemble methods, which we discuss later in this course, reduce variance without increasing bias. More prosaically, increasing sample size reduces variance but does not increase bias.\n\n\n\n\\(K\\)-Nearest Neighbors can be used for regression and classification.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. We give examples of both in the course notes.\n\n\n\nLinear models are preferred in high-dimensional scenarios because they have a low bias.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. Linear models are preferred in high-dimensional scenarios because they have lower variance than non-linear models."
  },
  {
    "objectID": "notes/tests/test1.html#footnotes",
    "href": "notes/tests/test1.html#footnotes",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVAR, or Vector Autoregressive, is a simple linear model for multivariate time series.↩︎"
  },
  {
    "objectID": "notes/tests/test1.html#true-or-false-30-points---10-questions-at-3-points-each",
    "href": "notes/tests/test1.html#true-or-false-30-points---10-questions-at-3-points-each",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "True or False (30 points - 10 questions at 3 points each)",
    "text": "True or False (30 points - 10 questions at 3 points each)\n\nLinear regression is a supervised learning method because it has a matrix of features \\(\\bX \\in \\R^{n \\times p}\\) and a vector of responses \\(\\by \\in \\R^{n}\\) which we attempt to predict using \\(\\bX\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. Linear regression methods are supervised learning methods because they attempt to predict (continuous) responses \\(y\\) using a set of covariates \\(\\bx\\).\n\n\n\nModels with higher training error always have higher test error.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. If this were true, we could solve all of life’s problems by minimizing training error with no regard for overfitting.\nAside: This story is made somewhat more difficult by recent studies in ML practice where we have learned to fit rather complex models in a way that keeps both training and test error small. We will return to this later in this course, but it is still by no means an always relationship.\n\n\n\nFor the same level of sparsity, best subsets provides smaller training error than the lasso.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. Because best subsets does not apply shrinkage, it achieves smaller training error than the lasso. But this is not necessarily a good thing: see, e.g., T. Hastie, R. Tibshirani, R. Tibshirani. “Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons. Statistical Science 35(4), p.579-592. 2020. DOI:10.1214/19-STS733.\n\n\n\nBecause kernel methods are more flexible than pure linear models, they always provide in-sample (training) error improvements.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. Linear functions are contained in (essentially?) all kernel spaces, so the kernel finds the best training fit over linear and non-linear functions, allowing improvements over pure linear fits.\nThis question was a bit ambiguous about the function space used and whether any regularization was applied (as it almost always is in kernel methods), so I will also accept False here. (But you should understand that the answer is essentially always True - more flexible methods fit better on the training data except in exceptional circumstances.)\nAlternatively, in the case where \\(n=p\\), OLS has 0 MSE and kernels cannot improve upon it.\n\n\n\nOLS finds the linear model with the lowest test MSE.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. OLS identifes the linear model with the lowest training MSE but it does not guarantee optimal test MSE (no method can do that). In fact, we know that suitably-tuned ridge regression will always achieve lower test MSE than OLS (in expectation).\n\n\n\nWhen cross-validation is used to select the optimal value of \\(\\lambda\\) in lasso regression, the CV estimate of the out-of-sample (test) error of the selected model is unbiased because the cross-validation error is computed on an unseen ‘hold-out’ set and not on the training data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. Whenever we use an error estimate to select a parameter or a hyperparameter, that error is no longer an unbiased answer for the eventual test error. There is no difference between optimizing \\(\\hat{\\beta}\\) and optimizing \\(\\lambda\\) in this regard.\n\n\n\nOrdinary least squares is BLUE when applied to a VAR1 model if the underlying data generating process is truly linear, the errors are mean zero and have constant variance (no `heteroscedasticity’).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. BLUE-ness of OLS presumes independent errors, which is not stated in the above question and is generally not true for time series models.\n\n\n\nReducing variance always increases bias.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. While this is often true, it is not always so. Ensemble methods, which we discuss later in this course, reduce variance without increasing bias. More prosaically, increasing sample size reduces variance but does not increase bias.\n\n\n\n\\(K\\)-Nearest Neighbors can be used for regression and classification.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTrue. We give examples of both in the course notes.\n\n\n\nLinear models are preferred in high-dimensional scenarios because they have a low bias.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFalse. Linear models are preferred in high-dimensional scenarios because they have lower variance than non-linear models."
  },
  {
    "objectID": "notes/tests/test1.html#short-answer-50-points---10-questions-at-5-points-each",
    "href": "notes/tests/test1.html#short-answer-50-points---10-questions-at-5-points-each",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "Short Answer (50 points - 10 questions at 5 points each)",
    "text": "Short Answer (50 points - 10 questions at 5 points each)\n\nGive an example that demonstrates why the \\(\\ell_0\\)-“norm” is not convex.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that a convex function satisfies the inequality\n\\[f(\\lambda \\bx + (1-\\lambda)\\by) \\leq \\lambda f(\\bx) + (1-\\lambda) f(\\by)\\]\nfor all \\(\\bx, \\by\\) and all \\(\\lambda \\in [0, 1]\\).\nLet us take\n\\[\\bx = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\by = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\]\nwith \\(\\lambda = 0.5\\) so\n\\[\\lambda \\bx + (1-\\lambda) \\bx = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}.\\]\nThis then gives us\n\\[\\|\\bx\\|_0 = 1, \\|\\by\\|_0 = 1 \\text{ but } \\|\\lambda \\bx + (1-\\lambda)\\by\\|_0 = 2\\]\nwhich violates our inequality.\n\n\n\nList three reasons we may choose to use a sparse model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPossible answers include:\n\nInterpretability\nBetter Estimation / Improved Test Performance / Shrinkage\nFewer Features to Collect on Test Points\nFaster Predictions / Computational Efficiency\nFaster Fitting\n\nThis list is not comprehensive and alternate correct answers will also receive credit.\n\n\n\nCompare and contrast spline and kernel models. Give at least 2 key similarities (“compare”) and 2 key differences (“contrast”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPotential answers include the following similarities:\n\nBoth perform non-linear supervised learning\nBoth fit smooth non-linear functions\n\nand differences:\n\nKernel methods fit combinations (interactions) of features while standard spline models are fit to a single feature at a time\nSplines are fit locally while kernels are fit globally\nBecause they are univariate, splines are easy to interpret and can be used in sparse models\n\nThis list is not comprehensive and alternate correct answers will also receive credit.\n\n\n\nThe elastic net is a combination of ridge and lasso regression: \\[\\argmin_{\\bbeta} \\frac{1}{2}\\left\\|\\by - \\bX\\bbeta\\right\\|_2^2 + \\lambda_1 \\|\\bbeta\\|_1 + \\frac{\\lambda_2}{2} \\|\\bbeta\\|_2^2.\\] In the case where \\(\\bX\\) is the identity matrix, what is \\(\\hat{\\bbeta}\\) in terms of \\(\\by, \\lambda_1, \\lambda_2\\)?\nHint: Note that the solution is the composition of the ridge and lasso shrinkage operators, applied in any order. That is, if the ridge and lasso shrinkage operators are \\(S_1(\\cdot), S_2(\\cdot)\\), the solution will be of the form \\(S_1(S_2(\\cdot)) = S_2(S_1(\\cdot))\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that the lasso and ridge shrinkage operators are given by\n\\[S_{\\lambda, \\ell_1}(x) = \\begin{cases} x - \\lambda & x &gt; \\lambda \\\\ 0 & |x| &lt; \\lambda \\\\ x + \\lambda & x &lt; -\\lambda\\end{cases}\\]\nand\n\\[S_{\\lambda, \\ell_2^2}(x) = \\frac{x}{1+\\lambda}\\]\nso their composition is given by:\n\\[S(x) = \\begin{cases}\\frac{x - \\lambda_1}{1+\\lambda_2} & x &gt; \\lambda_1 \\\\ 0 & |x| &lt; \\lambda_1 \\\\ \\frac{x + \\lambda_1}{1+\\lambda_2} & x &lt; -\\lambda_1\\end{cases} = \\frac{1}{1+\\lambda_2}\\mathcal{S}_{\\lambda_1}(x) \\]\nwhere \\(\\mathcal{S}_{\\lambda}(\\cdot)\\) is the soft-threshold operator. Hence the solution is \\[\\hat{\\bbeta} = \\frac{1}{1+\\lambda_2}\\mathcal{S}_{\\lambda_1}(\\by)\\]\nSee Y. Yu “On Decomposing the Proximal Map” in NeurIPS 2013: Advances in Neural Information Processing Systems 26.\nThis operation:\n\\[f(\\bz) = \\argmin_{\\bx} f(\\bx) + \\frac{1}{2}\\|\\bx - \\bz\\|_2^2\\]\nis known as the proximal operator of the function \\(f\\) and it has applications throughout machine learning and statistics. See N.G. Polson, J.G. Scott, and B.T. Willard. “Proximal Algorithms in Statistics and Machine Learning.” Statistical Science 30(4), pp.559-581. 2015. DOI:10.1214/15-STS530 and N. Parikh and S. Boyd “Proximal Algorithms.” Foundations and Trends in Optimization 1(3), p.123-231. 2013 DOI:10.1561/2400000003 for more.\n\n\n\nName three advantages of convexity in formulating machine learning approaches.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPossible answers include:\n\nEfficient optimization algorithms\nGuarantees of global optimality\nStatistical stability - the minima of convex functions are less sensitive to noise\nTheoretical simplicity - convex functions are easier to analyze than non-convex functions\n\n\n\n\nIn your own words, explain why use of a holdout set (or similar techniques) is important for choosing hyperparameters in machine learning.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYour answer should discuss how the use of a holdout set (distinct from a training set) can be used to get unbiased estimates of the test error which we can then use to select the hyperparameter. If we don’t use a holdout set, we are more likely to overfit the training data since we never test a model’s out-of-sample predictive capability.\n\n\n\nGive two reasons why is the lasso preferred over best subsets for fitting linear models.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nComputational tractability. Best subsets is hard.\nShrinkage and improved out of sample performance\n\n\n\n\nRank the following models in terms of (statistical) complexity with 1 being the lowest complexity and 5 being the highest: OLS, 1-Nearest Neighbor Regression, Cubic Spline Regression, Cubic Polynomial Regression, Ridge Regression\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nRidge Regression is simpler than\nOLS is simpler than\nCubic Spline Regression is simpler than\nCubic Polynomial Regression is simpler than\n\\(1\\)-Nearest Neighbor Regression\n\nI will accept answers that switch the order of cubic splines and cubic polynomials as a cubic spline with many knots can have more complexity (effective degrees of freedom) than a single cubic polynomial fit. This question should have said piecewise cubic polynomial regression.\n\n\n\nOn the first plot, draw a curve of typical training and test errors for a \\(K\\)-nearest neighbor regressor. On the second plot, draw a curve of the typical bias and variance for \\(K\\)-NN regression.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYour plots should:\n\nHave the training error of \\(K\\)-NN start at 0 for \\(K\\)=1\nHave the training error of \\(K\\)-NN increase in \\(K\\)\nHave a “U” shape for the test error of \\(K\\)-NN\nHave the bias of \\(K\\)-NN start at 0 and increase in \\(K\\)\nHave the variance of \\(K\\)-NN start high and decrease in \\(K\\)\nThe test error should remain above the training error\n\n\n\n\nOn the first plot, draw a curve of typical training and test errors for ridge regression. On the second plot, draw a curve of the typical bias and variance for ridge regression.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYour plots should:\n\nIncrease Bias as \\(\\lambda\\) increases\nDecrease Variance as \\(\\lambda\\) increases\nHave monotonically increasing training error\nHave a \\(U\\)-shape for test error"
  },
  {
    "objectID": "notes/tests/test1.html#mathematics-of-machine-learning-20-points-total",
    "href": "notes/tests/test1.html#mathematics-of-machine-learning-20-points-total",
    "title": "STA 9890 - Spring 2025  Test 1: Regression",
    "section": "Mathematics of Machine Learning (20 points total)",
    "text": "Mathematics of Machine Learning (20 points total)\nIn this section, you will analyze so-called generalized ridge and lasso penalties. These methods apply the ridge or lasso penalties to something other than the vector of estimated coefficients (\\(\\hat{\\bbeta}\\)):\n\\[\\begin{align*}\n    \\argmin_{\\bbeta} &\\frac{1}{2}\\left\\|\\by - \\bX\\bbeta\\right\\|_2^2 +\\frac{\\lambda}{2} \\|\\bD\\bbeta\\|_2^2\\tag{Generalized Ridge} \\\\\n    \\argmin_{\\bbeta} &\\frac{1}{2}\\left\\|\\by - \\bX\\bbeta\\right\\|_2^2 +\\lambda \\|\\bD\\bbeta\\|_1\\tag{Generalized Lasso}\n\\end{align*}\\]\nMost commonly, \\(\\bD\\) is taken to be some sort of first-or-second order difference matrix so that \\[\\|\\bD_1\\bbeta\\|^2_2 = \\sum_{i=1}^{p-1} (\\beta_{i+1}-\\beta_i)^2 \\text{ and } \\|\\bD_1\\bbeta\\|_1 = \\sum_{i=1}^{p-1} |\\beta_{i+1}-\\beta_i| \\tag{First order difference}\\] and \\[\\|\\bD_2\\bbeta\\|^2_2 = \\sum_{i=1}^{p-2} (\\beta_{i+2} - 2\\beta_{i+1}+\\beta_i)^2 \\text{ and } \\|\\bD_2\\bbeta\\|_1 = \\sum_{i=1}^{p-2} |\\beta_{i+2} - 2\\beta_{i+1}+\\beta_i|\\tag{2nd order difference}\\] but other choices are also popular.\n\nBy analyzing the stationarity (zero gradient) condition, derive the closed-form solution for generalized ridge regression (arbitrary \\(\\bD\\)). Box your answer so that it is clearly identifiable. You may assume all relevant matrices are full-rank and/or invertible. (8 points)\nHint: Note that \\(\\partial \\|\\bD\\bbeta\\|_2^2 / \\partial \\bbeta = 2\\bD^{\\top}\\bD\\bbeta\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe simply take the gradient of the generalized ridge loss and set it equal to zero:\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial \\bbeta}\\left(\\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 + \\frac{\\lambda}{2}\\|\\bD\\bbeta\\|_2^2\\right) &= -\\bX^{\\top}(\\by - \\bX\\bbeta) + \\lambda\\bD^{\\top}\\bD\\bbeta \\\\\n\\bzero &= (\\bX^{\\top}\\bX + \\lambda \\bD^{\\top}\\bD)\\bbeta - \\bX^{\\top}{\\by}\\\\\n\\implies \\hat{\\bbeta} &= (\\bX^{\\top}\\bX + \\lambda \\bD^{\\top}\\bD)^{-1}\\bX^{\\top}\\by\n\\end{align*}\\]\n\n\n\nIn order to build intuition, let us consider the case where \\(\\bX = \\bI\\) and consider what happens when we change \\(\\lambda\\) in the generalized lasso problem. On the following plot, I have drawn the vector of observation \\(\\by\\) (dots) as well as the \\(\\bD_1\\)-generalized lasso solution for \\(\\lambda=0\\) (small dashes) and \\(\\lambda=\\infty\\) (large dashes).\nOn the blank set of axes, draw the estimated \\(\\hat{\\bbeta}\\) vectors from the \\(\\bD_1\\)-generalized lasso at a both ‘small-ish’ and ‘large-ish’ value of \\(\\lambda\\). Label each solution carefully. (4 points)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou plots should be piecewise constant, with fewer jumps for moderate \\(\\lambda\\) than for small \\(\\lambda\\).\n\n\n\nWhat is the relationship between \\(\\bD_2\\)-generalized ridge regression and spline methods?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe \\(\\bD_2\\) matrix is a second-difference matrix and it plays the same role as the second derivative on a finite grid. As such, \\(\\bD_2\\)-generalized ridge regression will find a curve that fits the data while controlling the sum of squared second differences. Spline regression will find a curve that fits the data while controlling the integral of squared second derivative. For simple equispaced settings, these approaches essentially coincide.\nKey points to mention:\n\nBoth can be used fit smooth curves\nBoth capture “second derivative” type information\nBoth induce a roughness penalty regularizer which must be tuned\n\n\n\n\nDescribe a situation in which generalized ridge or generalized lasso regression would be appropriate. What value of \\(\\bD\\) would you choose for your application?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere are many possible answers for this part."
  },
  {
    "objectID": "notes/notes06.html",
    "href": "notes/notes06.html",
    "title": "STA 9890 - Generative Classifiers",
    "section": "",
    "text": "\\[\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\P}{\\mathbb{P}}\\]\nThis week, we begin our study of classifiers with an analysis of generative classifiers. While many generative classifiers have appeared in the literature, they can essentially all be derived from a standard formula. We will discuss this standard formula first and then turn our attention to specific named instances."
  },
  {
    "objectID": "notes/notes06.html#mixture-models",
    "href": "notes/notes06.html#mixture-models",
    "title": "STA 9890 - Generative Classifiers",
    "section": "Mixture Models",
    "text": "Mixture Models\nRecall from STA 9715 that a (discrete) mixture model is a distribution of the form:\n\\[X \\sim \\begin{cases} X_1 & Z = 1 \\\\ X_2 & Z = 2 \\\\ \\dots \\\\ X_K & Z=K \\end{cases} \\text{ where } \\textsf{supp}(Z) = \\{1,\\dots, K\\} \\text{ and } X_i \\sim \\mathcal{P}_i \\text{ for all $i$ and } Z \\perp X_i, X_i \\perp X_j\\, (i\\neq j)\\]\nThat’s quite a bit to unpack, so let’s take it piece by piece: the random variable \\(X\\) has a \\(K\\)-component mixture distribution if we can sample \\(X\\) by generating \\(K\\) independent random variables \\(\\{X_1, \\dots, X_K\\}\\) (with different distributions) and then selecting one of them at random.\nLet’s look at a concrete version of this: suppose we have a population of male and female basset hounds that is 60% female. The weight of male basset hounds follows a \\(\\mathcal{N}(65, 10)\\) distribution while females have weight distributed as \\(\\mathcal{N}(55, 10)\\). The weight of a randomly selected basset can be sampled as\n\nrbasset &lt;- function(n){\n    Z &lt;- rbinom(n, size=1, prob=0.6) # Z=1 &lt;- Female\n    ifelse(Z, rnorm(n, mean=55, sd=sqrt(10)), \n              rnorm(n, mean=65, sd=sqrt(10)))\n}\nrbasset(1)\n\n[1] 67.67984\n\n\nIf we collect the weights of many basset hounds, we see that the resulting distribution is not normally distributed:\n\nhist(rbasset(10000), breaks=50, main=\"Distribution of Basset Hound Weights\")\n\n\n\n\n\n\n\n\nIn fact, this is a bimodal distribution with each ‘lump’ representing males or females.1 Distributions of this form can often be written more compactly using a ‘convex combination’ notation:\n\\[\\text{Basset} \\sim \\frac{2}{5}\\mathcal{N}(65, 10) + \\frac{3}{5}\\mathcal{N}(55, 10)\\]\nThe above notation can be interpreted as specifying a draw from the first distribution with probability \\(2/5 = 40\\%\\) and from the second with probability \\(3/5 = 60\\%\\); that is, it’s a particular 2-component mixture of normals.\nIn specifying this mixture, the probabilities of each component must be non-negative and must sum to 1; these are exactly the same restrictions we put on a convex combination of points earlier in the course. In fact, the set of mixture distributions can be consider the ‘convex set’ containing all of its individual components.\n\nInference from a Mixture Distribution\nNow suppose you are on a walk and you meet a very friendly 62 pound basset hound. You of course want to pet this marvelous dog, but you need to ask the owner’s permission first: in doing so, should you ask “may I pet him?” or “may I pet her?”\nWhile the basset hound won’t care if you misgender, you recall that you can use Bayes’ Rule to ‘invert’ the mixture distribution. That is, while the mixture distribution normally goes from sex to weight, Bayes’ rule will let us go from weight back to sex.\nSpecifically, in this case, Bayes’ rule tells us:\n\\[\\P(\\text{Female}|\\text{Weight=62}) = \\frac{\\P(\\text{Weight=62}|\\text{Female})\\P(\\text{Female})}{\\P(\\text{Weight=62}|\\text{Female})\\P(\\text{Female}+\\P(\\text{Weight=62}|\\text{Male})\\P(\\text{Male}))}\\]\nThe math here is a bit nasty numerically, but we can do it in R quite simply:\n\np_female &lt;- 0.6\np_male &lt;- 1-p_female\n\np_62_female &lt;- dnorm(62, mean=55, sd=sqrt(10))\np_62_male   &lt;- dnorm(62, mean=65, sd=sqrt(10))\n\np_female_62 &lt;- (p_62_female * p_female) / (p_62_female * p_female +\n                                           p_62_male * p_male)\n\npaste0(\"There is a \", round(p_female_62 * 100, 2), \"% chance that basset is a female.\")\n\n[1] \"There is a 16.87% chance that basset is a female.\"\n\n\nGiven that calculation, you take a chance and ask whether you can pet him.\nWhat have we done here? We have built a basset-sex classifier from a combination of the population distribution and Bayes’ rule. This is the essence of generative classifiers - if we know the distribution, accurate probabilistic classification is a straightforward application of Bayes’ rule. In practice, we don’t know the distribution a priori and it must be estimated from data. Different generative classifiers essentially are just different ways of estimating the population (mixture) distribution."
  },
  {
    "objectID": "notes/notes06.html#building-generative-classifiers",
    "href": "notes/notes06.html#building-generative-classifiers",
    "title": "STA 9890 - Generative Classifiers",
    "section": "Building Generative Classifiers",
    "text": "Building Generative Classifiers\nLet’s formalize our approach for a two-class classifier though the extension to multi-class is simple enough. Given a training set\n\\[\\mathcal{D}_{\\text{Train}} = \\{(\\bx_1, y_1), (\\bx_2, y_2), \\dots, (\\bx_n, y_n)\\}\\]\nwe assume \\(\\bX\\) follows a two-component mixture where \\(Y\\) is the mixing variable:\n\\[\\bX | Y=0 \\sim \\mathcal{P}_0 \\text{ and } \\bX | Y=1 \\sim \\mathcal{P}_1\\]\nClearly to fully specify this distribution, we need three things:\n\nTo know the distribution of \\(Y\\)\nTo know the distribution \\(\\mathcal{P}_0\\)\nTo know the distribution \\(\\mathcal{P}_1\\)\n\nThe distribution of \\(Y\\) is relatively straightforward: since it only takes values \\(\\{0, 1\\}\\) it must have a Bernoulli distribution and hence the only thing we need to estimate is the probability, \\(\\pi_1\\), that \\(Y=1\\). The simplest approach to estimating the Bernoulli probability is to simply take the empirical probability from the training data set, i.e., the fraction of observations where \\(Y=1\\), but this can also be estimated using knowledge of a broader data set or a different population than the training data. For instance, if a data set is known to overindex on \\(Y=1\\), e.g., because it contains patients who self-reported some disease, it may be more appropriate to estimate \\(p\\) based on the population-wide prevalance of the disease. These concerns are typically rather problem specific.\nEstimation of \\(\\mathcal{P}_0, \\mathcal{P}_1\\) may be more difficult. If the training data is extremely large, we may use a non-parametric estimator, e.g., a multivariate histogram, but in general we will need to use some sort of restrictive (parametric) model. The most commonly used generative classifiers assume some sort of multivariate normal structure, but you can create ‘custom’ classifiers appropriate to specific domains.\nFor now, let us assume \\(\\mathcal{P}_{0}, \\mathcal{P}_1\\) have been estimated and that they have PDFs \\(p_0(\\cdot), p_1(\\cdot)\\). Applying Bayes’ rule as before, with prior (baseline) probabilities \\(\\pi_0, \\pi_1 = 1-\\pi_0\\), we get the following probabilities\n\\[\\begin{align*}\n\\P(Y = 1 | \\bx) &= \\frac{\\P(\\bx | Y = 1)\\P(Y=1)}{\\P(\\bx | Y = 0)\\P(Y=0)+\\P(\\bx | Y = 1)\\P(Y=1)} \\\\\n&= \\frac{p_1(\\bx)\\pi_1}{p_0(\\bx)\\pi_0 + p_1(\\bx)\\pi_1}\n\\end{align*}\\]\nIf we are happy with a probabilistic classifier, we are essentially done. If we want a true binary classifier, i.e., a \\(\\{0, 1\\}\\) valued ‘point predictor’, we also need to set a classification threshold, \\(\\overline{p}\\), to obtain:\n\\[\\delta(\\bx) = \\begin{cases} 0 & \\frac{p_1(\\bx)\\pi_1}{p_0(\\bx)\\pi_0 + p_1(\\bx)\\pi_1} &lt; \\overline{p} \\\\ 1 & \\text{ otherwise} \\end{cases}\\]\n\\(\\overline{p}\\) can be chosen to optimize a relevant performance metric (if you practice good sample splitting hygiene) or via a full decision theoretic analysis, as discussed last week.\nThat’s basically it!"
  },
  {
    "objectID": "notes/notes06.html#footnotes",
    "href": "notes/notes06.html#footnotes",
    "title": "STA 9890 - Generative Classifiers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that a mixture of normals is not always multimodal (e.g., if the two means are very close together), but a mixture of normals is a common model for multimodal data.↩︎\nSometimes, we go further and assume the standard deviations of each feature are the same: this assumption is typically not made explicitly as such but is done ‘off-camera’ by pre-standardizing features.)↩︎"
  },
  {
    "objectID": "notes/notes07.html",
    "href": "notes/notes07.html",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "",
    "text": "\\[\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\P}{\\mathbb{P}}\\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\by}{\\mathbf{y}}\\newcommand{\\argmax}{\\text{arg\\,max}}\\newcommand{\\argmin}{\\text{arg\\,min}}\\]\nThis week we begin to examin discriminative classifiers. Unlike generative classifiers, which attempt to model \\(\\bx\\) as a function of \\(y\\) and then use Bayes’ rule to invert that model, discriminative classifiers take the more standard approach of directly modeling \\(y\\) as a function of \\(\\bx\\). This is the approach you have already seen in our various variants of OLS."
  },
  {
    "objectID": "notes/notes07.html#why-not-accuracy-maximization",
    "href": "notes/notes07.html#why-not-accuracy-maximization",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "Why Not Accuracy Maximization?",
    "text": "Why Not Accuracy Maximization?\nAs we saw in our regression unit, we can pose many interesting estimators as loss minimization problems. For least squares, we primarily focused on mean squared error (MSE) as a loss function, though we also briefly touched on MAE, MAPE, and ‘check’ losses. There is not a single ‘canonical’ loss for the classification setting and different choices of loss will yield different classifiers.\nBefore we build our actual first loss, it is worth asking why we can’t use something like classification accuracy as our loss function. Specifically, define \\[\\text{Acc}(y, \\hat{y}) = \\begin{cases} 1 & y = \\text{sign}(\\hat{y}) \\\\ 0 & y \\neq \\text{sign}(\\hat{y}) \\end{cases}\\] Here we consider the case where \\(\\hat{y}\\) may be real-valued as a (slight) generalization of probabilistic classification and we use the \\(\\pm 1\\) convention for the two classes.\nFixing \\(y = 1\\), \\(\\text{Acc}\\) has the following shape:\n\ny_hat &lt;- seq(-3, 3, length.out=201)\nacc &lt;- function(y_hat, y=1) sign(y_hat) == sign(y)\n\nplot(y_hat, acc(y_hat), type=\"l\",\n     xlab=expression(hat(y)),\n     ylab=expression(Acc(hat(y), y==1)), \n     main=\"Accuracy as a Loss Function\")\n\n\n\n\n\n\n\n\nThis is a terrible loss function! It is both non-convex (can you see why?) and just generally unhelpful. Because it is flat almost everywhere, we have no access to gradient information (useful for fitting) or to any sense of ‘sensitivity’ to our loss. In this scenario, if \\(y=1\\), both \\(\\hat{y}=0.00001\\) and \\(\\hat{y}=0.9999\\) have the same accuracy even though the latter is a more ‘confident’ prediction."
  },
  {
    "objectID": "notes/notes07.html#ols-for-classification",
    "href": "notes/notes07.html#ols-for-classification",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "OLS for Classification?",
    "text": "OLS for Classification?\nSo if not accuracy, where else might we get a good loss function? You might first ask if we an use OLS? It works perfectly well for regression and we know it is well-behaved, so what happens if we try it for classification? The answer is … complicated.\nOLS as a loss function for classification is a bit strange, but it turns out to be more-or-less fine. In particular, OLS can be used to evaluate probabilistic classifiers, where it is known as the Brier score. OLS as a predictor can be a bit more problematic: in particular, if we just predict with \\(\\P(y=1) = \\bx^{\\top}\\bbeta\\), we have to deal with the fact that \\(\\hat{y}\\) can be far outside a \\([0, 1]\\) range we might want from a probabilistic classifier. In certain limited circumstances, we can assume away this problem (perhaps by putting specific bounds on \\(\\bx\\)), but these fixes are fragile. Alternatively, we can try to ‘patch’ this approach and use a classifier like\n\\[\\P(y=1) = \\begin{cases} 1 & \\bx^{\\top}\\bbeta &gt; 1 \\\\ 0 & \\bx^{\\top}\\bbeta &lt; 0 \\\\ \\bx^{\\top}{\\bbeta} & \\text{ otherwise}\\end{cases}\\]\nEven if this feels a bit unsophisticated, Ttis is not awful, but it still struggles from the ‘long flat region’ problems that accuracy encounters.\nAt this point, it’s hopefully clear that it will be a bit hard to ‘hack together’ a suitable loss and that we might benefit from approaching the problem with more theoretical grounding."
  },
  {
    "objectID": "notes/notes07.html#deriving-logistic-regression",
    "href": "notes/notes07.html#deriving-logistic-regression",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "Deriving Logistic Regression",
    "text": "Deriving Logistic Regression\nWhile we argued that OLS is well-grounded in MSE alone, we can recall it has additional connections to the Gaussian. These “Gaussian vibes” were not essential to making OLS work, but they were useful in deriving OLS as a sensible procedure. Can we do something similar for classification? That is, can we assume a ‘working model’ to come up with a good loss function and then use it, even if we doubt that the model is ‘correct’?\nAs with all leading questions, the answer is of course a resounding yes. We start from the rather banal observation that \\(y\\) must have a Bernoulli distribution conditional on \\(\\bx\\). After all, the Bernoulli is (essentially) the only \\(\\{0, 1\\}\\) distribution we have to work with. So we really just need a way to model the \\(p\\) parameter of a Bernoulli as a function of \\(\\bx\\). Because we love linear models, we may choose to set \\(p = \\bx^{\\top}\\bbeta\\), but this gets us back to the range problem we had above.\nSpecifically, we require the Bernoulli parameter to take values in \\([0, 1]\\) but our linear predictor \\(\\bx^{\\top}\\bbeta\\) takes values in all of \\(\\R\\). We can fix this with a simple ‘hack’: we need a function that ‘connects’ \\(\\R\\) to \\([0, 1]\\). If we call this function \\(\\mu\\), we then can specify our whole model as \\[y | \\bx \\sim \\text{Bernoulli}(\\mu(\\bx^{\\top}\\bbeta))\\] and reduce our problem to estimating \\(\\bbeta\\). Where can we get such a function?\nThe most common choice is \\[\\mu(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1+e^{-z}}\\], which is known as the logistic or sigmoid function due to its ‘s-like’ shape:\n\nz &lt;- seq(-5, 5, length.out=201)\nplot(z, 1/(1+exp(-z)), type=\"l\", main=\"Sigmoid Mapping\")\n\n\n\n\n\n\n\n\nMore generally, we can use essentially the CDF of any random variable supported on the real line as a choice of \\(\\mu\\): since CDFs map the support onto the \\([0, 1]\\) range they are perfect for this choice. Unfortunately, most CDFs are a bit unwieldy so this approach, while theoretically satisfying, is not too widely used in practice. If you see it, the most common choice is \\(\\mu(z) = \\Phi(z)\\), the normal CDF, resulting in a method known as “probit” regression (after ‘probability integral transform,’ an old-fashioned name for the CDF) or \\[\\mu(z) = \\frac{\\tan^{-1}(z)}{\\pi} + \\frac{1}{2}\\] which gives a method known as “cauchit” regression, since this is the CDF of the standard Cauchy distribution. These choices are far less common than the default sigmoid we used above. Generally, they are only slightly different in practice and far more computationally burdensome and just aren’t really worth it.\nReturning to our default sigmoid, we now have the model \\[ y | \\bx \\sim \\text{Bernoulli}\\left(\\frac{1}{1+e^{-\\bx^{\\top}\\bbeta}}\\right)\\] This model is well-posed (in the sense that the distribution ‘fits’ the data and we are guaranteed never to put invalid parameters in) but we don’t yet have a way to use it as a loss function."
  },
  {
    "objectID": "notes/notes07.html#from-model-to-loss-function",
    "href": "notes/notes07.html#from-model-to-loss-function",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "From Model to Loss Function",
    "text": "From Model to Loss Function\nWe can build a loss-function by relying on the maximum likelihood principle. The maximum likelihood principle is a core idea of statistics - and one you will explore in much greater detail in other courses - but, in essence, it posits that we should use the (negative) PMF or PDF as our loss function. Specifically, the ML principle says that, if many models could fit our data, we should pick the one that makes our data most probable (as determined by the PMF/PDF).\nBefore we work out the ML estimator (MLE) for our classification model, let’s take a brief detour and look at the MLE for (Gaussian) regression. Specifically, if we assume a model \\[y \\sim \\mathcal{N}(\\bx^{\\top}\\bbeta, \\sigma^2)\\] for known \\[\\sigma^2\\], the PDF of the training point \\((\\bx_1, y_1)\\) is\n\\[p(y_1 | \\bx_1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{\\|y_1 - \\bx_1^{\\top}\\bbeta\\|_2^2}{2\\sigma^2}\\right\\}\\]\nIf we have a set of \\(n\\) IID training data points, the joint PDF can be obtained by multiplying together PDFs (IID is great!) to get\n\\[\\begin{align*}\np(\\mathcal{D}_{\\text{train}}) &= \\prod_{i=1}^n p(y_i | \\bx_i) \\\\\n&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{\\|y_i - \\bx_i^{\\top}\\bbeta\\|_2^2}{2\\sigma^2}\\right\\} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\|y_i - \\bx_i^{\\top}\\bbeta\\|_2^2\\right\\} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2} \\|\\by - \\bX\\bbeta\\|_2^2\\right\\} \\\\\n\\end{align*}\\]\nWe could maximize this, but a few minor tweaks make the problem much easier:\n\nSince all of the action is in an exponential, we might as well maximize the log of the PDF instead of the raw PDF. Since logarithms are a monotonic transformation, this won’t change our minimizer. Similarly, we will also strategically drop some constant terms as they also do not change the minimizer.\nBy convention, we like to minimize more than maximize, specifically when dealing with convex approaches, so we will introduce a sign flip.\n\n\\[\\begin{align*}\n\\hat{\\bbeta} &= \\argmax_{\\bbeta} p(\\mathcal{D}_{\\text{train}}) \\\\\n             &= \\argmax_{\\bbeta} \\log p(\\mathcal{D}_{\\text{train}}) \\\\\n             &= \\argmax_{\\bbeta} \\log\\left( (2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\|\\by - \\bX\\bbeta\\|_2^2\\right\\}\\right) \\\\\n             &= \\argmax_{\\bbeta} -\\frac{n}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\|\\by - \\bX\\bbeta\\|_2^2 \\\\\n             &= \\argmin_{\\bbeta} \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\|\\by - \\bX\\bbeta\\|_2^2 \\\\\n             &= \\argmin_{\\bbeta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2\n\\end{align*}\\]\nWhy were we able to do the simplifications in the last line? (You might also recognize the penultimate line as a term used in the AIC of a linear model.)\nThis is quite cool! If we assume \\(\\by\\) follows a particular Gaussian, we get OLS by applying the general MLE approach. Deeper statistical theory tells us that the MLE is (essentially) optimal, so if our data is not too ‘un-Gaussian’ it makes sense that the Gaussian MLE (OLS) will perform reasonably well. Since many things in this world are Gaussian-ish, OLS is optimal-ish for a large class of interesting problems.\nReturning to our classification problem, we see that if our training data is IID, the MLE can be obtained by minimizing the negative sum of the log PDFs. For space, we often start from this point instead of doing the full set of manipulations from scratch.\nSo what is the Bernoulli log PMF? Recall that if \\(B \\sim \\text{Bernoulli}(p)\\), its PMF is given by:\n\\[\\P(B=k) = \\begin{cases} p & k=1 \\\\ 1-p & k = 0 \\end{cases}\\]\nor more compactly,\n\\[\\P(B = k) = p^k(1-p)^(1-k)\\]\nTaking logarithms, we have\n\\[\\log \\P(B = k) = k \\log p + (1-k) \\log(1-p)\\]\nand hence the negative log-likelihood:\n\\[-\\log \\P(B = k) = -k \\log p -(1-k) \\log(1-p)\\]\nFrom our working model, we take \\(p = 1/(1+e^{-\\bx^{\\top}\\bbeta})\\) so our joint MLE is given by:\n\\[\\hat{\\bbeta} = \\argmin_{\\bbeta} \\sum_{i=1}^n -y_i \\log\\left(1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right) - (1-y_i)\\log\\left(1 - 1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right)\\]\nThis is still quite hairy, so let’s simplify it. In the first term, we note that \\(-\\log(1/x) = \\log x\\) to get:\n\\[\\hat{\\bbeta} = \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) - (1-y_i)\\log\\left(1 - 1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right)\\]\nFor the second term, note that\n\\[1-\\frac{1}{1+e^{-z}} = \\frac{1+e^{-z} - 1}{1+e^{-z}} = \\frac{e^{-z}}{1+e^{-z}} \\implies \\log\\left(1-\\frac{1}{1+e^{-z}}\\right) = -z - \\log(1 + e^{-z})\\]\nso we get:\n\\[\\begin{align*}\n\\hat{\\bbeta} &=  \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) - (1-y_i)\\log\\left(1 - 1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right) \\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) - (1-y_i)\\left[-\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right] \\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) + (1-y_i)\\left[\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right] \\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) + \\left[\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right] -y_i\\left[\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right]\\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n-y_i \\bx_i^{\\top}\\bbeta + \\bx_i^{\\top}\\bbeta + \\log\\left(1-e^{-\\bx_i^{\\top}\\bbeta}\\right)\n\\end{align*}\\]\nWe could work in this form, but it turns out to actually be a bit nicer to ‘invert’ some of the work we did above to get rid of a term. In particular, note\n\\[z + \\log(1-e^{-z}) = \\log(e^z) + \\log(1-e^{-z}) = \\log(e^z + e^{z-z}) = \\log(1 + e^z)\\]\nwhich gives us:\n\\[\\hat{\\bbeta} = \\argmin_{\\bbeta} \\sum_{i=1}^n-y_i \\bx_i^{\\top}\\bbeta  + \\log\\left(1+e^{\\bx_i^{\\top}\\bbeta}\\right)\\]\nWow! Long derivation. And we still can’t actually solve this!\nUnlike OLS, where we could obtain a closed-form solution, we have to use an iterative algorithm here. While we could use gradient descent, we can get to an answer far more rapidly if we use more advanced approaches. Since this is not an optimization course, we’ll instead use some software to solve for us:\n\n# Generate data from the logistic model\nn &lt;- 500\np &lt;- 5\nX    &lt;- matrix(rnorm(n * p), ncol=p)\nbeta &lt;- c(1, 2, 3, 0, 0)\n\nP &lt;- 1/(1+exp(-X %*% beta))\ny &lt;- rbinom(n, size=1, prob=P)\n\n# Use optimization software\n# See https://cvxr.rbind.io/cvxr_examples/cvxr_logistic-regression/ for details\nlibrary(CVXR)\n\n\nAttaching package: 'CVXR'\n\n\nThe following object is masked from 'package:stats':\n\n    power\n\nbeta &lt;- Variable(p)\neta  &lt;- X %*% beta\n\nobjective &lt;- sum(- y * eta + logistic(eta))\nproblem   &lt;- Problem(Minimize(objective))\n\nbeta_hat &lt;- solve(problem)$getValue(beta)\n\n\n\n\n\n\nbeta_hat\n\n\n\n\n1.0574628\n\n\n2.0266554\n\n\n2.7117364\n\n\n-0.0225406\n\n\n0.1103746\n\n\n\n\n\nNot too bad, if we compare this to R’s built-in logistic regression function, we see our results basically match:\n\nsummary(glm(y ~ X + 0, family=binomial))\n\n\nCall:\nglm(formula = y ~ X + 0, family = binomial)\n\nCoefficients:\n   Estimate Std. Error z value Pr(&gt;|z|)    \nX1  1.05746    0.17219   6.141 8.19e-10 ***\nX2  2.02666    0.22207   9.126  &lt; 2e-16 ***\nX3  2.71174    0.26442  10.255  &lt; 2e-16 ***\nX4 -0.02254    0.13977  -0.161    0.872    \nX5  0.11037    0.14815   0.745    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 500  degrees of freedom\nResidual deviance: 319.95  on 495  degrees of freedom\nAIC: 329.95\n\nNumber of Fisher Scoring iterations: 6\n\n\nGood match!\nNote that, in the above, we had to use the logistic function built into CVX. If we used log(1+exp(eta)), the solver would not be able to prove that it is convex and would refuse to try to solve. See the CVX documentation for more details.\nWe can compute accuracy after defining a ‘decision rule’. Here, let’s just round the predicted probability\n\np_hat &lt;- 1/(1 + exp(-X %*% beta_hat))\ny_hat &lt;- round(p_hat)\n\nmean(y_hat == y)\n\n[1] 0.856\n\n\nNow that we have this toolkit built up, we can also easily apply ridge and lasso penalization:\n\npenalty &lt;- sum(abs(beta))\n\nlasso_problem &lt;- Problem(Minimize(objective + 9 * penalty))\nbeta_hat_lasso &lt;- solve(lasso_problem)$getValue(beta)\n\n\n\n\n\n\n0.6700834\n\n\n1.4177753\n\n\n1.9420811\n\n\n0.0000000\n\n\n0.0000000\n\n\n\n\n\nWe got some sparsity, as we would expect with \\(\\ell_1\\) penalization."
  },
  {
    "objectID": "notes/notes07.html#generalized-linear-models",
    "href": "notes/notes07.html#generalized-linear-models",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nThe approach we took here is a special case of a generalized linear model: generalized linear models (GLMs) consist of three parts:\n\nA linear model for the ‘core predictor’ \\(\\eta = \\bX\\bbeta\\);\nA sampling distribution for the data \\(y \\sim \\text{Dist}\\); and\nAn ‘adaptor’ function1, \\(\\mu\\), that maps \\(\\eta \\in \\R\\) to the parameter space of \\(y\\): specifically, we need \\(\\E[y | \\bx] = \\mu(\\bx^{\\top}\\bbeta)\\).\n\nThe adaptor function needs to be smooth (for nice optimization properties) and monotonic (or else the interpretation gets too weird), but we otherwise have some flexibility. As noted above, if we pick \\(\\mu\\) to be the normal or Cauchy CDFs, we get alternative ‘Bernoulli Regressions’ (probit and cauchit).\nWe can generalize the model for \\(\\eta\\) by allowing it to be an additive (spline) model or a kernel method. To fit splines, you can use the gam function from the mgcv function.\nFinally, we also have choices in the sampling distribution. While normal and Bernoulli are the most common, you will also sometimes see Poisson and Gamma in the wild. For both of these, the mean is a positive value, so we require an adaptor that maps \\(\\R\\) to \\(\\R \\geq 0\\) and we typically take \\(\\mu(z) = e^z\\).\nA good exercise is to repeat the above analysis for Poisson regression and compare your result (obtained with CVXR) to the Poisson regression built into R."
  },
  {
    "objectID": "notes/notes07.html#loss-functions-for-classification",
    "href": "notes/notes07.html#loss-functions-for-classification",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "Loss Functions for Classification",
    "text": "Loss Functions for Classification\nWe now have a workable loss function for discriminative classification. Returning to our example from above (changed to \\(\\{0, 1\\}\\) convention briefly). Let us set \\(\\alpha = y\\hat{y}\\) and investigate our loss functions as a property of \\(\\alpha\\):\n\nalpha &lt;- seq(-3, 3, length.out=201)\nacc     &lt;- function(alpha) ifelse(alpha &lt; 0, 1, 0)\nlr_loss &lt;- function(alpha) log(1 + exp(-2*alpha))\n\nplot(alpha, acc(alpha), type=\"l\",\n     xlab=expression(alpha),\n     ylab=\"Loss\", \n     main=\"Classification Losses\", \n     ylim=c(0, 5))\nlines(alpha, lr_loss(alpha), col=\"red4\")\nlegend(\"topright\", \n       col=c(\"black\", \"red4\"), \n       lwd=2, \n       legend=c(\"Accuracy\", \"Logistic Regression\"))\n\n\n\n\n\n\n\n\nWe see here that the logistic loss defines a convex surrogate of the underlying accuracy. Just like we used \\(\\ell_1\\) as a tractable approximation for best squares, logistic regression can be considered a tractable approximation for accuracy minimization.2\nOther useful loss functions can be created using this perspective, perhaps none less important than the hinge loss, which gives rise to a classifier known as the support vector machine (SVM)."
  },
  {
    "objectID": "notes/notes07.html#footnotes",
    "href": "notes/notes07.html#footnotes",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor historical reasons, this is known as the inverse link, but I prefer to simply think of it as an adaptor and to never use the (forward) link.↩︎\nThis ‘surrogate loss’ perspective was first discussed in: P.L. Barlett, M.I. Jordan, and J.D. McAuliffe. “Convexity, Classification, and Risk Bounds”. Journal of the American Statistical Association 101(473), pp.138-156. 2006. DOI:10.1198/016214505000000907↩︎"
  },
  {
    "objectID": "notes/notes07.html#support-vector-machines",
    "href": "notes/notes07.html#support-vector-machines",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nIn this section, we go back to \\(\\{\\pm 1\\}\\) convention.\nInstead of using a logistic loss, consider a hinge loss of the form\n\\[H(y, \\hat{y}) = (1 - y\\hat{y})_+\\]\nWhen \\(y = \\hat{y}=1\\) or \\(y = \\hat{y} = 0\\), this is clearly 0 as we would expect from a good loss. What happens for cases where our prediction is wrong or not ‘full force’, say \\(\\hat{y} = 1/2\\).\nLooking ahead, we will use a linear combination of features to create \\(\\hat{y} = \\bx^{\\top}\\bbeta\\) yielding:\n\\[H(\\bbeta) = (1 - y \\bx^{\\top}\\bbeta)_+\\]\nIf \\(y=1\\), we get zero loss so long as \\(\\bx^{\\top}\\bbeta &gt; 1\\) and a small loss for \\(0 &lt; \\bx^{\\top}\\bbeta &lt; 1\\). As \\(\\bx^{\\top}\\bbeta\\) crosses zero and goes negative, the loss grows linearly without bound. (Reverse all of this for the case \\(y = -1\\).) This is an interesting loss function: we cannot make our loss decrease as our predictions become ‘more right’, but our loss continues to increase as our prediction becomes ‘more wrong’.\nVisually, we can draw this on our plot from above:\n\nalpha &lt;- seq(-3, 3, length.out=201)\nacc     &lt;- function(alpha) ifelse(alpha &lt; 0, 1, 0)\nlr_loss &lt;- function(alpha) log(1 + exp(-2*alpha))\nhinge_loss &lt;- function(alpha) pmax(0, 1-alpha)\n\nplot(alpha, acc(alpha), type=\"l\",\n     xlab=expression(alpha),\n     ylab=\"Loss\", \n     main=\"Classification Losses\", \n     ylim=c(0, 5))\nlines(alpha, lr_loss(alpha), col=\"red4\")\nlines(alpha, hinge_loss(alpha), col=\"green4\")\nlegend(\"topright\", \n       col=c(\"black\", \"red4\", \"green4\"), \n       lwd=2, \n       legend=c(\"Accuracy\", \"Logistic Regression\", \"Hinge Loss\"))\n\n\n\n\n\n\n\n\nThis is also a pretty nice surrogate loss. In this case, let’s just go ‘straight at it’ to construct a classifier:\n\\[\\hat{\\bbeta} = \\argmin \\sum_{i=1}^n (1-y_i \\bx_i^{\\top}\\bbeta)_+\\]\nThis classifier is not uniquely defined for many problems (because of the long flat part of the loss), so it is conventional to add a regularization term to the SVM:\n\\[\\hat{\\bbeta} = \\argmin \\sum_{i=1}^n (1-y_i \\bx_i^{\\top}\\bbeta)_+ + \\lambda \\|\\bbeta\\|_2^2\\]\nWe can solve this directly using CVX as follows:\n\n# Generate data from the logistic model and fit an SVM\nn &lt;- 500\np &lt;- 5\nX    &lt;- matrix(rnorm(n * p), ncol=p)\nbeta &lt;- c(1, 2, 3, 0, 0)\n\nP &lt;- 1/(1+exp(-X %*% beta))\ny &lt;- rbinom(n, size=1, prob=P)\n## Convert to +/-1 convention\ny &lt;- 2 * y - 1\n\nlibrary(CVXR)\n\nbeta &lt;- Variable(p)\neta  &lt;- X %*% beta\n\nobjective &lt;- sum(pos(1 - y * eta)) + 0.5 * norm2(beta)^2\nproblem   &lt;- Problem(Minimize(objective))\n\nbeta_hat &lt;- solve(problem)$getValue(beta)\n\nUsing the decision rule \\(\\hat{y} = \\text{sign}(\\bx^{\\top}\\bbeta)\\), this gives us good accuracy:\n\nmean(sign(X %*% beta_hat) == y)\n\n[1] 0.868\n\n\nThis is comparable to what we got for logistic regression even though:\n\nthe loss function is ‘wrong’ (doesn’t model the DGP)\nwe did no tuning of the ridge parameter\n\nSo, all in all, pretty impressive.\nSVMs are a remarkably powerful and general classification technology. Before we move past them, we will now take a second perspective focusing on the geometry of SVMs."
  },
  {
    "objectID": "notes/notes07.html#svm-the-geometric-viewpoint",
    "href": "notes/notes07.html#svm-the-geometric-viewpoint",
    "title": "STA 9890 - Discriminative Classifiers",
    "section": "SVM: The Geometric Viewpoint",
    "text": "SVM: The Geometric Viewpoint"
  },
  {
    "objectID": "competition.html#data",
    "href": "competition.html#data",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "Data",
    "text": "Data\nIn the United States, local taxes are typically collected using an ad valorem tax on real property (land and buildings). Ad valorem means that the tax amount is determined as a fixed percentage of the value of the property, almost always taken to be the fair market value.1 Hence, to determine taxes, government authorities must undertake an annual assessment process.\nTypically, assessment procedes as follows:\n\nEarly in the year, the local assessment authority estimates the fair market value of each property in the relevant area and informs the owner\nThe owner then has the option to contest (variously called “grieve”, “protest”, or “challenge”) the assessment in front of a neutral party\nThe neutral party then assigns an updated valuation to contested properties\nThe local assessment authority collects unchallenged and new assessment values and uses them to create property tax bills\nThe assessment authority sends tax bills to the relevant tax collector who then collects taxes from property owners\n\nFor large regions, this assessment process typically relies (at least in part) on some sort of automated process. In this task, you will be predicting the (finalized) assessment values of residental properties for 2019. You will be provided with assessment values for 2015-2018 (4 years) as well as various additional information about the underlying property.\nWhile assessment values are typically relatively constant over time (growing at a rate proportional to local housing prices), large assessment shifts can occur if the property is remodeled, rebuilt, or otherwise significantly altered. In the region of interest, assessments are the sum of a building assessment and a land assessment, which may not move in lockstep.\nThe following files are provided:\n\nbuilding_details_2015.csv: Details of the relevant properties in 2015\nbuilding_details_2016.csv: Details of the relevant properties in 2016\nbuilding_details_2017.csv: Details of the relevant properties in 2017\nbuilding_details_2018.csv: Details of the relevant properties in 2018\nbuilding_details_2019.csv: Details of the relevant properties in 2019\nassessment_history_train.csv: A subset of realized assessments from 2015 to 2019, along with some additional useful information\nassessment_history_test.csv: A subset of realized assessments from 2015 to 2018. Your task is to predict the 2019 assessments for these properties.\n\nLet us review these in some detail:\n\nlibrary(readr); library(dplyr)\nglimpse(read_csv(\"competition_data/building_details_2015.csv.gz\"))\n\nRows: 990,765\nColumns: 27\n$ acct                &lt;chr&gt; \"bb75f25168addc1117840b10c0fd6cd0c2a7b7c6\", \"5dd76…\n$ floor_area_primary  &lt;dbl&gt; 1658, 912, 1496, 1517, 1508, 1670, 1164, 1434, 272…\n$ floor_area_upper    &lt;dbl&gt; 879, 0, 0, 1870, 0, 0, 1164, 0, 272, 1176, 272, 27…\n$ floor_area_lower    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10…\n$ garage_area         &lt;dbl&gt; 0, 0, 0, 529, 420, 0, 0, 0, 380, 0, 380, 380, 0, 0…\n$ porch_area          &lt;dbl&gt; 266, 48, 182, 174, 0, 64, 0, 28, 60, 332, 60, 60, …\n$ deck_area           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mobile_home_area    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ floors              &lt;dbl&gt; 2, 2, 1, 2, 1, 1, 2, 1, 3, 2, 3, 3, 3, 2, 1, 1, 3,…\n$ half_bath           &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,…\n$ full_bath           &lt;dbl&gt; 2, 2, 1, 3, 2, 2, 3, 1, 3, 2, 3, 3, 3, 2, 1, 1, 3,…\n$ total_rooms         &lt;dbl&gt; 8, 6, 6, 6, 5, 5, 8, 6, 5, 9, 5, 5, 12, 8, 5, 6, 8…\n$ bedrooms            &lt;dbl&gt; 3, 3, 3, 3, 3, 2, 4, 2, 3, 4, 3, 3, 6, 4, 2, 3, 3,…\n$ fireplaces          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,…\n$ elevator            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ quality             &lt;chr&gt; \"B\", \"D\", \"D\", \"B\", \"C\", \"D\", \"B\", \"D\", \"B\", \"B\", …\n$ quality_description &lt;chr&gt; \"Good\", \"Low\", \"Low\", \"Good\", \"Average\", \"Low\", \"G…\n$ year_built          &lt;dbl&gt; 2004, 1949, 1917, 2015, 2003, 1940, 2014, 1920, 19…\n$ year_remodeled      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 2004, 0, 1996, 0, 0, 0, NA, 0…\n$ building_condition  &lt;chr&gt; \"Fair\", \"Fair\", \"Poor\", \"Average\", \"Fair\", \"Fair\",…\n$ foundation_type     &lt;chr&gt; \"Slab\", \"Crawl Space\", \"Crawl Space\", \"Slab\", \"Sla…\n$ grade               &lt;chr&gt; \"B-\", \"D-\", \"D\", \"B\", \"C\", \"D\", \"B\", \"D+\", \"B+\", \"…\n$ has_cooling         &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE,…\n$ has_heat            &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE,…\n$ physical_condition  &lt;chr&gt; \"Fair\", \"Fair\", \"Poor\", \"Average\", \"Average\", \"Fai…\n$ exterior_walls      &lt;chr&gt; \"Stucco\", \"Concrete Block\", \"Concrete Block\", \"Bri…\n$ year                &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 20…\n\n\nFeatures here are generally self-explanatory with the exception of acct, a unique identifier I have created to identify assessment parcels. You will use this as the “key” to identify your predictions.\nNote that these data are obtained from government records and are likely full of errors and irregularities (e.g., the year_remodeled column has many zeros); part of your task is to deal with this messiness in building your ML pipeline. I will not further clean or prepare this data for you.\nThe training file looks like:\n\nlibrary(readr); library(dplyr)\nglimpse(read_csv(\"competition_data/assessment_history_train.csv.gz\"))\n\nRows: 628,287\nColumns: 37\n$ acct                &lt;chr&gt; \"44b70ecbb15f5815f76a5982f4a80f4c48cd6958\", \"a126c…\n$ building_area_2015  &lt;dbl&gt; 1853, 1282, 1705, 1525, 2873, NA, 3955, 1659, 1935…\n$ land_area_2015      &lt;dbl&gt; 4263, 6780, 11400, 6050, 12371, NA, 9360, 2250, 86…\n$ building_area_2016  &lt;dbl&gt; 1853, 1282, 1705, 1525, 2873, NA, 3955, 1659, 1935…\n$ land_area_2016      &lt;dbl&gt; 4263, 6780, 11400, 6050, 12371, NA, 9360, 2250, 86…\n$ building_area_2017  &lt;dbl&gt; 1853, 1282, 1705, 1525, 2873, NA, 3955, 1659, 1935…\n$ land_area_2017      &lt;dbl&gt; 4263, 6780, 11400, 6050, 12371, NA, 9360, 2654, 86…\n$ building_area_2018  &lt;dbl&gt; 1853, 1282, 1705, 1525, 2873, NA, 3955, 1659, 1935…\n$ land_area_2018      &lt;dbl&gt; 4263, 6780, 11400, 6050, 12371, NA, 9360, 2654, 86…\n$ region              &lt;chr&gt; \"05946c0909f5c241db6179659dc763e14544cf75\", \"7fb55…\n$ building_area_2019  &lt;dbl&gt; 1853, 1282, 1705, 1525, 2873, 2434, 3955, 1659, 19…\n$ land_area_2019      &lt;dbl&gt; 4263, 6780, 11400, 6050, 12371, 5292, 9360, 2654, …\n$ building_value_2015 &lt;dbl&gt; 80939, 74056, 49042, 85632, 141722, NA, 536000, 33…\n$ land_value_2015     &lt;dbl&gt; 22061, 13526, 11325, 21368, 23175, NA, 464000, 770…\n$ building_value_2016 &lt;dbl&gt; 80939, 74308, 53543, 116825, 141722, NA, 490000, 2…\n$ land_value_2016     &lt;dbl&gt; 22061, 13526, 11325, 25988, 23175, NA, 464000, 770…\n$ building_value_2017 &lt;dbl&gt; 97988, 78545, 77254, 132614, 141722, NA, 551000, 3…\n$ land_value_2017     &lt;dbl&gt; 24512, 13526, 11325, 25988, 23175, NA, 464000, 847…\n$ building_value_2018 &lt;dbl&gt; 97988, 78545, 77254, 128475, 141722, NA, 551000, 3…\n$ land_value_2018     &lt;dbl&gt; 24512, 13526, 11325, 25988, 23175, NA, 464000, 847…\n$ building_value_2019 &lt;dbl&gt; 120488, 70374, 77390, 127337, 166403, 202963, 5260…\n$ land_value_2019     &lt;dbl&gt; 24512, 12580, 11325, 25988, 23175, 22631, 464000, …\n$ assessed_2015       &lt;dbl&gt; 103000, 92171, 64968, 107000, 167732, NA, 1000000,…\n$ protested_2015      &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, NA, TRUE, TRUE, F…\n$ assessed_2016       &lt;dbl&gt; 103000, 92171, 64968, 142813, 167732, NA, 954000, …\n$ protested_2016      &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA, TRUE, FALSE,…\n$ assessed_2017       &lt;dbl&gt; 122500, 92171, 93038, 158602, 167732, NA, 1015000,…\n$ protested_2017      &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA, TRUE, FALSE,…\n$ assessed_2018       &lt;dbl&gt; 122500, 92171, 93038, 154463, 167732, NA, 1015000,…\n$ protested_2018      &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, NA, TRUE, FALSE, …\n$ assessed_2019       &lt;dbl&gt; 145000, 87291, 93038, 153325, 192413, 225594, 9900…\n$ protested_2019      &lt;lgl&gt; TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FALS…\n$ school_dist         &lt;dbl&gt; 8, 9, 1, 8, 24, 5, 25, 9, 9, 5, 4, 1, 9, 1, 16, 1,…\n$ zone                &lt;chr&gt; \"1b2fc12a8ee5147b030dae24f76bfb63bf401fdd\", \"34827…\n$ subneighborhood     &lt;chr&gt; \"9bd56df07606b41647719f2022859ef0571cf82c\", \"25f65…\n$ neighborhood        &lt;chr&gt; \"78bde8025e65c00938c9f5a63f652c33c7b7fc61\", \"5495e…\n$ TARGET              &lt;dbl&gt; 145000, 87291, 93038, 153325, 192413, 225594, 9900…\n\n\nHere the acct identifier matches the building information files. Here, I have given you assessment history (both building and land) where available as well as a record of years in which the property owners protested their initial assessment. The School District, Zone, Region, Subneighborhood, and Neighborhood identifiers can be used geographically similar properties, which may be helpful to identify ‘hot’ neighborhoods.2 The TARGET column is simply a repeat of the assessed_2019 column, emphasizing the prediction goal of this task.\nFinally, you receive the test set file:\n\nlibrary(readr); library(dplyr)\nglimpse(read_csv(\"competition_data/assessment_history_test.csv.gz\"))\n\nRows: 418,858\nColumns: 33\n$ acct                &lt;chr&gt; \"e8edfed00598c883053399cfc54b207b42a519f9\", \"3090f…\n$ building_area_2015  &lt;dbl&gt; 2537, 1496, 1508, 1670, 1944, 1944, 1732, 3841, 11…\n$ land_area_2015      &lt;dbl&gt; 5000, 5000, 6250, 6250, 1765, 1720, 6050, 5000, 50…\n$ building_area_2016  &lt;dbl&gt; 2537, 1496, 1508, 1670, 1944, 1944, 1732, 3841, 11…\n$ land_area_2016      &lt;dbl&gt; 5000, 5000, 6250, 6250, 1765, 1720, 6050, 5000, 50…\n$ building_area_2017  &lt;dbl&gt; 2537, 1496, 1508, 1670, 1944, 1944, 1732, 3841, 11…\n$ land_area_2017      &lt;dbl&gt; 5000, 5000, 6250, 6250, 1765, 1720, 6050, 5000, 50…\n$ building_area_2018  &lt;dbl&gt; 2537, 1496, 1508, 1670, 1944, 1944, 1732, 3841, 11…\n$ land_area_2018      &lt;dbl&gt; 5000, 5000, 6250, 6250, 1765, 1720, 6050, 5000, 50…\n$ region              &lt;chr&gt; \"02a37f2eadcf42e9ab0748a5814e8d2f43319ecd\", \"02a37…\n$ building_area_2019  &lt;dbl&gt; 2537, 1496, 1508, 1670, 1944, 1944, 1732, 3841, 11…\n$ land_area_2019      &lt;dbl&gt; 5000, 5000, 6250, 6250, 1765, 1720, 6050, 5000, 50…\n$ building_value_2015 &lt;dbl&gt; 279405, 2618, 158027, 4899, 222791, 217277, 40861,…\n$ land_value_2015     &lt;dbl&gt; 75000, 75000, 84375, 84375, 88956, 88236, 110500, …\n$ building_value_2016 &lt;dbl&gt; 233935, 2797, 132288, 4153, 210044, 217277, 31457,…\n$ land_value_2016     &lt;dbl&gt; 125000, 125000, 140625, 140625, 88956, 88236, 1381…\n$ building_value_2017 &lt;dbl&gt; 235375, 2870, 113805, 4153, 210044, 211567, 32141,…\n$ land_value_2017     &lt;dbl&gt; 125000, 125000, 140625, 140625, 88956, 88236, 1381…\n$ building_value_2018 &lt;dbl&gt; 235375, 2909, 113805, 4153, 210044, 211567, 32823,…\n$ land_value_2018     &lt;dbl&gt; 125000, 125000, 140625, 140625, 88956, 88236, 1381…\n$ assessed_2015       &lt;dbl&gt; 355945, 78118, 242402, 89274, 311747, 305513, 1538…\n$ protested_2015      &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE…\n$ assessed_2016       &lt;dbl&gt; 360475, 128297, 272913, 144778, 299000, 305513, 17…\n$ protested_2016      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRU…\n$ assessed_2017       &lt;dbl&gt; 360475, 128297, 254430, 144778, 299000, 299803, 17…\n$ protested_2017      &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE…\n$ assessed_2018       &lt;dbl&gt; 360475, 128297, 254430, 144778, 299000, 299803, 17…\n$ protested_2018      &lt;lgl&gt; FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE…\n$ protested_2019      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRU…\n$ school_dist         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ zone                &lt;chr&gt; \"ea08b1b7cad521f20b71379bcef05c2741eaeee8\", \"ea08b…\n$ subneighborhood     &lt;chr&gt; \"6f23373854222ed999c4c2114f5cb4929f22773e\", \"6f233…\n$ neighborhood        &lt;chr&gt; \"6315b211033894c97f758c5b2d0e53a00ca66f81\", \"6315b…\n\n\nThis is similar to the training file, but with certain 2019 data deleted. (Note that you retain building and land area as well as the protest indicator.)"
  },
  {
    "objectID": "competition.html#prediction-accuracy-100-points",
    "href": "competition.html#prediction-accuracy-100-points",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "Prediction Accuracy (100 Points)",
    "text": "Prediction Accuracy (100 Points)\nPrediction accuracy will be assessed using Kaggle Competitions. I have provided Kaggle with a private test set on which your predictions will be evaluated. Kaggle will further divide the test set into a public leaderboard and a private leaderboard. You will be able to see your accuracy on the public leaderboard but final scores will be based on private leaderboard accuracy so don’t overfit.\nGrades for this portion will be assigned as:\n\\[\\text{Grade} = 1 - \\frac{\\text{Class Best MSE}}{\\text{Constant MSE}}\\]\nThat is, your grade will essentially be a ‘rescaled’ \\(R^2\\) where the best performance in the class is 100% and everyone else is somefraction thereof. (Note that this means the top performance automatically gets 100% on this component.)\nYou will be allowed to submit only two sets of predictions to Kaggle per day so use them wisely."
  },
  {
    "objectID": "competition.html#final-presentation-50-points",
    "href": "competition.html#final-presentation-50-points",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "Final Presentation (50 Points)",
    "text": "Final Presentation (50 Points)\nTBD"
  },
  {
    "objectID": "competition.html#final-report-50-points",
    "href": "competition.html#final-report-50-points",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "Final Report (50 Points)",
    "text": "Final Report (50 Points)\nTBD"
  },
  {
    "objectID": "competition.html#footnotes",
    "href": "competition.html#footnotes",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnsurprisingly, New York makes this more complicated.↩︎\nI will not clarify which geographic features are bigger or larger than others. All I wil guarantee is that the different hexadecimal codes are consistent across files.↩︎"
  },
  {
    "objectID": "competition.html#example-prediction",
    "href": "competition.html#example-prediction",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "Example Prediction",
    "text": "Example Prediction\nTBD"
  },
  {
    "objectID": "notes/notes06.html#named-generative-classifiers",
    "href": "notes/notes06.html#named-generative-classifiers",
    "title": "STA 9890 - Generative Classifiers",
    "section": "Named Generative Classifiers",
    "text": "Named Generative Classifiers\nAs mentioned above, the ‘popular’ generative classifiers are essentially defined by specific multivariate normality assumptions on \\(\\mathcal{P}_0, \\mathcal{P}_1\\). Let us review these in ascending order of complexity.\nRecall that a multivariate normal distribution is uniquely identified by its mean and (co)variance (matrix), just like a standard univariate normal distribution. When estimating means, we will simply use the class means for each classifier, though James-Stein time estimators could be used for a (likely imperceptible) improvement. Estimation of variances is a harder task: to estimate the full covariance matrix of \\(p\\) random variables (or a random \\(p\\)-vector), we must estimate:\n\n\\(p\\) (marginal) variances and \\(\\frac{p^2-3p}{2}\\) correlations; or\n\\(\\binom{p}{2} = \\frac{p(p-1)}{2}\\) covariances\n\nMaking this even harder, we have to deal with the fact that the “variance of the variance”, i.e., the kurtosis, is typically quite large. While we do not focus on high-dimensional covariance estimation in these notes, it is quite a rich subject: if you want dig further into it, the Ledoit-Wolf estimator is a good place to start.\n\nNaive Bayes\nIn Naive Bayes, we assume \\(\\mathcal{P}_0, \\mathcal{P}_1\\) are multivariate normal distributions with no correlation among the features and a common covariance across classes.2 This significantly simplifies the covariance estimation problem, since we now have all \\(n\\) samples available to estimate only \\(p\\) variances. Our estimates are given as follows:\n\\[\\hat{\\mu}_0 = \\frac{1}{|\\mathcal{C}_0|} \\sum_{i \\in \\mathcal{C}_0} \\bx_i\\]\nwhere \\(\\mathcal{C}_0\\) is the set of observations from class 0 and\n\\[\\hat{\\mu}_1 = \\frac{1}{|\\mathcal{C}_1|} \\sum_{i \\in \\mathcal{C}_1} \\bx_i.\\] The variance estimates are a bit trickier:\n\\[\\hat{\\Sigma}_{ii} = \\frac{1}{n}\\left(\\sum_{j\\in\\mathcal{C}_0} (x_{ji} - \\hat{\\mu}_{0i})^2 + \\sum_{j\\in\\mathcal{C}_1} (x_{ji} - \\hat{\\mu}_{1i})^2\\right)\\] This looks a bit nasty, but it’s not too hard: we take the standard ‘average squared difference’ estimator of the variance, but instead of using the overall mean, we subtract the estimated mean from the relevant class. Note that here we are only estimating \\(\\Sigma_{ii}\\), the diagonal elements, and not a full matrix \\(\\Sigma\\).\nNaive Bayes classifiers are particularly important to the history of ML as they were used for one of the first and most influential spam filters.\n\n\nLinear Discriminant Analysis,\nGeneralizing a bit, we can relax the assumption that the covariance matrix \\(\\Sigma\\) is diagonal and only assume that it is constant across the two classes. Here,"
  },
  {
    "objectID": "notes/notes06.html#a-non-standard-example",
    "href": "notes/notes06.html#a-non-standard-example",
    "title": "STA 9890 - Generative Classifiers",
    "section": "A Non-Standard Example",
    "text": "A Non-Standard Example\nSuppose we have"
  }
]