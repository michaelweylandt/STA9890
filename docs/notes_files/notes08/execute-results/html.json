{
  "hash": "efb37ed788793431dd95c2ea4c263890",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"8\"\nauthor: \"Michael Weylandt\"\ntopic: \"Ensemble Learning & Resampling Methods\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n$$\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bw}{\\mathbf{w}} \\newcommand{\\P}{\\mathbb{P}} \\newcommand{\\V}{\\mathbb{V}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\E}{\\mathbb{E}}$$\n\n\n\n\n\n\n\nTo this point, we have created individual models for regression and\nclassification tasks. We have covered regression-type models for\nlow-dimensional settings (GLMs: mainly linear and logistic), generative\nmodels for classification (NB, LDA, QDA), purely discriminative models\n(SVM), and flexible non-parametric models (KNN). Building on these tools,\nwe have developed non-linear extensions (splines and kernels) suitable\nfor the $n \\gg p$ setting and regularization techniques (ridge and lasso)\nnecessary for the $p \\gg n$ setting. \n\nIn each of these, we have argued that certain models (more precisely,\ncertain estimators) have different amounts of bias and variance and\ngiven a rule of thumb that in low-data settings high-bias/low-variance\nmodels are preferred while in high-data settings, we should feel free\nto use low-bias models since the variance will be controlled by the\nlarge training set. \n\nFurthermore, we have discussed the importance of 'hold-outs' (test/train\nsplits, cross-validation, *etc.*) as a tool to avoid overfitting. If we\nevaluate our models on new data (or at least new to that model), we \nminimize the chance of 'regurgitation' and ensure that our models are \nlearning true(-ish) generalizabile relationships. Importantly, we know\nthat _any time_ we make a selection, it is important to have a new data\nset for downstream tasks. So: \n\n- If we just want to to train a single pre-specified model with no \n  hyperparameters and use it blindly, we can use all of our data\n- If we want to to train a single pre-specified model with no \n  hyperparameters and assess its predictive performance, we need to\n  split our data into two sets (training + test)\n- If we want to train a model and select hyperparameters but do not\n  need to assess its predictive performance, we need to split our\n  data into two sets (training + 'validation')\n- If we want to train a model, select hyperparameters, and provide\n  an unbiased assessment of predictive performance, we need to split \n  our data into three sets (training + validation + test)\n  \nPersonally, I find the jargon of these different sets a bit confusing--what\nis testing and how is it different than validation?--but the 'new data\nfor each task' heuristic is easy to apply and generalize. \n\nFinally, we have noted that - while hold out techniques are an excellent tool\nfor maximizing predictive performance - they aren't the best to ensure \n*reliability of interpretation*. If *insights* are more important than pure\n*performance*, stability techniques are typically a better strategy for \ntuning hyperparameters. \n\nFor the next two weeks, we are going to go beyond this 'single model' \nparadigm and consider how we might handle scenarios where we have two\nor more models available for our use. This is the domain of *ensemble \nlearning* - building models consisting of multiple 'sub-models', termed\n'base learners'. Ensemble learning is particularly powerful when \napplied in conjunction with fast and flexible base learners, so we will\nalso use this as a jumping off point for one more class of models. \n\n## Ensemble Learning\n\nWe begin with a discussion of *ensemble learning*, the task of combining\nmultiple (simpler) models into a 'super model' to achieve a task. \nEnsembles can be used for essentially any ML task, but we will focus on \nensemble learning for the two tasks we have focused on to date: regression\nand classification. (The differences in these two tasks will not really\nhave any bearing on our 'ensembling' discussion.)\n\nSuppose that we want to predict the outcome of some variable $Y$ using\ntwo *statistically independent* predictors $\\hat{Y}_1, \\hat{Y}_2$: if\n*arguendo* we assume these are unbiased, the MSE of each prediction\nis simply\n\n$$\\begin{align*}\n\\text{MSE}_{\\hat{Y}_i} &= \\E_{Y, \\hat{Y}_i}[(Y - \\hat{Y}_i)^2] \\\\\n                       &= \\E_{Y, \\hat{Y}_i}[(Y - \\E[Y] + \\E[Y] - \\hat{Y}_i)^2] \\\\\n                       &= \\E_{Y, \\hat{Y}_i}[(Y - \\E[Y])^2 +2(Y - \\E[Y])(\\E[Y] - \\hat{Y}_i)+(\\E[Y] - \\hat{Y}_i)^2] \\\\\n                       &= \\E_{Y}[(Y - \\E[Y])^2 +2\\E_{Y, \\hat{Y}_i}[(Y - \\E[Y])(\\E[Y] - \\hat{Y}_i)]+\\E_{\\hat{Y}_i}[(\\E[Y] - \\hat{Y}_i)^2] \\\\\n                       &= \\sigma_Y^2 + 2\\E_{Y}[Y - \\E[Y]]\\E_{\\hat{Y}_i}[\\E[Y] - \\hat{Y}_i)] + \\sigma^2_{\\hat{Y}_i} \\\\\n                       &= \\sigma_Y^2 + \\sigma^2_{\\hat{Y}_i}\n\\end{align*}$$\n\n(Why do the cross terms vanish?) That is, the MSE is simply sum of the \n'irreducible error' associated with the best possible prediction ($\\sigma_Y^2$\nagainst the optimal prediction $\\E[Y]$) and the variance (error) in $\\hat{Y}_i$\nas an estimator of $\\E[Y]$.\n\nIf $\\hat{Y}_1, \\hat{Y}_2$ are both predictors of $Y$, it is natural to ask\nif we can do better using a *combination* of the two of them than we can with\neither separately. Indeed, if we let $\\widehat{Y} = \\frac{1}{2}(\\hat{Y}_1 + \\hat{Y}_2)$, the above analysis tells us: \n\n$$\\begin{align*}\n\\text{MSE}_{\\widehat{Y}} &= \\sigma_Y^2 + \\sigma_{\\widehat{Y}}^2 \\\\\n&= \\sigma_Y^2 + \\mathbb{V}\\left[\\frac{\\hat{Y}_1 + \\hat{Y}_2}{2}\\right] \\\\\n&= \\sigma_Y^2 + \\sigma_{\\hat{Y}_1}^2/4 + \\sigma_{\\hat{Y}_2}^2/4\n\\end{align*}$$\n\nHow does this compare to the separate predictions? Well, if \n$\\sigma_{\\hat{Y}_1} = \\sigma_{\\hat{Y}_2}$, then we have clearly reduced our \nexpected MSE by half of the variance error. (By definition, we can't do anything\nabout the irreducible error.) If, on the other hand, one predictor is much\nbetter than the other, say $\\sigma_{\\hat{Y}_1}^2 = 400 \\sigma_{\\hat{Y}_2}^2$,\nthen we have\n\n$$\\begin{align*}\n\\text{MSE}_{\\hat{Y}_1} &= \\sigma_Y^2 + 400 \\sigma_{\\hat{Y}_2}^2 \\\\\n\\text{MSE}_{\\hat{Y}_2} &= \\sigma_Y^2 + \\sigma_{\\hat{Y}_2}^2 \\\\\n\\text{MSE}_{\\widehat{Y}} &= \\sigma_Y^2 + 100.25 \\sigma_{\\hat{Y}_2}^2 \\\\\n\\end{align*}$$\n\nSo we don't beat the superior predictor ($Y_2$), but we comfortably beat the\ninferior predictor ($Y_1$). Notably, if we didn't know which predictor\nwas better and had to select randomly (50/50), we would have an expected\nMSE of $\\sigma_Y^2 + 200\\sigma_{\\hat{Y}_2}^2$, so the 'averaging' predictor\nis still better. More generally, if $\\hat{Y_1}$ is sometimes better and\n$\\hat{Y_2}$ is better at other times, a suitable averaging strategy will \ndo better  *in the long run* than using a single predictor. (We should pick\nthe averaging weights as a function of the variance of the two approaches and\nthe relatively frequencies of the $\\hat{Y}_1$-better and $\\hat{Y}_2$-better\nscenarios.)\n\nThis example - simple as it is - gets at the core insight of stacking: if we\nhave several 'good enough' models, we can do better - sometimes much better -\nby using a combination of them. This is not guaranteed - if $\\hat{Y}_2$ is \nalways the 'lower variance' model, adding in some $\\hat{Y}_1$ pretty much\nalways hurts - but we do not expect to have a single 'dominant' model. In \npractice, we typically find ourselves armed with models of similar-enough\nperformance (logistic regression, linear SVMs, RBF Kernel SVMs, RBF Kernel \nlogistic regression) and find that different models perform better on different\ninputs. *E.g.*, the 'linear' models might do well far from the decision boundary,\nwhile the 'kernelized' models might improve performance near the decision\nboundary while suffering from extra 'wiggliness' (variance) far from the \ndecision boundary. \n\nSo what can we take away from this discussion? \n\n> Low correlation among predictors is helpful. If our base learners give\n  the same predictions at every point, there's nothing to be gained by \n  combining and comparing them. \n\nPut another way, ensemble learning benefits from a *diverse* set of base \nlearners. We hope to 'combine strength' from different approaches to build\nan ensemble predictor that is better than any of its individual components.\n\nThe three major ensembling techniques we will cover - stacking, bagging, and \nboosting - essentially come down to different ways of getting this diversity.\n\n## Stacking\n\nSo this is all well and good, but where do we *actually get* different \npredictors? Here, we're going to change notation to $\\hat{f}_1, \\hat{f}_2, \\dots\n\\hat{f}_K$ to emphasize that we want to learn a combination of _predictors_ \n(functions estimated from data), not _predictions_. \n\nWell... perhaps the easiest thing to do is to train different models on the\nsame data. Logistic Regression, LDA, and SVMs will likely find _similar_ \ndecision boundaries, but they won't be exactly the same. Suppose we have\n\n- $\\hat{f}_{\\text{LR}}$: Logistic Regression (for simplicity, plain and\n  unregularized)\n- $\\hat{f}_{\\text{LDA}}$: Linear Discriminant Analysis\n- $\\hat{f}_{\\text{SVM}}$: A Linear SVM with the default hyperparameter \n  from `sklearn`\n  \nFurther, let's assume that we have the binarized predictions ($\\{0, 1\\}$, no\nsoft labels for now) from each model. How can we combine these? \n\nPerhaps the simplest rule is the 'majority vote' rule: if $\\hat{f}_{\\text{LR}}(\\bx) = \\hat{f}_{\\text{LDA}}(\\bx) = 1$ and $\\hat{f}_{\\text{SVM}}(\\bx) = 0$, then $\\hat{f}_{\\text{Majority}}(\\bx) = 1$. \n\nThis approach is quite easy to implement, but it treats all the inputs as\nequally reliable. While this isn't the _worst_ assumption, we can do better.\nIn particular, what if we _learned_ an optimal set of weights. Specifically, \nwe want to learn a vector of weights $\\bw \\in \\mathbb{R}^3$ to maximize\npredictive accuracy of the linear combination $w_1 \\hat{f}_{\\text{LR}} + w_2 \\hat{f}_{\\text{LDA}} + w_3 \\hat{f}_{\\text{SVM}}$. This gives us another\nclassification problem, so let's use logistic regression to determine the weights:\n\n$$\\hat{\\bw} = \\text{arg min}_{\\bw} \\sum_{i=1}^n -y_i \\bw^{\\top}\\hat{\\mathbf{f}} + \\log(1 + e^{\\bw^{\\top}\\hat{\\mathbf{f}}})$$\nwhere $\\hat{\\mathbf{f}}$ is a vector of the three base learner predictions.\n\nThe solution to this problem, $\\hat{\\bw}$, gives the _stacking weights_\nfor our three base learners. Once we have $\\hat{\\bw}$, we can make\npredictions at our our new test point, $\\tilde{\\bx}$, by: \n\n1. Apply three base learners separately: \n   - $\\hat{y}_{\\text{LR}} = \\hat{f}_{\\text{LR}}(\\tilde{\\bx})$\n   - $\\hat{y}_{\\text{LDA}} = \\hat{f}_{\\text{LDA}}(\\tilde{\\bx})$   \n   - $\\hat{y}_{\\text{SVM}} = \\hat{f}_{\\text{SVM}}(\\tilde{\\bx})$\n2. Determine the weighted combination: \n   - $\\text{Stacking Score} = w_1\\hat{y}_{\\text{LR}} + w_2\\hat{y}_{\\text{LDA}}+w_3\\hat{y}_{\\text{SVM}}$\n3. Make a prediction using the stacking score: \n   $$\\hat{y}_{\\text{Ensemble}} = \\begin{cases} 1 & \\text{ if } \\text{Stacking Score} > \\theta \\\\ 0 & \\text{ if } \\text{ Stacking Scoore} \\leq \\theta \\end{cases}$$\n   where $\\theta$ is a threshold chosen to minimize the problem-specific loss.\n   (Alternatively, set $\\theta = 0$ and include an intercept term.)\n   \nWe note a few practical points here: \n\n1. What data should we use to estimate $\\bw$? This is a new step in our pipeline,\n   so we need a new 'chunk' of our data. This split of the data is typically\n   called either a query or validation set, but I tend to describe it as an\n   'ensembling' set to make its purpose clear. Regardless, it serves to \n   give us a new data set to fit our new ensemble model. (If we use the\n   training set used to train each $\\hat{f}_i$, the base learners will look\n   'too good' to the ensemble learning process.)\n2. Nothing actually requires us to use the binarized predictions. For models\n   that provide soft labels (class probabilities), we can and should use those\n   in the ensemble process (both in training the ensemble weights and in\n   constructing the predictor vectors $\\hat{\\mathbf{f}}$). Dropping the\n   probabilities is really just throwing away data. \n3. We motivated stacking by weighted averages. As such it is common to add\n   additional constraints to the stacking problem to make it look more like\n   averaging. For example, you will often see: \n   - $\\bw \\geq 0$: The stacking weights are non-negative. This makes sense\n     if you assume each predictor is generally reasonable. \n   - $\\sum w_i = 1$: The stacking weights sum to 1. Again, a natural\n     generalization of averaging. \n4. When fitting *large* ensembles (like any large model), it is not uncommon\n   to see ridge or lasso penalties. These are particularly important in ensemble\n   building since the base learners are generally highly correlated. \n5. Suitably tuned (and that's a big assumption!) stacking should never do worse\n   than any individual predictor because we can always pick $\\bw = (1, 0, 0)$ \n   if the first base learner dominates the others. \n   \nStacking creates a 'model of models' - the turducken of machine learning - and \nall of the ML tricks we have learned to date can be used in creating this\nmeta-model. Like any modeling step, we have to practice good data hygiene\n(splitting and hold outs) to make sure we don't overfit and to generate a good\nestimate of predictive accuracy, but otherwise it is particularly straightforward.\n\nStacking can only take us so far: the value-added of stacking comes from \ndifferences between the base learners and, when trained on the same data set,\nwe don't expect too much diversity among the base learners. In practical settings,\nstacking is particularly useful when we are given 'standard' or 'baseline' models\nfor a task that we want to combine without changing the individual models, but\nwe may want to modify our pipeline to explicitly generate more diversity. \n\nThis brings us to our next family of ensembling techniques - resampling methods,\nthe most famous of which is *bootstrap aggregation* or bagging. \n\n## Resampling\n\nIn our discussion above, we argued that diversity of (variance between) base \nlearners was key to ensemble performance. When these base learners are trained\non the same training data, we can only get so much variance. Since most of our\nmodels are not terrible, they generally pick out the same major patterns in\nthe data. \n\nSo how can we get more variance? We could further split our training data, using\na small chunk for each model, but this seems likely to bring about variance\nproblems. If we want to fit large ensembles of 10, 20, or even 100 models, we\nmight wind up using less than 1% of the overal data to train a single model,\nwhich is clearly subpar. \n\nWe can get around this problem using _sampling_ or _resampling_ techniques.\nWe can train our esnemble members on randomly selected subsets of our data \n(or subsets of the features). Because these models have different training sets,\nthey will be more varied. (Typically, this sort of 'data randomization' induces\nmore variance than just changing hyperparamters or model families.) \n\nThese randomization schemes have different names: \n\n- Minibatching: training on random subsets of samples (rows)\n- Random Subspace Method: training on random subsets of features (columns)\n- Minipatching: training on random subsets of samples and features \n  (row and columns)\n  \nIf the model is not too sensitive to the sample size (or if we use a high\nsampling rate), these strategies can work well.[^sampling] But we have a bit\nof a bind here: if we want to use large training sets for each learner, we go\nback to the overlapping training scenario we were trying to avoid. \n\nCan we be a bit more creative on our sampling? We want to minimize overlap\nbut also get 'full sized' training sets. \n\n[^sampling]: Most of these methods were actually designed to speed up \ntraining/fitting and that remains the main use case for minibatching today\n(in *stochastic gradient descent*). We won't focus too much on implications for\nspeed today. \n\n\n### Bootstrapping\n\nTo get around this problem, we rely on a key idea of late 20th century \nstatistics: **the bootstrap.**[^bootstrap] \nRecall the core idea of bootstrapping: \n\n- We have samples $\\{(\\bx_1, y_1), (\\bx_2, y_2), \\dots, (\\bx_n, y_n)\\}$ from\n  an unknown distribution. \n- We would like to have more samples from this distribution, but we don't have\n  the ability to gain more data. \n- If our data set is large, we expect that the _empirical distribution_\n  $\\hat{\\P}_n$ will be close to the 'real' distribution $\\P$. Formally, the law\n  of large numbers (and its fancy variants like Glivenko-Cantelli) guarantees\n  us that $\\hat{P}_n \\to \\P \\text{ as } n \\to \\infty$. \n- Sampling from $\\hat{P}_n$ is 'close' to sampling from $\\P$\n\nSo how can we sample from the empirical distribution $\\hat{P}_n$? We sample\n_with replacement_. That is, we pick one of the training points $n$ times \n**independently** (with no regard for prior selections). Intuitively, this\ncaptures the idea of IID sampling - it's also necessary so we don't just \nreproduce the (shuffled) training data. \n\nThis scheme - sampling with replacement - is the essence of \n**bootstrap sampling**. \n\n### Bootstrap Aggregation (Bagging)\n\nWe can use a similar strategy to generate 'new' training data for our base\nlearners: \n\n- Repeat $b=1, \\dots, B$ times: \n  - Sample from the training data $\\mathcal{D}_{\\text{train}}$ $n$ times \n    *with replacement* to get a bootstrap set $\\tilde{\\mathcal{D}_b}$\n  - Train a base learner $\\hat{f}_b$ on $\\tilde{\\mathcal{D}_b}$\n- Create the ensemble predictor: \n  $$\\hat{f}(\\bx) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}_b(\\bx)$$\n  \nThis ensembling strategy is called **bootstrap aggregation** or more simply\n**bagging**. \n\nSo how much diversity / variance can we actually expect from this strategy? It\ndepends on how much overlap we get in our sampling. This is a fun little\nprobability exercise: \n\n::: {.callout-tip title=\"Probability of a Given Sample Being in a Bootstrap Data Set\"}\n\nIn our scenario above, what is the chance that $\\bx_1$ is in\n$\\tilde{\\mathcal{D}}_b$? \n\nFor each sample, there is a $1/n$ chance that we select $\\bx_1$, and hence\na $1-1/n$ chance that we don't. If we repeat this process $n$ times, there is\na $(1-1/n)^n$ chance that we never select $\\bx_1$. For a large data set, \nthis converges to\n\n$$\\lim_{n\\to\\infty}\\left(1-\\frac{1}{n}\\right)^n = e^{-1} \\approx \\frac{2}{3}$$\n\nso about $2/3$ of our data is used for each base learner. _Unlike_ straight\nsubsampling, however, this process includes repeats, so we have $n$ samples, \nguaranteeing us repeats. (A more detailed analysis can show that the _number_ \nof times a sample appears is asymptotically Poisson.)\n\n:::\n\nSo how does bagging actually work? We know that we're always trying to control\nbias and variance so let's look at those two terms separately: \n\n- Bias: Because we are fitting the same model $B$ times, there's really no\n  impact on bias. (*E.g.*, if we are fitting a linear model, the average of\n  $B$ lines is just another line so whatever approximation error we have is\n  unchanged.)\n- Variance: Suppose that the base learners $\\hat{f}_i$ have variance $\\sigma^2$\n  and correlation $\\rho$. (These are constant for all $\\hat{f}_i, \\hat{f}_j$ by\n  the IID sampling structure used). Then the variance of $\\hat{f}$ (the bagged \n  average) is given by \n  \n$$\\begin{align*}\n\\V[\\hat{f}] &= \\V\\left[\\frac{1}{B}\\sum_b \\hat{f}_b\\right] \\\\\n            &= B^{-2}\\V\\left[\\sum_b \\hat{f}_b\\right] \\\\\n            &= B^{-2}\\left(\\sum_{b=1}^B \\V[\\hat{f}_b] + \\sum_{\\substack{b, b'=1 \\\\ b \\neq b'}}^B \\C[\\hat{f}_b, \\hat{f}_{b'}]\\right)\\\\\n            &= B^{-2}\\left(\\sum_{b=1}^B \\sigma^2 + \\sum_{\\substack{b, b'=1 \\\\ b \\neq b'}}^B \\rho\\sigma^2\\right) \\\\\n            &= B^{-2}\\left(B\\sigma^2 + (B^2 - B) \\rho\\sigma^2\\right) \\\\ \n            &= B^{-2}\\left(B(\\rho\\sigma^2 + (1-\\rho)\\sigma^2) + (B^2 - B) \\rho\\sigma^2\\right) \\\\ \n            &= B^{-2}\\left(B\\rho\\sigma^2 + B(1-\\rho)\\sigma^2) + B^2\\rho\\sigma^2 - B\\rho\\sigma^2\\right) \\\\ \n            &= B^{-2}\\left(B(1-\\rho)\\sigma^2) + B^2\\rho\\sigma^2\\right) \\\\ \n            &= \\rho\\sigma^2 + \\frac{1}{B}(1-\\rho)\\sigma^2\n\\end{align*}$$\n\nThis is an interesting formula. As always, we can understand it best by taking\nthe extremes: \n\n- $B = 1$: In this case, the variance reduces to $\\sigma^2$, the variance of a \n  single predictor (as we would expect for 'ensemble of one'). \n- $B \\to \\infty$: The variance decreases to $\\rho\\sigma^2$, but no further. \n  Practically, we see diminishing returns in the $B\\approx 200-500$ range, which\n  you will see as the default for most software that has a bagging step. \n- $\\rho \\to 1$: As the base learners become more correlated, the variance\n  converges to $\\sigma^2$, the variance of a single predictor. (As we would\n  expect since we have no diversity among base learners.)\n- $\\rho \\to 0$: As the base learners become uncorrelated, the variance converges\n  to $\\sigma^2/B$, the variance of a standard average. (You might ask why not\n  take $B \\to \\infty$ in this case to have no bias: we really can't get an \n  infinite number of uncorrelated models, no matter how hard we try, if they are\n  modeling the same underlying DGP)\n  \nSo we see we want a flexible (low-bias) base learner that is also going to give\nus low correlation (and low variance) among the bagged predictors. How can we\nget such a base learner? We'll cover this more next week. \n\n[^bootstrap]: If you have seen bootstrapping before, it was likely in the\ncontext of estimating the sampling variance of an estimator. (If you haven't\nseen this, go look it up - it's awesome!) Here, we're using bootstrap sampling\nto induce some variance, not to estimate variance, but the sampling strategy\nis the same. \n\n### Out of Bag Error\n\n## Boosting\n\nDelayed to next week...\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}