{
  "hash": "9405537a9663800c02593dc035ff029f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"5\"\nauthor: \"Michael Weylandt\"\ntopic: \"Introduction to Classification\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n$$\\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\Ycal}{\\mathcal{Y}}$$\n\nAs we move into Unit II of this course, we turn our focus to *classification*. \n\n## A Taxonomy of Classification\n\nRecall from before that we defined _regression_ as the subset of supervised\nlearning where the response ($\\by$) was a continuous variable, or at least one\nthat is appropriate to model as such.[^effective_cont] By contrast, \n*classification* considers $\\by$ to take values in a discrete set. Though this\nrestriction may feel limiting, it is actually quite powerful. We consider several\ndifferent forms of classification, based on the *support* of $\\by$: \n\n[^effective_cont]: Consider, *e.g.*, predicting someone's annual tax liability.\n*Technically*, this is *discrete* since tax liabilities are rounded to the nearest\ndollar, but functionally it may as well be continuous. \n\n- *Binary* classification: in this simplest case, $\\by$ takes one of two values,\n  which we conventionally code as $\\{0, 1\\}$ or $\\{-1, +1\\}$, depending on which\n  makes the math easier. Note that the actual response need not be these numbers;\n  they are simply a mathematical convenience. \n  \n  Examples of binary classification include: \n  \n  - Does an incoming student graduate within the next 6 years or no?\n  - Does a patient have a given disease? \n  - Is there a dog present in an image? \n  - Is this applicant qualified for a job? \n  \n  Note that, in at least two of these, the underlying question is (arguably)\n  not quite so binary, but we may choose to treat it as such. In particular,\n  when assessing onset of a disease, the actual impairment to the patient is \n  continuous, but it is common in medical practice to \"binarize\" the outcome. \n  Any of you with aging family members know that cognitive decline occurs long\n  before it's 'officially' Alzheimer's. Similarly, qualification for a job\n  is perhaps best understood as a continuous (or even purely comparative)\n  quantity \"Is A more qualified than B?\" but we chooes to binarize it when\n  deciding to whom a job offer should be made. \n  \n  Recalling your probability theory, there is (essentially) only one distribution\n  on $\\{0, 1\\}$, the Bernoulli, so this scenario typically has the cleanest and\n  simplest mathematics. \n  \n- *Multiclass* classification: in this case, $\\by$ takes values in a set of \n  finite and known, but potentially large, set of outcomes, which we will\n  call $\\Ycal$. Before about 20 years ago, multiclass problems took \n  $|\\Ycal|$ to be not much larger than 2, *e.g.*, which blood type does\n  a sample match or what language is a text sample written in, but much of the \n  modern practical success of machine learning comes from the ability to treat \n  *very large* outcome sets $\\Ycal$. \n  \n  For instance, modern computer vision (CV) has moved beyond far simple binary\n  classification: \n  \n\n\n  {{< video https://www.youtube.com/watch?v=ACmydtFDTGs >}}\n\n\n\n  \n  The famed[`ImageNet`](https://en.wikipedia.org/wiki/ImageNet) benchmark data\n  set has over 20,000 classes (arranged with additional structure) and modern\n  data sets include even more. This can be contrasted with the older \n  [`MNIST`](https://en.wikipedia.org/wiki/MNIST_database) data set which \n  collects only ten handwritten digits.\n  \n  Beyond CV, large multiclass classification has also revolutionized text modeling: \n  when a chatbot generates text, it selects the next word from a large but finite\n  set of English words. \n  \n  Just like binary classification is rooted in the Bernoulli distribution, \n  multiclass classification is rooted in the [categorical\n  distribution](https://en.wikipedia.org/wiki/Categorical_distribution). When\n  applied to a large class of outcomes, the categorical distribution becomes a\n  [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution), \n  *cf* Bernoulli to binomial, so multiclass classification is also frequently\n  called *multinomial* classification. \n  \nWhile our main focus is on *binary* and *multiclass* problems, it is worth noting\nsome additional \"modes\" of classification. \n\n- *Ordinal* classification: in this case, $\\Ycal$ is an _ordered_ set. The most\n  common form of ordinal classification is the ubiquitous \n  [*Likert scale*](https://en.wikipedia.org/wiki/Likert_scale) which\n  you have certainly encountered any time you have filled out a customer survey:\n  \n  - *Strongly Agree*\n  - *Agree*\n  - *Neutral*\n  - *Disagree*\n  - *Strongly Disagree*\n  \n  In this case, the categories clearly have an order and a ranking, but we can't\n  quite treat them as 'numerical' in the usual sense: *Strongly Agree* is not\n  just *Agree* + *Disagree*. \n  \n  Proper analysis of *ordinal* data is tricky and often poorly done. \n  Complexities include the inappropriateness of statisticians' most beloved tool,\n  averaging[^set], and a fundamental 'subjectiveness.' \n  Another common ordinal classification problem is the assignment of grades\n  (A > B > C ...). In this case, students and professors may agree that \n  one piece of work may be better than another, but a student may feel their\n  assignment deserves an A while the professor may judge it B work. It is\n  quite difficult to infer each individual's personal threshold for each category\n  without extensive analysis. \n  \n  We will not discuss ordinal data much in this course as it requires specialized\n  methods. Typically ordinal data is modeled by treating the response as a rounded\n  version of some underlying continuous variable: *e.g.*, when grading hurricanes\n  the classes (Category 1, Category 2, ...) are derived from the underlying\n  continuous variable of wind speed. This 'hidden regression' structure makes\n  analysis of ordinal classification much closer to regression than binary\n  classification. \n\n[^set]: Even though it doesn't make sense to average ordinal data, University\nadministrators are *notorious* for averaging Likert data to evaluate faculty. \nThis is a nigh-universal grievance of numerically-minded faculty, but getting\nuniversities to change their established procedures to something statistically\nreasonable is only slightly less impossible than drawing blood from a turnip.\n  \n- *Probabilistic* classification: we noted above that binary classification can\n  be thought of as modeling a Bernoulli random variable. If we shift our focus\n  to modeling the Bernoulli parameter (the probability $p \\in [0, 1]$) than\n  the outcome ($y \\in \\{0, 1\\}$), we have the paradigm of *probabilistic*\n  classification. \n  \n  As a general rule, *statistically*-grounded classification methods have\n  a natural probabilistic output built-in while methods coming from the CS \n  tradition are not inherently probabilistic. Because probabilistic estimates\n  are often quite useful, there exist several [useful\n  methods](https://en.wikipedia.org/wiki/Platt_scaling) to 'bolt-on' \n  probabilistic outputs to pure binary classifiers. \n  \n## Metrics of Classification Accuracy\n\nIn the regression context, we had a relatively small number of loss functions: \n\n- Mean Squared Error (MSE): mathematically convenient, easy interpretation, \n  natural for OLS, optimizes for mean\n- Mean Absolute Error (MAE): mathematically a bit tricky, most practical in many\n  circumstances, gives rise to *robust* regression, optimizes for median\n- Mean Absolute Percent Error (MAPE)\n- Checkmark/Pinball Loss: gives rise to *quantile* estimation\n- Huber loss: interpolates MSE and MAE\n\nBy contrast, in the classification context, we have *many* loss functions to \nwork with. To get into them, it's worth thinking back to the formalism of\n*statistical hypothesis testing*. Recall that, in hypothesis testing, we have\na 2-by-2 table of possible outcomes: \n\n+--------------+----------------+-----------------+\n|              | **Truth**                        |\n+==============+:==============:+:===============:+\n| **Decision** | **Null**       | **Alternative** |\n+==============+================+=================+\n| **Retain**   | True           | False Negative  |\n| **Null**     | Negative       | (Type II Error) |\n+--------------+----------------+-----------------+\n| **Reject**   | False          | True Positive   |\n| **Null**     | Positive       |                 |\n|              | (Type I Error) |                 |\n+--------------+----------------+-----------------+\n\n: Hypothesis Testing Table\n\nSuppose that we have a large number ($n$) of binary classification points. If we\nlet $\\hat{y}_i$ be our prediction and $y^*_i$ be the actual outcome, we can\nconstruct a similar table: \n\n+----------------+-----------------+--------------------------+\n|                | **Truth** (Second index)                   |\n+================+:===============:+:========================:+\n| **Prediction** | $y_i^* = 0$     | $y_i^* = 1$              |\n| (First Index)  |                 |                          |\n+================+=================+==========================+\n| $\\hat{y}_i=0$  | $n_{00}$ True   | $n_{01}$ False Negative  |\n|                | Negatives       | (Type II Errors)         |\n+----------------+-----------------+--------------------------+\n| $\\hat{y}_i=1$  | $n_{10}$ False  | $n_{11}$ True Positives  |\n|                | Positives       |                          |\n|                | (Type I Errors) |                          |\n+----------------+-----------------+--------------------------+\n\n: Classification Outcome Table\n\n\nFrom this structure, we get several useful ideas: \n\n- A *confusion matrix* is a 2x2 (or $K$-by-$K$ for $K$-multiclass problems)\n  table comparing the right answer (here \"Truth\" on the columns) with our guess\n  (here \"Decision\" on the rows). A good classifier will have large values \n  \"on diagonal\" ($n_{00} + n_{11}$) and small values \"off diagonal\" \n  ($n_{01} + n_{10}$)\n- The notion of *true/false* *positive/negatives*. These can be a bit tricky\n  to remember, but the convention is: \n  \n  - The *noun* (positive/negative) captures the *prediction*\n  - The *adjective* (true/false) assesses the prediction\n  \n  So a \"false positive\" means that we guessed the positive (+1) class, but that\n  we were wrong and the truth was the negative (-1 or 0) class.\n  \nFrom this simple count table, we can create _many_ different ratios. I find the\nstatistical terminology a bit more intuitive than the ML terminology: \n\n- True Positive Rate: Of the actual positives ($y^*=1$), what fraction are\n  predicted correctly (truly) positives ($\\hat{y} = 1$)? \n  $\\text{TPR} = n_{11}/(n_{01} + n_{11})$\n- False Negative Rate: Of the actual positives ($y^*=1$), what fraction are\n  predicted incorrectly (falsely) as negatives ($\\hat{y} = 0$)? \n  $\\text{FNR} = n_{01}/(n_{01} + n_{11})$\n- True Negative Rate: Of the actual negatives ($y^*=0$), what fraction are\n  predicted correctly (truly) as negatives ($\\hat{y} = 0$)?  \n  $\\text{FNR} = n_{00}/(n_{00} + n_{10})$\n- False Positive Rate: Of the actual negatives ($y^*=0$), what fraction are\n  predicted incorrectly (falsely) as positives ($\\hat{y} = 1$)? \n  $\\text{FPR} = n_{10} / (n_{00} + n_{10})$\n  \nThese clearly satisfy some useful relationships: \n\n$$\\text{TPR} = 1 - \\text{FNR} \\Leftrightarrow \\text{FNR} = 1 - \\text{TPR}$$\n\nand \n\n$$\\text{FPR} = 1 - \\text{TNR} \\Leftrightarrow \\text{TNR} = 1 - \\text{FPR}$$\n\nIn this scenario, we recognize the *false positive rate* as the *size* (or \n*level*) associated with a statistical test (5% for a 95% confidence test) and\nthe true positive rate as the *power*. Again, the statistical definitions can\nhelp us navigate the confusing terminology here: the confidence of a test,\nand therefore its false positive rate, are defined _assuming the null_ (*i.e.*,\nin a \"true\" negative scenario). \n\nThe \"other direction\" of rates -- where we condition on the prediction \ninstead of the truth -- are less common, except for the *false discovery rate*: \n\n$$\\text{FDR} = \\frac{n_{10}}{n_{10} + n_{11}}$$\n\nThe FDR is commonly used in scientific settings and it answers this question: \nif a scientist makes $K$ discoveries, what fraction _of those claimed \ndiscoveries_ are correct? \n\n::: {.callout-warning} \n\nNote that the convention used to define *rates* can be quite confusing\nand is perhaps opposite the convention used to define *counts*. \n\nTo compute the number of *true positives*, we look at predicted \npositives and count the number of \"truths\", while to compute\nthe *true positive rate*, our denominator is the number of \n_actual_ positives and we count the fraction of correct predictions. \n\nTo me, the FDR is actually the most clearly named rate; I try to remember\nthe definition of FDR and 'work backwards' to FPR since it must be something\ndifferent than FDR.\n\n:::\n\nIn ML context, you will commonly hear reference to *precision* and *recall*. \nWith our definitions above, we have\n\n- Precision = $1 - \\text{FDR}$\n- Recall = $\\text{FNR} = 1 - \\text{TPR}$\n\nso, by maximizing precision, we ensure that all of our predicted positives are\ntrue positives and, by maximizing recall, we ensure that we have no false \nnegatives (that is, we truly identify all of the real positives). \n\nIn the medical context, you may encounter alternative metrics of \n\n- Sensitivity, equal to TPR\n- Specificity, equal to TNR\n\nThis zoo of terms is admittedly quite confusing, but the Wikipedia article\non [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Definition)\nhas an excellent table. \n\n### Combining Metrics\n\nIf you look at the 2x2 table above, you can convince yourself that it has \ntwo \"degrees of freedom:\" having a high TPR implies nothing about the TNR \n(or equivalent metrics). That is, it is possible to have a classifier with\narbitrarily good TPR (equivalently, small FNR) and terrible TNR (equivalently,\nsmall FPR). \n\nIn fact, we can construct one trivially: \n\n$$\\hat{f}(\\cdot) = 1$$ \n\nThat is, the function that predicts $1$ _always_. This classifier has zero false\nnegatives, because it has no negatives whatsoever, but it necessarily has many\nfalse positives.[^rate] \n\nConversely, \n\n$$\\hat{f}(\\cdot) = 0$$ \n\nhas no false positives, but many false negatives. \n\n[^rate]: In this case, the FPR is exactly equal to the negative rate in the \npopulation. \n\nClearly, we need a 'combination' metric that combines both degrees of freedom\ninto a single score. Popular choices include: \n\n- Accuracy: the fraction of correct predictions ($(n_{00}+n_{11})/n$)\n- Balanced Accuracy: $\\frac{\\text{TPR} + \\text{TNR}}{2}$\n- $F_1$ Score: $2 *(1 - \\text{FDR}) / (\\text{TPR} - \\text{FDR} +1)$\n\nWhile these choices are popular, none of them are actually all that suitable\nfor a real application. To properly evaluate a classifier, we need to know\nthe real cost of false positives and false negatives, as well as the population\nbase rates, and balance things accordingly. \n\nFor instance, if we are designing a 'baseline' medical diagnostic to be used\nas part of annual physicals, we need to think about what happens with false\nnegatives and false positives. \n\n- If we give a false positive, patients will undergo a better test that costs\n  \\$1,000 but gives an accurate answer in all circumstances (a \"gold standard\").\n- If we give a false negative, patients will leave a condition untreated until\n  it becomes far worse, requiring expensive surgery costing \\$50,000. \n  \nIn this case, we might desire a classifier that minimizes $$n_{10} + 50 n_{01}$$,\nnot any of the of the 'standard' metrics. Note also that the particular FPR, FNR\nof this classifier will depend on the rate of the condition in the population,\nso we can't actually find the best procedure by looking at FPR, FNR in isolation.\n\nWhile this *decision-theoretic* approach is optimal, it is often quite difficult\nto characterize in practice. In the example above, for instance, we made the\nchoice to only minimize cost, without seeking to minimize suffering. (What if\nthe untreated condition is _incredibly_ painful? Should we intervene more often?)\nThis sort of decision-making is ultimately very problem-specific and too often\nignored in the academic literature. In practice, designing (and optimizing)\nthe correct loss function is far more relevant to business outcomes than\nfinding the best classifier to optimize an irrelevant cost function. \nprocedure requires \n\n### Trade-Off of Recall and Precision\n\nOur discussion of 'trivial' classifiers that focus on one metric while completely\nignoring the other may remind you of the way this is typically solved in\nstatistical testing procedures. We specify a maximum acceptable false positive\nrate (level or 1 - confidence) and then minimize the false negative rate\nsubject to that constraint; in statistical speak, we seek a test with \n_maximal power_. \n\nAs we have seen with ridge and lasso regression, there is typically a 'knob'\n(hyperparameter) we can pick to tune a particular classifier. It is common to\napply the classifier at all values of this knob, measure the TPR and TNR at all\nlevels, and plot them in a trade-off curve. This curve is, for historical \nreasons, known as the receiver operating characteristic (ROC)[^roc] curve and\nit looks something like: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(yardstick)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(dplyr)\n# Example from ?roc_curve\nroc_curve(two_class_example, truth, Class1) %>%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes05_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nHere we see that, for this particular classifier, increased specificity is\nassociated with decreased sensitivity or, in the alternative terms, increased TNR\nis associated with decreased TPR. As illustrated by our example of constant\npredictors above, this is a quite typical trade-off. \n\nCurves like this can be used in a few ways, none of which are fully satisfying: \n\n- If we have two curves, we can plot their ROCs on the same set of axes. If one\n  classifier is _uniformly_ above the other, it is said to _dominate_ the other\n  and is better for all applications. Sadly, this is rarely the case. \n- If we have two curves and plot their ROCs, but the ROCs cross, we don't have\n  strong guidance on which model to prefer, but if one is _almost always_ above\n  the other, we will likely still use it. \n\nA commonly reported metric is the _area under the ROC curve_ (AUC), which ranges\nfrom 0.0 to 1.0, with higher values being better. This is a bit unsatisfying\ncompared to the decision theoretic analysis, but it is an improvement over\nsome other metrics as it at least addresses the fact there is a tradeoff. \n\n::: {.callout-info title=\"Pause for Reflection\"}\n\nWhat does an AUC less than 0.5 imply? If you get an AUC less than 0.5, what \nshould you do? Why is AUC near 0 actually a good thing?\n\n:::\n\n[^roc]: Don't confuse this with _Rate of Change_. \n\nWhile the ROC is typically motivated by presence of a 'tuning knob', even\nmethods like logistic regression (which have no obvious knobs) have an implicit\nknob we can choose to tune: the _classification threshold_. There is no law of\nnature that says that a predicted probability of 55\\% has to correspond to\na prediction of the positive class. If false positives are very expensive, we\nmay want to reduce the fraction of positives predicted and only predict \nthe positive class when the estimated probability is over 80\\%. Similarly, \nif false negatives are very bad, we may want to predict positive even when\nthe probability of a positive is in the 40\\% range: this is particularly \ncommon in medical contexts where the impact of a missed diagnosis is quite\nsubstantial, so doctors order extra diagnostic work 'just to be sure' even\nif the test does not strongly suggest a disease. \n\n### Scoring Functions\n\n_Scoring_ functions are loss functions particularly suited for evaluation of \nprobabilistic classifiers. Unlike our confusion-matrix set of metrics, it's less\nclear how we should evaluate probabilistic classifiers. \n\nIt is clear that, if we predict something with 80% probability and it doesn't\nhappen, this is a 'worse' mistake than if we only had a 51% probability, but\nhow can we measure this formally? \n\n- Calibration and Tightness\n- Log Scores\n\n## Types of Classification Methods\n\nIn this course, we will consider two main types of classifiers: \n\ni) Generative classifiers, which attempt to jointly model $(\\bX, \\by)$ and use\n   probabilistic mathematics (mainly Bayes' Rule) to infer $\\by | \\bX$ from\n   the joint model. \n   \nii) Discriminative classifiers, which directly attempt to estimate the 'boundary'\n    between the classes. \n\nAs you will see, the terminology around these methods is _particularly_ terrible. \n\n## Building Mutliclass Classifiers from Binary Classifiers\n  \nAs we introduce methods, we will typically derive them for the binary\nclassification task. If we want to apply them to the multiclass task, how can \nwe extend them? It would be quite cumbersome if we needed _brand new_ methods\neach time. \n\nIn general, there are two main strategies we might use: \n\n- One vs Rest\n- Each vs Each\n\nFor simplicity, assume we are predicting $K$ classes. In the \"one-vs-rest\"\nstrategy, we will build $K$ binary classifiers, of the form: \n\n- Class 1 vs Not Class 1 (*i.e.*, Class 2, 3, 4, $\\dots$, $K$)\n- Class 2 vs Not Class 2 (*i.e.*, Class 1, 3, 4, $\\dots$, $K$)\n- *etc.*\n- Class $K$ vs Not Class $K$ (*i.e.*, Class 1, 2, 3, $\\dots$, $K-1$)\n\nTo make an actual prediction, we take the *highest* score of each of these\nclassifiers. *E.g.*, if $K=3$ for categories \"Red\", \"Green\" and \"Blue\", we\nmight predict: \n\n- $\\mathbb{P}(\\text{Red}) = 0.8$ and $\\mathbb{P}(\\text{Not Red}) = 0.2$\n- $\\mathbb{P}(\\text{Green}) = 0.4$ and $\\mathbb{P}(\\text{Not Red}) = 0.6$\n- $\\mathbb{P}(\\text{Blue}) = 0.2$ and $\\mathbb{P}(\\text{Not Red}) = 0.8$\n\nOur final prediction would then be the maximizer, \"Red.\"\n\nThis strategy is relatively simple and intuitive, but for very large $K$, \nthe maximum can be far less than 50%, so it may feel a bit funny. (Why is \nthis not an issue for $K=2$?)\n\nConversely, in the \"each vs each\" (sometimes called \"one vs one\") setting, \nwe train $\\binom{K}{2} = K(K-1)/2$ classifiers for every possible _pair_ of\nclasses. To get the prediction, we then take a 'majority vote' of the classifiers. \n\nIn our $K=3$ example above, we would have three classifiers: \n\n- Red vs Blue, which we fit after throwing out the \"Green\" points\n- Red vs Green, which we fit after throwing out the \"Blue\" points\n- Green vs Blue, which we fit after throwing out the \"Red\" points\n\nIf we make predictions for a single point, our binary classifications might be\n\n- Red vs Blue: Red\n- Red vs Green: Red\n- Green vs Blue: Blue\n\nIn this case, we would take a majority vote to get \"Red\" as our final prediction.\n\nI find this strategy a bit counterintuitive, but it is sometimes faster than\nthe one-vs-rest strategy since each individual model is fit to a subset\nof the entire data set.\n\nCertain modern ML strategies, in particular deep neural networks, predict\nthe probability of all $K$ classes from a single model. In this case, we \ntypically select the single most probable class as our prediction. \n\nIn applications where there are multiple contenders and the difference\nbetween them is small, it is common to randomly select a class in\nproportion to its predicted probability. This strategy is one of the things\nthat gives ChatGPT its randomness; it usually predicts the *most likely* next\nword, but sometimes it does something a little bit weird.)\n\nNote that it is actually usually pretty hard to make a model that predicts\na large set of probabilities simultaneously: in particular, it's hard to get\na vector that sums to one (like probabilities should). To address this, a\n[softmax](https://en.wikipedia.org/wiki/Softmax_function) normalization is\napplied. This is essentially nothing more than i) making sure each predicted\nvalue is positive; and ii) dividing them all by the sum so they add to 1. \n\n",
    "supporting": [
      "notes05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}