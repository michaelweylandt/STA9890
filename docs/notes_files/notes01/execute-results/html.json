{
  "hash": "04f0c74624ba22e34d5e273f33cd94cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"{{< var course.short >}} - Introduction to Machine Learning\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n## A Taxonomy of Machine Learning\n\nWhere classical statistics focuses on learning information about a\npopulation from a (representative) sample,  Machine Learning \nfocuses on out-of-sample prediction accuracy. \n\nFor example, given a large set of medical records, the natural\ninstinct of a statistician is to find the indicators of cancer in\norder assess them via some sort of follow-on genetic study, while a\nML practitioner will typically start by building a predictive\nalgorithm to predict who will be diagnosed with cancer. The\nstatistician will perform calculations with $p$-values, test\nstatistics, and the like to make sure that any discovered\nrelationship is accurate, while the ML practitioner will verify\nthe performance by finding a new (hopefully similar) set of medical\nrecords to test algorithm performance. \n\nClearly, this difference is more one of style than substance: the\nstatistician might see what features are important in the ML model to\ndecide what to investigate, while the ML modeler will use statistical\ntools to make sure the model is finding something real and not just\nfitting to noise. In this course, the distinction may be even\nblurrier as our focus is *statistical machine learning* - that little\nniche right on the boundary between the two fields. \n\nIn brief, \"statistics vs ML\" is a bit of a meaningless distinction as\nboth fields draw heavily from each other. I tend to say one is *doing\nstatistics* whenever the end-goal is to *better understand* something\nabout the real world (*ie.*, the end product is *knowledge*), while\none is *doing ML* whenever one is building a system to be used in an\nautomated fashion (*ie.*, the end product is *software*), but\ndefinitions vary.[^culture]\n\n[^culture]: There are some cultural differences that come from ML's\nCS history vs Statistics' \"Science-Support\" history: notably, ML\nfolks think of (binary) classification while statisticians think of\nregression as the first task to teach.)\n\nIn this course we will use the following taxonomy borrowed from the\nML literature: \n\n- Supervised Learning: Tasks with a well-defined target variable (output)\nthat we aim to predict\n\n  Examples: \n  - Given an image of a car running a red-light, read its license plate.\n  - Given attendance records, predict which students will not pass a course.\n  - Given a cancer patient's medical records, predict whether a certain drug\n    will have a beneficial impact on their health outcomes. \n  \n- Unsupervised Learning: Given a whole set of variables, none of which is\n  considered an output, learn useful underling structure. \n\n  Examples: \n  - Given a set of climate simulations, identify the general trends of a \n    climate intervention.\n  - Given a set of students' class schedules, group them into sets. (You might\n    imagine these sets correspond to majors, but without that sort of \"label\" \n    (output variable) this is only speculative, so we're unsupervised)\n  - Given a social network, identify the most influential users.\n\nThere are other types of learning tasks: *e.g.* semi-supervised, online,\nreinforcement, but the Supervised/Unsupervised distinction is the main one\nwe will use in this course. \n\nWithin Supervised Learning, we can further subdivide into two major categories:\n\n- Regression Problems: problems where the response (label) is a\n  real-valued number\n- Classification Problems: problems where the response (label) is a\n  category label.\n  \n  - Binary classification: there are only two categories\n  - Multi-way or Multinomial classification: multiple categories\n  \nLinear Regression, which we will study more below, is the canonical\nexample of a *regression* tool for *supervised* learning.\n\nAt this point, you can already foresee one of the (many) terminology\ninconsistencies will will encounter in this course: _logistic regression_\nis a tool for _classification_, not regression. As modern ML is the intersection\nof many distinct intellectual traditions, the terminology is rarely \nconsistent.[^uk]\n\n[^uk]: You may recall the famous quip about the US and the UK: \"Two countries,\nseparated by a common language.\"\n\n## Test and Training Error\n\nAs we think about measuring a model's predictive performance, it becomes\nincreasingly important to distinguish between _in-sample_ and _out-of-sample_\nperformance, also called training (in-sample) and testing (out-of-sample)\nperformance. \n\nIn previous courses, you likely have assessed model fit by seeing how well\nyour model fits the data it was trained on: statistics like $R^2$, SSE, SSR in\nregression or $F$-tests for model comparison do just this. As you have used them,\nthey are primarily useful for comparison of _similar_ models, *e.g.*, OLS with\ndifferent numbers of predictor variables. But it's worth reflecting on this\nprocess a bit more: didn't the model with more predictors always fit the data\nbetter? If so, why don't we always just include all of our predictors? \n\nOf course you know, the answer is that we want to avoid overfitting. Just\nbecause a model fit the data a bit better doesn't mean it is actually better. \nIf you need 1,000 variables to get a 0.1% reduction in MSE, do you really\nbelieve those features are doing much? No! \n\nYou likely have a sense that a feature needs to \"earn its keep\" to be worth\nincluding in a model. Statisticians have formalized this idea very well\nin some contexts: quantities like _degrees of freedom_ or _adjusted_ $R^2$\nattemps to measure whether a variable provides a _statistically significant_\nimprovement in performance. These calculations typically rely on subtle\ncalculations involving nice properties of the multivariate normal distribution\nand ordinary least squares, or things that can be (asymptotically) considered\nessentially equivalent. \n\nIn this class, we don't want to make those sorts of strong distributional and\nmodeling assumptions. So what can we do instead? Well, if we want to see if \nModel A predicts more accurately than Model B on new data, why don't we just\ndo that? Let's get some new data and compare the MSEs of Model A and Model B:\nwhichever one does better is the one that does better.[^sig]\n\nThis is a pretty obvious idea, so it's worth asking why it's not the baseline\nand why statisticians bothered with all the degrees of freedom business to start\nwith. As always, you have to know your history: statistics comes from a lineage\nof scientific experimentation where data is limited and often quite expensive to\nget. If you are running a medical trial, you can't just toss a few hundred extra\nparticipants in - this costs money! If you are doing an agricultural experiment,\nit may take several years to see whether a new seed type actually has higher\nyield than the previous version. It's also not clear how one should separate\ndata into training and test sets: if you are studying, *e.g.*, friendship\ndynamics on Facebook, you don't have an (obvious) \"second Facebook\" that you can\nuse to assess model accuracy. \n\nBy contrast, CS-tradition Machine Learning comes from a world of \"internet-scale\"\nwhere data is plentiful, cheap, and is continuously being collected.[^anec] Not\nall problems fall in this regime but, as we will see, enough do that it's worth \nthinking about what we should do in this scenario. Excitingly, if we don't\ndemand a full and exhaustive mathematical characterization of a method before we\nactually apply it, we can begin to explore much more complex and interesting\nmodels.\n\n::: {.callout-tip title=\"Advice\"}\n\nA good rule of thumb for applied statistical and data science work: begin by\nasking yourself what you would do if you had access to the whole population\n(or an infinitely large sample) and then adapt that answer to the limited data\nyou actually have. You always want to make sure you are asking the right \nquestion, even if you are only able to give an approximate finite-data answer,\nrather than giving an 'optimal' answer to a question you don't actually care\nabout.\n\n:::\n\nSo, for the first two units of this course, we will put this idea front and\ncenter: we will fit our models to a _training set_ and then see how well they\nperform on a _test set_. Our goal is to not to find find models which perform\nwell on the test set *per se*: we really want to find models that perform well\non the all future data, not just one test set. But this training/test split will\ncertainly get us going in the right direction. \n\nLooking ahead, let's note some of the key questions we will come back to again\nand again: \n\n- If we don't have an explicit test set, where can we get one? Can we 'fake it\n  to make it'? \n- What types of models have small \"test-training\" gap, *i.e.* do about as well\n  on the test and training sets, and what models have a large gap? \n\n[^anec]: Anecdotally, a Google Data Scientist once mentioned to me that they\nrarely bother doing $p$-value or significance calculations. At the scale of\nGoogle's A/B testing on hundreds of billions of ad impressions, _everything_\nis statistically significant. \n\n[^sig]: You might still worry about the randomness of this comparison process:\nif we had a slightly different sample of our new data, would the better model\nstill look better? You aren't wrong to worry about this, but we have at a minimum\nmade the problem much easier: now we are just comparing the means of two error\ndistributions - classic $t$-test stuff - as opposed to comparing two (potentially\ncomplex) models.\n\n## Model Complexity\n\n\n## Nearest Neighbor Methods\n\nLet's now see how complexity plays out for a very simple classifier, \n$K$-Nearest Neighbors (KNN). KNN formalizes the intuition of \n\"similar inputs -> similar outputs.\" KNN looks at the $K$ most similar points\nin its training data  (\"nearest neighbors\" if you were to plot the data) and\ntakes the average label to make its prediction.[^details]\n\n[^details]: There are details here about how we measure similarity, but for\nnow we will restrict our attention to simple Euclidean distance. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(class) # Provides a KNN function for classification\nargs(knn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) \nNULL\n```\n\n\n:::\n:::\n\n\n\nYou can see here that KNN requires access to the full training set at\nprediction time: this is different than something like OLS where we reduce our\ndata to a set of regression coefficients (parameters).[^np]\n\n[^np]: KNN is a *non-parameteric* method, but not all *non-parametric* methods\nrequire access to the training data at test time. We'll cover some of those\nlater in the course.\n\nWe'll also need some data to play with. For now, we'll use synthetic data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Make two interleaving half-circles\n#'\n#' @param n_samples Number of points (will be divided equally among the circles)\n#' @param shuffle Whether to randomize the sequence\n#' @param noise Standard deviation of Gaussian noise applied to point positions\n#'\n#' @description Imitation of the Python \\code{sklearn.datasets.make_moons} function.\n#' @return a \\code{list} containining \\code{samples}, a matrix of points, and \\code{labels}, which identifies the circle from which each point came.\n#' @export\nmake_moons <- function(n_samples=100, shuffle=TRUE, noise=0.25) {\n  n_samples_out = trunc(n_samples / 2)\n  n_samples_in = n_samples - n_samples_out\n  \n  points <- matrix( c(\n    cos(seq(from=0, to=pi, length.out=n_samples_out)),  # Outer circle x\n    1 - cos(seq(from=0, to=pi, length.out=n_samples_in)), # Inner circle x\n    sin(seq(from=0, to=pi, length.out=n_samples_out)), # Outer circle y\n    1 - sin(seq(from=0, to=pi, length.out=n_samples_in)) - 0.5 # Inner circle y \n  ), ncol=2) \n  \n  if (!is.na(noise)) points <- points + rnorm(length(points), sd=noise)\n  \n  labels <- c(rep(1, n_samples_out), rep(2, n_samples_in))\n  \n  if (!shuffle) {\n    list(\n      samples=points, \n      labels=labels\n    )\n  } else {\n    order <- sample(x = n_samples, size = n_samples, replace = F)\n    list(\n      samples=points[order,],\n      labels=as.factor(ifelse(labels[order] == 1, \"A\", \"B\"))\n    )\n  }\n}\n```\n:::\n\n\n\nThis function comes from the `clusteringdatasets` R package, but \nthe underlying idea comes from a function in `sklearn`, a popular Python ML \nlibrary.\n\nLet's take a look at this sort of data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTRAINING_DATA <- make_moons()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyverse)\ndata.frame(TRAINING_DATA$samples, \n           labels=TRAINING_DATA$labels) |>\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=3)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nWe can make this a bit more attractive:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMY_THEME <- theme_bw(base_size=20) + theme(legend.position=\"bottom\")\ntheme_set(MY_THEME)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %>%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=2) + \n  ggtitle(\"Sample Realization of the Moons Dataset\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nMuch better!\n\nLet's try making a simple prediction at the point (0, 0):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn(TRAINING_DATA$samples, \n    cl=TRAINING_DATA$labels, \n    test=data.frame(X1=0, X2=0), k=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] B\nLevels: A B\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip title=\"Pause to Reflect\" collapse=\"true\"}\n\nDoes this match what you expect from the plot above? Why or why not?\nThe following image might help: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggforce)\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %>%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point() + \n  ggtitle(\"Sample Realization of the Moons Dataset\") + \n  geom_circle(aes(x0=0, y0=0, r=0.2), linetype=2, color=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nWhy is `R` returning a `factor` response here? What does that tell us about\nthe type of ML we are doing? \n\n:::\n\nWe can also visualize the output of KNN at every point in space:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize_knn_boundaries <- function(training_data, k=NULL){\n    xrng <- c(min(training_data$samples[,1]), max(training_data$samples[,1]))\n    yrng <- c(min(training_data$samples[,2]), max(training_data$samples[,2]))\n    \n    xtest <- seq(xrng[1], xrng[2], length.out=101)\n    ytest <- seq(yrng[1], yrng[2], length.out=101)\n    \n    test_grid <- expand.grid(xtest, ytest)\n    colnames(test_grid) <- c(\"X1\", \"X2\")\n    \n    pred_labels = knn(training_data$samples, \n                      cl=training_data$labels, \n                      test_grid, \n                      k=k)\n    \n  ggplot() + \n  geom_point(data=data.frame(TRAINING_DATA$samples, \n                             labels=TRAINING_DATA$labels), \n             aes(x=X1, y=X2, color=labels), \n             size=3) + \n  geom_point(data=data.frame(test_grid, pred_labels=pred_labels), \n             aes(x=X1, y=X2, color=pred_labels), \n             size=0.5) + \n  ggtitle(paste0(\"KNN Prediction Boundaries with K=\", k))\n}\n\nvisualize_knn_boundaries(TRAINING_DATA, k=1)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nIf we raise $K$, we get smoother boundaries: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize_knn_boundaries(TRAINING_DATA, k=5)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nAnd if we go all the way to $K$ near to the size of the training data, \nwe get very boring boundaries indeed:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize_knn_boundaries(TRAINING_DATA, k=NROW(TRAINING_DATA$samples)-1)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n::: {.callout-tip title=\"Pause to Reflect\" collapse=\"true\"}\n\nWhat does this tell us about the _complexity_ of KNN as a function of $K$?\n\n:::\n\nIn the terminology we introduced above, we see that increasing $K$ \ndecreases model complexity (wiggliness). \n\nLet's now see how training error differs as we change $K$: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTEST_DATA <- make_moons()\n\nTRAINING_ERRORS <- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train <- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TRAINING_DATA$samples, k=k)\n    true_labels_train <- TRAINING_DATA$labels\n    err <- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the training (in-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TRAINING_ERRORS <- rbind(TRAINING_ERRORS, data.frame(k=k, training_error=err))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAt k = 1, the training (in-sample) error of KNN is 0%\nAt k = 2, the training (in-sample) error of KNN is 9%\nAt k = 3, the training (in-sample) error of KNN is 6%\nAt k = 4, the training (in-sample) error of KNN is 4%\nAt k = 5, the training (in-sample) error of KNN is 5%\nAt k = 6, the training (in-sample) error of KNN is 9%\nAt k = 7, the training (in-sample) error of KNN is 5%\nAt k = 8, the training (in-sample) error of KNN is 6%\nAt k = 9, the training (in-sample) error of KNN is 7%\nAt k = 10, the training (in-sample) error of KNN is 4%\nAt k = 11, the training (in-sample) error of KNN is 6%\nAt k = 12, the training (in-sample) error of KNN is 5%\nAt k = 13, the training (in-sample) error of KNN is 6%\nAt k = 14, the training (in-sample) error of KNN is 6%\nAt k = 15, the training (in-sample) error of KNN is 7%\nAt k = 16, the training (in-sample) error of KNN is 9%\nAt k = 17, the training (in-sample) error of KNN is 8%\nAt k = 18, the training (in-sample) error of KNN is 8%\nAt k = 19, the training (in-sample) error of KNN is 9%\nAt k = 20, the training (in-sample) error of KNN is 8%\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(TRAINING_ERRORS, aes(x=k, y=training_error)) + \n    geom_point() + \n    ggtitle(\"Training (In-Sample) Error of KNN\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nCompare this to the test error: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTESTING_ERRORS <- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train <- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TEST_DATA$samples, k=k)\n    true_labels_train <- TEST_DATA$labels\n    err <- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the test (out-of-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TESTING_ERRORS <- rbind(TESTING_ERRORS, data.frame(k=k, test_error=err))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAt k = 1, the test (out-of-sample) error of KNN is 9%\nAt k = 2, the test (out-of-sample) error of KNN is 10%\nAt k = 3, the test (out-of-sample) error of KNN is 8%\nAt k = 4, the test (out-of-sample) error of KNN is 6%\nAt k = 5, the test (out-of-sample) error of KNN is 8%\nAt k = 6, the test (out-of-sample) error of KNN is 7%\nAt k = 7, the test (out-of-sample) error of KNN is 7%\nAt k = 8, the test (out-of-sample) error of KNN is 6%\nAt k = 9, the test (out-of-sample) error of KNN is 5%\nAt k = 10, the test (out-of-sample) error of KNN is 5%\nAt k = 11, the test (out-of-sample) error of KNN is 5%\nAt k = 12, the test (out-of-sample) error of KNN is 5%\nAt k = 13, the test (out-of-sample) error of KNN is 5%\nAt k = 14, the test (out-of-sample) error of KNN is 5%\nAt k = 15, the test (out-of-sample) error of KNN is 5%\nAt k = 16, the test (out-of-sample) error of KNN is 6%\nAt k = 17, the test (out-of-sample) error of KNN is 5%\nAt k = 18, the test (out-of-sample) error of KNN is 5%\nAt k = 19, the test (out-of-sample) error of KNN is 5%\nAt k = 20, the test (out-of-sample) error of KNN is 6%\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(TESTING_ERRORS, aes(x=k, y=test_error)) + \n    geom_point() + \n    ggtitle(\"Test (Out-of-Sample) Error of KNN\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nThe difference between the two is clearer if we put them on the same figure:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nERRS <- inner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |>\n        pivot_longer(-k) |>\n        rename(Error=value, Type=name)\nggplot(ERRS, aes(x=k, y=Error, color=Type)) + \n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nWe notice a few things here: \n\n1) Training Error decreases in $K$, with 0 training error at $K=1$ (Why?)\n2) Test Error is basically always higher than test error\n3) The best training error does not have the best test error\n\nWe can also look at the gap between training and test error: this is called\n`generalization error` or `optimism`: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |>\n    mutate(optimism=test_error - training_error) |>\n    ggplot(aes(x=k, y=optimism)) + \n    geom_point() + \n    geom_line()\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n::: {.callout-tip title=\"Pause to Reflect\" collapse=\"true\"}\n\nConsider the following questions:\n\n- What is the relationship between optimism and model complexity\n- What is the best value of $K$ for this data set?\n- How should we pick the best value of $K$?\n- How might that change if we increase the number of training samples?\n- How might that change if we increase the number of test samples? \n\n:::\n",
    "supporting": [
      "notes01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}