<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michael Weylandt">

<title>STA 9890 - Classification II: Discriminative Classifiers – STA 9890</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7a426326b279d3ba9cc0818763c3c226.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">STA 9890</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././notes.html"> 
<span class="menu-text">Handouts and Additional Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../reports.html"> 
<span class="menu-text">Research Reports</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././competition.html"> 
<span class="menu-text">Course Competition</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././resources.html"> 
<span class="menu-text">Additional Resources and Policies</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././objectives.html"> 
<span class="menu-text">Learning Objectives</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-not-accuracy-maximization" id="toc-why-not-accuracy-maximization" class="nav-link active" data-scroll-target="#why-not-accuracy-maximization">Why Not Accuracy Maximization?</a></li>
  <li><a href="#ols-for-classification" id="toc-ols-for-classification" class="nav-link" data-scroll-target="#ols-for-classification">OLS for Classification?</a></li>
  <li><a href="#deriving-logistic-regression" id="toc-deriving-logistic-regression" class="nav-link" data-scroll-target="#deriving-logistic-regression">Deriving Logistic Regression</a></li>
  <li><a href="#from-model-to-loss-function" id="toc-from-model-to-loss-function" class="nav-link" data-scroll-target="#from-model-to-loss-function">From Model to Loss Function</a></li>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models">Generalized Linear Models</a></li>
  <li><a href="#loss-functions-for-classification" id="toc-loss-functions-for-classification" class="nav-link" data-scroll-target="#loss-functions-for-classification">Loss Functions for Classification</a></li>
  <li><a href="#support-vector-machines" id="toc-support-vector-machines" class="nav-link" data-scroll-target="#support-vector-machines">Support Vector Machines</a></li>
  <li><a href="#svm-the-geometric-viewpoint" id="toc-svm-the-geometric-viewpoint" class="nav-link" data-scroll-target="#svm-the-geometric-viewpoint">SVM: The Geometric Viewpoint</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/michaelweylandt/STA9890/edit/main/notes/notes07.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/michaelweylandt/STA9890/blob/main/notes/notes07.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/michaelweylandt/STA9890/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">STA 9890 - Classification II: Discriminative Classifiers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Michael Weylandt </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p><span class="math display">\[\newcommand{\bX}{\mathbf{X}}\newcommand{\bx}{\mathbf{x}}\newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\P}{\mathbb{P}}\newcommand{\R}{\mathbb{R}} \newcommand{\by}{\mathbf{y}}\newcommand{\argmax}{\text{arg\,max}}\newcommand{\argmin}{\text{arg\,min}}\]</span></p>
<p>This week we begin to examin <em>discriminative</em> classifiers. Unlike generative classifiers, which attempt to model <span class="math inline">\(\bx\)</span> as a function of <span class="math inline">\(y\)</span> and then use Bayes’ rule to <em>invert</em> that model, discriminative classifiers take the more standard approach of directly modeling <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(\bx\)</span>. This is the approach you have already seen in our various variants of OLS.</p>
<section id="why-not-accuracy-maximization" class="level2">
<h2 class="anchored" data-anchor-id="why-not-accuracy-maximization">Why Not Accuracy Maximization?</h2>
<p>As we saw in our regression unit, we can pose many interesting estimators as <em>loss minimization</em> problems. For least squares, we primarily focused on mean squared error (MSE) as a loss function, though we also briefly touched on MAE, MAPE, and ‘check’ losses. There is not a single ‘canonical’ loss for the classification setting and different choices of loss will yield different classifiers.</p>
<p>Before we build our actual first loss, it is worth asking why we can’t use something like classification accuracy as our loss function. Specifically, define <span class="math display">\[\text{Acc}(y, \hat{y}) = \begin{cases} 1 &amp; y = \text{sign}(\hat{y}) \\ 0 &amp; y \neq \text{sign}(\hat{y}) \end{cases}\]</span> Here we consider the case where <span class="math inline">\(\hat{y}\)</span> may be real-valued as a (slight) generalization of probabilistic classification and we use the <span class="math inline">\(\pm 1\)</span> convention for the two classes.</p>
<p>Fixing <span class="math inline">\(y = 1\)</span>, <span class="math inline">\(\text{Acc}\)</span> has the following shape:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out=</span><span class="dv">201</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">&lt;-</span> <span class="cf">function</span>(y_hat, <span class="at">y=</span><span class="dv">1</span>) <span class="fu">sign</span>(y_hat) <span class="sc">==</span> <span class="fu">sign</span>(y)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y_hat, <span class="fu">acc</span>(y_hat), <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="fu">expression</span>(<span class="fu">hat</span>(y)),</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="fu">expression</span>(<span class="fu">Acc</span>(<span class="fu">hat</span>(y), y<span class="sc">==</span><span class="dv">1</span>)), </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Accuracy as a Loss Function"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes07_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This is a terrible loss function! It is both non-convex (can you see why?) and just generally unhelpful. Because it is flat almost everywhere, we have no access to gradient information (useful for fitting) or to any sense of ‘sensitivity’ to our loss. In this scenario, if <span class="math inline">\(y=1\)</span>, both <span class="math inline">\(\hat{y}=0.00001\)</span> and <span class="math inline">\(\hat{y}=0.9999\)</span> have the same accuracy even though the latter is a more ‘confident’ prediction.</p>
</section>
<section id="ols-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="ols-for-classification">OLS for Classification?</h2>
<p>So if not accuracy, where else might we get a good loss function? You might first ask if we an use OLS? It works perfectly well for regression and we know it is well-behaved, so what happens if we try it for classification? The answer is … complicated.</p>
<p>OLS as a <em>loss</em> function for classification is a bit strange, but it turns out to be more-or-less fine. In particular, OLS can be used to evaluate <em>probabilistic</em> classifiers, where it is known as the <em>Brier score</em>. OLS as a <em>predictor</em> can be a bit more problematic: in particular, if we just predict with <span class="math inline">\(\P(y=1) = \bx^{\top}\bbeta\)</span>, we have to deal with the fact that <span class="math inline">\(\hat{y}\)</span> can be <em>far</em> outside a <span class="math inline">\([0, 1]\)</span> range we might want from a probabilistic classifier. In certain limited circumstances, we can assume away this problem (perhaps by putting specific bounds on <span class="math inline">\(\bx\)</span>), but these fixes are fragile. Alternatively, we can try to ‘patch’ this approach and use a classifier like</p>
<p><span class="math display">\[\P(y=1) = \begin{cases} 1 &amp; \bx^{\top}\bbeta &gt; 1 \\ 0 &amp; \bx^{\top}\bbeta &lt; 0 \\ \bx^{\top}{\bbeta} &amp; \text{ otherwise}\end{cases}\]</span></p>
<p>Even if this feels a bit unsophisticated, Ttis is not awful, but it still struggles from the ‘long flat region’ problems that accuracy encounters.</p>
<p>At this point, it’s hopefully clear that it will be a bit hard to ‘hack together’ a suitable loss and that we might benefit from approaching the problem with more theoretical grounding.</p>
</section>
<section id="deriving-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="deriving-logistic-regression">Deriving Logistic Regression</h2>
<p>While we argued that OLS is well-grounded in MSE alone, we can recall it has additional connections to the Gaussian. These “Gaussian vibes” were not essential to making OLS work, but they were useful in <em>deriving</em> OLS as a sensible procedure. Can we do something similar for classification? That is, can we assume a ‘working model’ to come up with a good loss function and then use it, even if we doubt that the model is ‘correct’?</p>
<p>As with all leading questions, the answer is of course a resounding yes. We start from the rather banal observation that <span class="math inline">\(y\)</span> must have a Bernoulli distribution conditional on <span class="math inline">\(\bx\)</span>. After all, the Bernoulli is (essentially) the only <span class="math inline">\(\{0, 1\}\)</span> distribution we have to work with. So we really just need a way to model the <span class="math inline">\(p\)</span> parameter of a Bernoulli as a function of <span class="math inline">\(\bx\)</span>. Because we love linear models, we may choose to set <span class="math inline">\(p = \bx^{\top}\bbeta\)</span>, but this gets us back to the range problem we had above.</p>
<p>Specifically, we require the Bernoulli parameter to take values in <span class="math inline">\([0, 1]\)</span> but our linear predictor <span class="math inline">\(\bx^{\top}\bbeta\)</span> takes values in all of <span class="math inline">\(\R\)</span>. We can fix this with a simple ‘hack’: we need a function that ‘connects’ <span class="math inline">\(\R\)</span> to <span class="math inline">\([0, 1]\)</span>. If we call this function <span class="math inline">\(\mu\)</span>, we then can specify our whole model as <span class="math display">\[y | \bx \sim \text{Bernoulli}(\mu(\bx^{\top}\bbeta))\]</span> and reduce our problem to estimating <span class="math inline">\(\bbeta\)</span>. Where can we get such a function?</p>
<p>The most common choice is <span class="math display">\[\mu(z) = \frac{e^z}{1+e^z} = \frac{1}{1+e^{-z}}\]</span>, which is known as the <em>logistic</em> or <em>sigmoid</em> function due to its ‘s-like’ shape:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out=</span><span class="dv">201</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(z, <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>z)), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">main=</span><span class="st">"Sigmoid Mapping"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes07_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>More generally, we can use essentially the CDF of any random variable supported on the real line as a choice of <span class="math inline">\(\mu\)</span>: since CDFs map the support onto the <span class="math inline">\([0, 1]\)</span> range they are perfect for this choice. Unfortunately, most CDFs are a bit unwieldy so this approach, while theoretically satisfying, is not too widely used in practice. If you see it, the most common choice is <span class="math inline">\(\mu(z) = \Phi(z)\)</span>, the normal CDF, resulting in a method known as “probit” regression (after ‘probability integral transform,’ an old-fashioned name for the CDF) or <span class="math display">\[\mu(z) = \frac{\tan^{-1}(z)}{\pi} + \frac{1}{2}\]</span> which gives a method known as “cauchit” regression, since this is the CDF of the standard Cauchy distribution. These choices are far less common than the default sigmoid we used above. Generally, they are only <em>slightly</em> different in practice and far more computationally burdensome and just aren’t really worth it.</p>
<p>Returning to our default sigmoid, we now have the model <span class="math display">\[ y | \bx \sim \text{Bernoulli}\left(\frac{1}{1+e^{-\bx^{\top}\bbeta}}\right)\]</span> This model is well-posed (in the sense that the distribution ‘fits’ the data and we are guaranteed never to put invalid parameters in) but we don’t yet have a way to use it as a loss function.</p>
</section>
<section id="from-model-to-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="from-model-to-loss-function">From Model to Loss Function</h2>
<p>We can build a loss-function by relying on the <em>maximum likelihood</em> principle. The maximum likelihood principle is a core idea of statistics - and one you will explore in much greater detail in other courses - but, in essence, it posits that we should use the (negative) PMF or PDF as our loss function. Specifically, the ML principle says that, if many models could fit our data, we should pick the one that makes our data <em>most probable</em> (as determined by the PMF/PDF).</p>
<p>Before we work out the ML estimator (MLE) for our classification model, let’s take a brief detour and look at the MLE for (Gaussian) regression. Specifically, if we assume a model <span class="math display">\[y \sim \mathcal{N}(\bx^{\top}\bbeta, \sigma^2)\]</span> for known <span class="math display">\[\sigma^2\]</span>, the PDF of the training point <span class="math inline">\((\bx_1, y_1)\)</span> is</p>
<p><span class="math display">\[p(y_1 | \bx_1) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{\|y_1 - \bx_1^{\top}\bbeta\|_2^2}{2\sigma^2}\right\}\]</span></p>
<p>If we have a set of <span class="math inline">\(n\)</span> IID training data points, the joint PDF can be obtained by multiplying together PDFs (IID is great!) to get</p>
<p><span class="math display">\[\begin{align*}
p(\mathcal{D}_{\text{train}}) &amp;= \prod_{i=1}^n p(y_i | \bx_i) \\
&amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{\|y_i - \bx_i^{\top}\bbeta\|_2^2}{2\sigma^2}\right\} \\
&amp;= (2\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n \|y_i - \bx_i^{\top}\bbeta\|_2^2\right\} \\
&amp;= (2\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2} \|\by - \bX\bbeta\|_2^2\right\} \\
\end{align*}\]</span></p>
<p>We could maximize this, but a few minor tweaks make the problem much easier:</p>
<ol type="i">
<li>Since all of the action is in an exponential, we might as well maximize the <em>log</em> of the PDF instead of the raw PDF. Since logarithms are a monotonic transformation, this won’t change our minimizer. Similarly, we will also strategically drop some constant terms as they also do not change the minimizer.</li>
<li>By convention, we like to <em>minimize</em> more than <em>maximize</em>, specifically when dealing with convex approaches, so we will introduce a sign flip.</li>
</ol>
<p><span class="math display">\[\begin{align*}
\hat{\bbeta} &amp;= \argmax_{\bbeta} p(\mathcal{D}_{\text{train}}) \\
             &amp;= \argmax_{\bbeta} \log p(\mathcal{D}_{\text{train}}) \\
             &amp;= \argmax_{\bbeta} \log\left( (2\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n \|\by - \bX\bbeta\|_2^2\right\}\right) \\
             &amp;= \argmax_{\bbeta} -\frac{n}{2}\log(2\pi\sigma^2) -\frac{1}{2\sigma^2} \|\by - \bX\bbeta\|_2^2 \\
             &amp;= \argmin_{\bbeta} \frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2} \|\by - \bX\bbeta\|_2^2 \\
             &amp;= \argmin_{\bbeta} \frac{1}{2}\|\by - \bX\bbeta\|_2^2
\end{align*}\]</span></p>
<p>Why were we able to do the simplifications in the last line? (You might also recognize the <em>penultimate</em> line as a term used in the AIC of a linear model.)</p>
<p>This is quite cool! If we assume <span class="math inline">\(\by\)</span> follows a particular Gaussian, we get OLS by applying the general MLE approach. Deeper statistical theory tells us that the MLE is (essentially) optimal, so if our data is not too ‘un-Gaussian’ it makes sense that the Gaussian MLE (OLS) will perform reasonably well. Since many things in this world are Gaussian-ish, OLS is optimal-ish for a large class of interesting problems.</p>
<p>Returning to our classification problem, we see that if our training data is IID, the MLE can be obtained by minimizing the negative sum of the log PDFs. For space, we often start from this point instead of doing the full set of manipulations from scratch.</p>
<p>So what is the Bernoulli log PMF? Recall that if <span class="math inline">\(B \sim \text{Bernoulli}(p)\)</span>, its PMF is given by:</p>
<p><span class="math display">\[\P(B=k) = \begin{cases} p &amp; k=1 \\ 1-p &amp; k = 0 \end{cases}\]</span></p>
<p>or more compactly,</p>
<p><span class="math display">\[\P(B = k) = p^k(1-p)^(1-k)\]</span></p>
<p>Taking logarithms, we have</p>
<p><span class="math display">\[\log \P(B = k) = k \log p + (1-k) \log(1-p)\]</span></p>
<p>and hence the negative log-likelihood:</p>
<p><span class="math display">\[-\log \P(B = k) = -k \log p -(1-k) \log(1-p)\]</span></p>
<p>From our working model, we take <span class="math inline">\(p = 1/(1+e^{-\bx^{\top}\bbeta})\)</span> so our joint MLE is given by:</p>
<p><span class="math display">\[\hat{\bbeta} = \argmin_{\bbeta} \sum_{i=1}^n -y_i \log\left(1/(1+e^{-\bx_i^{\top}\bbeta})\right) - (1-y_i)\log\left(1 - 1/(1+e^{-\bx_i^{\top}\bbeta})\right)\]</span></p>
<p>This is still quite hairy, so let’s simplify it. In the first term, we note that <span class="math inline">\(-\log(1/x) = \log x\)</span> to get:</p>
<p><span class="math display">\[\hat{\bbeta} = \argmin_{\bbeta} \sum_{i=1}^n y_i \log\left(1+e^{-\bx_i^{\top}\bbeta}\right) - (1-y_i)\log\left(1 - 1/(1+e^{-\bx_i^{\top}\bbeta})\right)\]</span></p>
<p>For the second term, note that</p>
<p><span class="math display">\[1-\frac{1}{1+e^{-z}} = \frac{1+e^{-z} - 1}{1+e^{-z}} = \frac{e^{-z}}{1+e^{-z}} \implies \log\left(1-\frac{1}{1+e^{-z}}\right) = -z - \log(1 + e^{-z})\]</span></p>
<p>so we get:</p>
<p><span class="math display">\[\begin{align*}
\hat{\bbeta} &amp;=  \argmin_{\bbeta} \sum_{i=1}^n y_i \log\left(1+e^{-\bx_i^{\top}\bbeta}\right) - (1-y_i)\log\left(1 - 1/(1+e^{-\bx_i^{\top}\bbeta})\right) \\
&amp;= \argmin_{\bbeta} \sum_{i=1}^n y_i \log\left(1+e^{-\bx_i^{\top}\bbeta}\right) - (1-y_i)\left[-\bx_i^{\top}\bbeta + \log\left(1+e^{-\bx_i^{\top}\bbeta}\right)\right] \\
&amp;= \argmin_{\bbeta} \sum_{i=1}^n y_i \log\left(1+e^{-\bx_i^{\top}\bbeta}\right) + (1-y_i)\left[\bx_i^{\top}\bbeta + \log\left(1+e^{-\bx_i^{\top}\bbeta}\right)\right] \\
&amp;= \argmin_{\bbeta} \sum_{i=1}^n y_i \log\left(1+e^{-\bx_i^{\top}\bbeta}\right) + \left[\bx_i^{\top}\bbeta + \log\left(1+e^{-\bx_i^{\top}\bbeta}\right)\right] -y_i\left[\bx_i^{\top}\bbeta + \log\left(1+e^{-\bx_i^{\top}\bbeta}\right)\right]\\
&amp;= \argmin_{\bbeta} \sum_{i=1}^n-y_i \bx_i^{\top}\bbeta + \bx_i^{\top}\bbeta + \log\left(1-e^{-\bx_i^{\top}\bbeta}\right)
\end{align*}\]</span></p>
<p>We could work in this form, but it turns out to actually be a bit nicer to ‘invert’ some of the work we did above to get rid of a term. In particular, note</p>
<p><span class="math display">\[z + \log(1-e^{-z}) = \log(e^z) + \log(1-e^{-z}) = \log(e^z + e^{z-z}) = \log(1 + e^z)\]</span></p>
<p>which gives us:</p>
<p><span class="math display">\[\hat{\bbeta} = \argmin_{\bbeta} \sum_{i=1}^n-y_i \bx_i^{\top}\bbeta  + \log\left(1+e^{\bx_i^{\top}\bbeta}\right)\]</span></p>
<p>Wow! Long derivation. And we still can’t actually solve this!</p>
<p>Unlike OLS, where we could obtain a closed-form solution, we <em>have to use</em> an iterative algorithm here. While we could use <em>gradient</em> descent, we can get to an answer far more rapidly if we use more advanced approaches. Since this is not an optimization course, we’ll instead use some software to solve for us:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from the logistic model</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X    <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), <span class="at">ncol=</span>p)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>X <span class="sc">%*%</span> beta))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="at">size=</span><span class="dv">1</span>, <span class="at">prob=</span>P)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Use optimization software</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># See https://cvxr.rbind.io/cvxr_examples/cvxr_logistic-regression/ for details</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(CVXR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'CVXR'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:dplyr':

    id</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:purrr':

    is_vector</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:stats':

    power</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">Variable</span>(p)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>eta  <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="sc">-</span> y <span class="sc">*</span> eta <span class="sc">+</span> <span class="fu">logistic</span>(eta))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>problem   <span class="ot">&lt;-</span> <span class="fu">Problem</span>(<span class="fu">Minimize</span>(objective))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)<span class="sc">$</span><span class="fu">getValue</span>(beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">beta_hat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.9975707</td>
</tr>
<tr class="even">
<td style="text-align: right;">1.6927524</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2.8273155</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.0471099</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.1771164</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Not too bad, if we compare this to <code>R</code>’s built-in logistic regression function, we see our results basically match:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">glm</span>(y <span class="sc">~</span> X <span class="sc">+</span> <span class="dv">0</span>, <span class="at">family=</span>binomial))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = y ~ X + 0, family = binomial)

Coefficients:
   Estimate Std. Error z value Pr(&gt;|z|)    
X1  0.99757    0.16318   6.113 9.76e-10 ***
X2  1.69275    0.19420   8.716  &lt; 2e-16 ***
X3  2.82732    0.27231  10.383  &lt; 2e-16 ***
X4  0.04711    0.13997   0.337    0.736    
X5  0.17712    0.12503   1.417    0.157    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 693.15  on 500  degrees of freedom
Residual deviance: 332.85  on 495  degrees of freedom
AIC: 342.85

Number of Fisher Scoring iterations: 6</code></pre>
</div>
</div>
<p>Good match!</p>
<p>Note that, in the above, we had to use the <code>logistic</code> function built into <code>CVX</code>. If we used <code>log(1+exp(eta))</code>, the solver would not be able to prove that it is convex and would refuse to try to solve. See the <code>CVX</code> documentation for more details.</p>
<p>We can compute accuracy after defining a ‘decision rule’. Here, let’s just round the predicted probability</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>X <span class="sc">%*%</span> beta_hat))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">round</span>(p_hat)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y_hat <span class="sc">==</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.844</code></pre>
</div>
</div>
<p>Now that we have this toolkit built up, we can also easily apply ridge and lasso penalization:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>penalty <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">abs</span>(beta))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>lasso_problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(<span class="fu">Minimize</span>(objective <span class="sc">+</span> <span class="dv">9</span> <span class="sc">*</span> penalty))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>beta_hat_lasso <span class="ot">&lt;-</span> <span class="fu">solve</span>(lasso_problem)<span class="sc">$</span><span class="fu">getValue</span>(beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<tbody>
<tr class="odd">
<td style="text-align: right;">0.6323610</td>
</tr>
<tr class="even">
<td style="text-align: right;">1.1913097</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2.0446974</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.0000000</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.0375919</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We got some sparsity, as we would expect with <span class="math inline">\(\ell_1\)</span> penalization.</p>
</section>
<section id="generalized-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="generalized-linear-models">Generalized Linear Models</h2>
<p>The approach we took here is a special case of a <em>generalized linear model</em>: generalized linear models (GLMs) consist of three parts:</p>
<ul>
<li>A linear model for the ‘core predictor’ <span class="math inline">\(\eta = \bX\bbeta\)</span>;</li>
<li>A sampling distribution for the data <span class="math inline">\(y \sim \text{Dist}\)</span>; and</li>
<li>An ‘adaptor’ function<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, <span class="math inline">\(\mu\)</span>, that maps <span class="math inline">\(\eta \in \R\)</span> to the parameter space of <span class="math inline">\(y\)</span>: specifically, we need <span class="math inline">\(\E[y | \bx] = \mu(\bx^{\top}\bbeta)\)</span>.</li>
</ul>
<p>The adaptor function needs to be smooth (for nice optimization properties) and monotonic (or else the interpretation gets too weird), but we otherwise have some flexibility. As noted above, if we pick <span class="math inline">\(\mu\)</span> to be the normal or Cauchy CDFs, we get alternative ‘Bernoulli Regressions’ (probit and cauchit).</p>
<p>We can generalize the model for <span class="math inline">\(\eta\)</span> by allowing it to be an additive (spline) model or a kernel method. To fit splines, you can use the <code>gam</code> function from the <code>mgcv</code> function.</p>
<p>Finally, we also have choices in the sampling distribution. While normal and Bernoulli are the most common, you will also sometimes see Poisson and Gamma in the wild. For both of these, the mean is a positive value, so we require an adaptor that maps <span class="math inline">\(\R\)</span> to <span class="math inline">\(\R \geq 0\)</span> and we typically take <span class="math inline">\(\mu(z) = e^z\)</span>.</p>
<p>A good exercise is to repeat the above analysis for Poisson regression and compare your result (obtained with <code>CVXR</code>) to the Poisson regression built into <code>R</code>.</p>
</section>
<section id="loss-functions-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions-for-classification">Loss Functions for Classification</h2>
<p>We now have a workable loss function for discriminative classification. Returning to our example from above (changed to <span class="math inline">\(\{0, 1\}\)</span> convention briefly). Let us set <span class="math inline">\(\alpha = y\hat{y}\)</span> and investigate our loss functions as a property of <span class="math inline">\(\alpha\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out=</span><span class="dv">201</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>acc     <span class="ot">&lt;-</span> <span class="cf">function</span>(alpha) <span class="fu">ifelse</span>(alpha <span class="sc">&lt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>lr_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(alpha) <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>alpha))</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(alpha, <span class="fu">acc</span>(alpha), <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="fu">expression</span>(alpha),</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"Loss"</span>, </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Classification Losses"</span>, </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(alpha, <span class="fu">lr_loss</span>(alpha), <span class="at">col=</span><span class="st">"red4"</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red4"</span>), </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"Accuracy"</span>, <span class="st">"Logistic Regression"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes07_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We see here that the logistic loss defines a <em>convex surrogate</em> of the underlying accuracy. Just like we used <span class="math inline">\(\ell_1\)</span> as a tractable approximation for best squares, logistic regression can be considered a tractable approximation for accuracy minimization.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Other useful loss functions can be created using this perspective, perhaps none less important than the <em>hinge loss</em>, which gives rise to a classifier known as the <em>support vector machine</em> (SVM).</p>
</section>
<section id="support-vector-machines" class="level2">
<h2 class="anchored" data-anchor-id="support-vector-machines">Support Vector Machines</h2>
<p>In this section, we go back to <span class="math inline">\(\{\pm 1\}\)</span> convention.</p>
<p>Instead of using a logistic loss, consider a hinge loss of the form</p>
<p><span class="math display">\[H(y, \hat{y}) = (1 - y\hat{y})_+\]</span></p>
<p>When <span class="math inline">\(y = \hat{y}=1\)</span> or <span class="math inline">\(y = \hat{y} = 0\)</span>, this is clearly 0 as we would expect from a good loss. What happens for cases where our prediction is wrong or not ‘full force’, say <span class="math inline">\(\hat{y} = 1/2\)</span>.</p>
<p>Looking ahead, we will use a linear combination of features to create <span class="math inline">\(\hat{y} = \bx^{\top}\bbeta\)</span> yielding:</p>
<p><span class="math display">\[H(\bbeta) = (1 - y \bx^{\top}\bbeta)_+\]</span></p>
<p>If <span class="math inline">\(y=1\)</span>, we get zero loss so long as <span class="math inline">\(\bx^{\top}\bbeta &gt; 1\)</span> and a small loss for <span class="math inline">\(0 &lt; \bx^{\top}\bbeta &lt; 1\)</span>. As <span class="math inline">\(\bx^{\top}\bbeta\)</span> crosses zero and goes negative, the loss grows linearly without bound. (Reverse all of this for the case <span class="math inline">\(y = -1\)</span>.) This is an interesting loss function: we cannot make our loss decrease as our predictions become ‘more right’, but our loss continues to increase as our prediction becomes ‘more wrong’.</p>
<p>Visually, we can draw this on our plot from above:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out=</span><span class="dv">201</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>acc     <span class="ot">&lt;-</span> <span class="cf">function</span>(alpha) <span class="fu">ifelse</span>(alpha <span class="sc">&lt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>lr_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(alpha) <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>alpha))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>hinge_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(alpha) <span class="fu">pmax</span>(<span class="dv">0</span>, <span class="dv">1</span><span class="sc">-</span>alpha)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(alpha, <span class="fu">acc</span>(alpha), <span class="at">type=</span><span class="st">"l"</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="fu">expression</span>(alpha),</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"Loss"</span>, </span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"Classification Losses"</span>, </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>))</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(alpha, <span class="fu">lr_loss</span>(alpha), <span class="at">col=</span><span class="st">"red4"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(alpha, <span class="fu">hinge_loss</span>(alpha), <span class="at">col=</span><span class="st">"green4"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red4"</span>, <span class="st">"green4"</span>), </span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd=</span><span class="dv">2</span>, </span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"Accuracy"</span>, <span class="st">"Logistic Regression"</span>, <span class="st">"Hinge Loss"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes07_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This is also a pretty nice surrogate loss. In this case, let’s just go ‘straight at it’ to construct a classifier:</p>
<p><span class="math display">\[\hat{\bbeta} = \argmin \sum_{i=1}^n (1-y_i \bx_i^{\top}\bbeta)_+\]</span></p>
<p>This classifier is not uniquely defined for many problems (because of the long flat part of the loss), so it is conventional to add a <em>regularization</em> term to the SVM:</p>
<p><span class="math display">\[\hat{\bbeta} = \argmin \sum_{i=1}^n (1-y_i \bx_i^{\top}\bbeta)_+ + \lambda \|\bbeta\|_2^2\]</span></p>
<p>We can solve this directly using CVX as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data from the logistic model and fit an SVM</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>X    <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), <span class="at">ncol=</span>p)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>X <span class="sc">%*%</span> beta))</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="at">size=</span><span class="dv">1</span>, <span class="at">prob=</span>P)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Convert to +/-1 convention</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> y <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(CVXR)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">Variable</span>(p)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>eta  <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">pos</span>(<span class="dv">1</span> <span class="sc">-</span> y <span class="sc">*</span> eta)) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">norm2</span>(beta)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>problem   <span class="ot">&lt;-</span> <span class="fu">Problem</span>(<span class="fu">Minimize</span>(objective))</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)<span class="sc">$</span><span class="fu">getValue</span>(beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the decision rule <span class="math inline">\(\hat{y} = \text{sign}(\bx^{\top}\bbeta)\)</span>, this gives us good accuracy:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">sign</span>(X <span class="sc">%*%</span> beta_hat) <span class="sc">==</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.88</code></pre>
</div>
</div>
<p>This is comparable to what we got for logistic regression even though:</p>
<ol type="i">
<li>the loss function is ‘wrong’ (doesn’t model the DGP)</li>
<li>we did no tuning of the ridge parameter</li>
</ol>
<p>So, all in all, pretty impressive.</p>
<p>SVMs are a remarkably powerful and general classification technology. Before we move past them, we will now take a second perspective focusing on the <em>geometry</em> of SVMs.</p>
</section>
<section id="svm-the-geometric-viewpoint" class="level2">
<h2 class="anchored" data-anchor-id="svm-the-geometric-viewpoint">SVM: The Geometric Viewpoint</h2>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>For historical reasons, this is known as the <em>inverse link</em>, but I prefer to simply think of it as an adaptor and to never use the (forward) link.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This ‘surrogate loss’ perspective was first discussed in: P.L. Barlett, M.I. Jordan, and J.D. McAuliffe. “Convexity, Classification, and Risk Bounds”. <em>Journal of the American Statistical Association</em> <strong>101(473)</strong>, pp.138-156. 2006. DOI:<a href="https://dx.doi.org/10.1198/016214505000000907">10.1198/016214505000000907</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/michael-weylandt\.com\/STA9890\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/michaelweylandt/STA9890/edit/main/notes/notes07.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/michaelweylandt/STA9890/blob/main/notes/notes07.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/michaelweylandt/STA9890/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>