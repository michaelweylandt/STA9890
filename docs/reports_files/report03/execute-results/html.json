{
  "hash": "9261830f640172e832d8e76b5eba7543",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"{{< var course.short >}} Research Report #{{< meta rr_num >}}: {{< meta rr_title >}}\"\nformat:\n  html:\n    code-link: true\nrr_num: \"03\"\nrr_title: \"Sparse Principal Components Analysis\"\nfilters:\n  - list-table\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### Key Dates\n\n- Released to Students: 2025\\-04\\-22\n- **Submission: 2025\\-05\\-09 11:45pm ET on Brightspace**\n\n*Estimated Time to Complete: 9 Hours*\n\n## Research Report #{{< meta rr_num >}}: {{< meta rr_title >}}\n\nIn the final Research Report of this course, you will investigate the\nintersection of *sparsity*, a form of interpretability, with *unsupervised\nlearning*, the branch of ML focused on finding meaningful patterns in data.\n\nClassically, principal components analysis (PCA) is the cornerstone of\ndimension reduction methods. By reducing a large number of features to a smaller\nnumber of linearly independent (uncorrelated) components, PCA is often said\nto increase interpretability. This assumes a degree of \"reification\" - is the PC\nsomething meaningful on its own? \n\nIf the PC captures a true underlying factor, then perhaps we have found\nsomething worth interpreting; but PCA always finds some sort of factorization,\neven when there is no \"true\" underlying factor. In some circumstances, this PC\nbecomes a thing in itself, *e.g.*, IQ, but that cannot always be assumed. If we\ndo not assume or define a meaning to the PC, it can only be understood as a\ncombination of all of the features used to create it, hardly a paragon of\ninterpretability. \n\nSparse PCA takes a different approach: rather than finding an interpretation of\nthe linear combination of all features that captures the most variance, it seeks\na linear combination of only a few features that explain nearly-the-most variance.\nBecause only a few features are used, the resulting PC is more interpretable\n*a priori* than the output of classical PCA. \n\nAs background, you should read the paper by Weylandt and Swiler[^beyondpca] and\nreview the [Supporting Materials](https://zenodo.org/records/10581710) to see how\nSparse PCA can be applied in a scientific setting. For more methodological \nbackground, see the references cited therein, especially the paper by Witten \n*et al.*, as well as ISL ยง6.3.1 and ยง12.2 and SLS ยง8.1-8.2.2. \n\n[^beyondpca]: M. Weylandt and L.P. Swiler. \"Beyond PCA: Additional Dimension\nReduction Techniques to Consider in the Development of Climate Fingerprints\". \n*Journal of Climate* **37(5)**, p.1723-1735. 2024. DOI:\n[10.1175/JCLI-D-23-0267.1](https://doi.org/10.1175/JCLI-D-23-0267.1). \n[Direct Link](https://journals.ametsoc.org/view/journals/clim/37/5/JCLI-D-23-0267.1.pdf)\n\n### Project Skeleton\n\n\nYour report should be divided into three sections, covering the\nfollowing: \n\ni)   Background on PCA and Sparse PCA\nii)  Computation - Implementing the Power Method and the Sparse Power Method\n     for PCA\niii) Implementation and Assessment of Sparse PCA\n\nAt a minimum, these should include the following elements: \n\n- Background on PCA and Sparse PCA\n  - Derivation of PCA from Variance Maximization to Singular Value Decomposition\n  - Proof of Convergence of the Power Method for SVD Computations\n  - Modification of Classical Power Method for Sparsity\n- Computation\n  - Implementation of Classical and Sparsified Power Method\n  - Discussion of Convergence for Power Methods[^convergence]\n  - How might one tune the sparsity level used? \n- Implementation and Assessment\n  - In-Simulation: Construct simulations to compare the accuracy of \n      classical and sparse PCA. Which does better when the \"true\" PCs are dense?\n      When they are sparse? Does this depend on the sample size? How can we\n      measure accuracy of the estimated PCs? \n  - On Real Data: Identify a real data set where PCA can be applied. Apply both\n    classical and sparse PCA and compare the results. Which PCA is better (in\n    whatever sense)? Which PCA is more interepretable? Is there a way to\n    validate your interpretation?\n    \n[^convergence]: Recall that PCs are only defined \"up to sign\". If $\\hat{\\mathbf{u}}$\nis a valid PC, so is $-\\hat{\\mathbf{u}}$. You will need to modify your usual\nconvergence checks to account for this. \n  \n  \n### Additional Background\n\nThe *Power Method* is a classical approach to computing eigenvectors and, by\nextension, singular vectors. For this research report, you will build upon\nthe singular vector variant, so I review the eigenvector variant here. \n\nSuppose $\\mathbf{\\Sigma} \\in \\mathbb{R}^{p \\times p}_{\\succ 0}$ is a strictly positive \ndefinite $p \\times p$ matrix. By the spectral theorem, it has an\neigendecomposition: \n\n$$\\mathbf{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{v}_i\\mathbf{v}_i^{\\top}$$\n\nwhere the eigenvalues $\\{\\lambda_i\\}$ are decreasing and strictly positive: \n($\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p$). If the eigenvalues are\ndistinct (as they are with probability 1 for continuous-valued data observed\nnoisly), then this decomposition is unique up to sign.[^sign]\n\nThe *power method* for computing the leading (highest eigenvalue) eigenvector\nof $\\mathbf{\\Sigma}$ proceeds as follows: \n\n- Initialize: select $\\mathbf{v}^{(0)}$ as a random unit vector\n- Repeat Until Convergence: \n  - Multiply: $\\tilde{\\mathbf{v}}^{(k+1)} = \\mathbf{\\Sigma} \\mathbf{v}^{(k)}$\n  - Normalize: $\\mathbf{v}^{(k+1)} = \\tilde{\\mathbf{v}}^{(k+1)} / \\|\\tilde{\\mathbf{v}}^{(k+1)}\\|_2$\n  - Iterate: $k \\leftarrow k + 1$\n- Return: At convergence, return the eigenvector $\\mathbf{v}^{(k)}$ and the\n  eigenvalue $\\lambda = \\|\\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\|_2$\n\nAdditional eigenvectors can be found by \"deflating\" $\\mathbf{\\Sigma}$ and \napplying the power method to $$\\mathbf{\\Sigma}' = \\mathbf{\\Sigma} - \\lambda \\hat{\\mathbf{v}}\\hat{\\mathbf{v}}^{\\top}.$$\n\nSo long as the eigenvalues of $\\mathbf{\\Sigma}$ are distinct, the power method\nconverges to the leading eigenvector as long as the initial guess is not\nperfectly orthogonal to the true eigenvector. If we pick our initial guess\nrandomly, this is a very reasonable assumption. To see why this is the case,\nnote that: \n\n$$\\begin{align*}\n\\mathbf{v}^{(k)} &\\propto \\mathbf{\\Sigma}^k \\mathbf{v}^{(0)} \\\\\n                 &= \\left(\\sum_{i=1}^p \\lambda_i^k \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\right)\\mathbf{v}^{(0)} \\\\\n                 &= \\sum_{i=1}^p \\lambda_i^k \\langle\\mathbf{v}_i, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_i\n\\end{align*}$$\n\nViewed here, you can see why the power method has its name. As $k \\to \\infty$,\nthe first term in this series becomes much much larger than the others, so we get\n\n$$\\mathbf{v}^{(k)} \\buildrel \\sim \\over \\propto \\lambda_1^k \\langle\\mathbf{v}_1, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_1$$\nAfter normalizing and dropping the scalar terms, we have\n\n$$\\mathbf{v}^{(k)} \\to \\mathbf{v}_1$$ \n\nas desired. This argument also clarifies the two possible failure modes of the\npower method: \n\ni) If the initial guess is unlucky and orthogonal to the true eigenvector, we\n   have $\\langle \\mathbf{v}_1, \\mathbf{v}^{(0)} \\rangle = 0$ so the \n   $\\mathbf{v}_1$ term drops out of the answer and we converge to the next \n   eigenvector (unless we are orthogonal to that one as well).\nii) If the eigenvalues are not distinct, then multiple terms become large\n    and we get a strange 'superposition' of multiple eigenvectors. (Alternatively,\n    there isn't a 'true' right answer, but we get one possible eigenvector.)\n\n[^sign]: Note that we can  substitute $\\mathbf{v}_i \\to -\\mathbf{v}_i$ without changing the result since the minus signs will cancel in $(-\\mathbf{v}_i)(-\\mathbf{v}_i)^{\\top} = \\mathbf{v}_i\\mathbf{v}_i^{\\top}$.\n\n### Potential Topics for Additional Research\n\nWhile the main focus of this report is on *Sparse PCA*, a suitably sparsified\nSVD can be used for a wide variety of multivariate analysis problems. You may\nwish to extend your analysis to other sparse multivariate methods, such as \n*Sparse CCA* or *Sparse PLS*. \n\nAlternatively, a huge number of approaches to Sparse PCA have been proposed, not\nall of which are based on the power method. You may wish to compare the approach\nI prefer (projected power method) with other approaches: be sure to consider \ni) accuracy; ii) interpretability; and iii) computational ease / efficiency \nin your comparison. \n\n### Submission Instructions\n\nSubmit your research report as a PDF of 6-8 pages on Brightspace.[^pdf]\nYour submission should include all *essential* code, *e.g.*, code\nused to fit models or to perform simulations, but may omit\n*incidental* code, *e.g.* code used to create figures or import\ndata. \n\nYou are **required** to implement ML methods by hand; use of 'built-in'\nML functionality is disallowed unless authorized by instructor. You may use\nbuilt-in linear algebra subroutines and are not required to, *e.g.*, implement\nmatrix multiplication from scratch. While you must use your own implementation\nof ML methods, it is smart to compare against existing 'reference' implementations.\n\nYour report should be in the style of a technical \"blog post\", accessible\nto someone who is just getting into Machine Learning but has solid\nbackground in the prerequisite subjects (*e.g.*, you on the day before\nthis class started). Make sure to (rigorously) define and prove all\nresults used. You should also cite relevant authoritative resources, \n*e.g.*, the recommended textbooks, where appropriate. \n\nEach student must submit an individual and independently written report, but you\nare encouraged to work together with your peers to understand methods, design\nsimulations, review relevant literature, *etc.* I strongly recommend having a\nclassmate [\"red-team\"](https://en.wikipedia.org/wiki/Red_team) your report\nbefore submission to find unclear writing, sub-standard figures and tables, \nunconvincing simulations, incorrect code, *etc.* Per the course's [Academic\nIntegrity Policy](../resources.html#aipolicy), you must explicitly acknowledge\nall collaborators in your submission.\n\n[^pdf]: If you wish to use an alternate submission format to allow\ninteractive and/or animated elements, please seek pre-approval from\nthe instructor. \n\n### Grading \n\nYour submission will be evaluated *holistically* and graded out of 100\npoints. The following rubric will guide this holistic evaluation, but\nthe instructor may deviate as necessary to accurately grade the final\nsubmission. \n\n::: {.list-table width=\"1,1,1,1,1,1\" aligns=\"c,c,c,c,c,c\" header-rows=1 .hover}\n\n * - Report Element\n   - Excellent. <br> \"A-\" to \"A\" (`90% to 100%`)\n   - Great. <br> \"B-\" to \"B+\" (`80% to 89%`)\n   - Average. <br> \"C\" to \"C+\" (`73% to 79%`)\n   - Poor. <br> \"D\" to \"C-\" (`60% to 72%`)\n   - Failure. <br> \"F\" (`0% to 59%`)\n   \n * - Presentation (15%)\n   - Report has excellent formatting, with particularly effective \n     tables and figures. Tables and Figures are \"publication-quality\"\n     and clearly and succinctly support claims made in the body text.\n   - Report has strong formatting; tables and figures make their\n     intended points, but do not do so optimally. \n   - Formatting is average; tables and figures do not clearly support\n     arguments made in the text and/or are not \"publication quality\"\n   - Poor formatting distracts from substance of report. Tables and\n     Figures exhibit significant deficiencies in formatting.  \n   - Formatting prohibits or significantly impairs reader understanding.\n \n * - Project Skeleton (15%)\n   - Report includes all required elements and goes significantly deeper\n     than required by the project skeleton in a manner that provides \n     additional insights. \n     \n     Mathematical definitions and proofs are clearly stated. \n     \n   - Report includes all required elements and dives deeper into the\n     topic, but does not generate additional insights. (*E.g.*,\n     additional simulations showing the same phenomena)\n     \n     Mathematical definitions and proofs are correctly stated, but \n     clarity could be improved. \n     \n   - Report includes all required elements. \n   \n     Mathematical definitions and proofs are essentially correct,\n     but difficult to understand and/or contain trivial errors.\n   \n   - Report does not include all required elements, but is still\n     able to capture key points. \n     \n     Mathematical definitions and proofs contain significant, but\n     non-fatal, errors. \n   - Report fails to adequately address the topic. \n   \n     Mathematical definitions and proofs are fundamentally incorrect. \n \n * - Algorithms and Computational Efficiency (20%)\n   - Report implements all methods efficiently with high-quality,\n     well-formatted and performant code. \n   - Report implements all methods efficiently with acceptable\n     code quality. \n   - Report implements all methods, but does not do so efficiently\n     and/or with substandard code. \n   - Report uses built-in methods instead of implementing methods\n     from scratch\n   - Code does not appear to run properly / insufficient code submitted. \n \n * - Methodological Aspects and Simulation Design(20%)\n   - Methods are accurately and correctly implemented in a robust / bullet-proof\n     manner, designed to responsibly check for and address possible modes of\n     failure. Simulations clearly and efficiently support all claims. \n   - Methods are accurately and correctly implemented, but are not robust to\n     failure. Simulations clearly and efficiently support all claims. \n   - Methods are implemented with minor harmless errors and/or poor validation.\n     Simulations do not fully support claims. \n   - Methods are implemented with significant errors leading to incorrect results. \n   \n     Simulations do not give sufficient support for key claims.\n   - Methods are not correctly implemented. Simulations do not support claims.\n \n * - Communication and Writing (20%)\n   - Report exhibits excellent written communication, making all points\n     exceptionally clearly and at the level of a quality academic\n     publication. Code, results, and text are skillfully woven together.\n   - Report exhibits great written communication, all points are made\n     clearly without notable grammatical errors. Code, results, and\n     text are neatly tied together. \n   - Report exhibits solid written communication, key points are made \n     understandably and any grammatical errors do not impair\n     understanding. Code, results, and text could be better integrated, \n     but it is clear which elements relate. \n   - Written communication is below standard: points are not always\n     understandable and/or grammatical errors actively distract from\n     content. Code, results, and text are not actively integrated, but\n     are generally located 'near' each other in a semi-systematic fashion.\n   - Written communication is far below standard, possibly bordering\n     on unintelligible. Large blocks of code are shown without any\n     relevant results or text. \n \n * - Contextualization (10%)\n   - Report draws on relevant, modern research literature to give context\n     to its findings and to broaden its scope. \n   - Report draws on standard textbooks and/or classical research literature\n     to give context to its findings and to broaden its scope. \n   - Report cites textbooks to give context, but does not take a 'big picture' \n     view. \n   - Report gives appropriate context, but lacks citations. \n   - Report gives limited, insufficient context.\n\n:::\n\n------------------------------------------------------------------------\n\nThis work ยฉ2025 by [Michael Weylandt](https://michael-weylandt.com) is licensed\nunder a [Creative Commons BY-NC-SA \n4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en) license.\n![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png){width=\"10%\"}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}