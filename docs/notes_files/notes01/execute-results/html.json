{
  "hash": "a07cdf5fa6ec2ec8167cd611e726707a",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"1\"\nauthor: \"Michael Weylandt\"\ntopic: \"Course Overview & Introduction to ML\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n::: {.cell}\n\n:::\n\n\n\n## A Taxonomy of Machine Learning\n\nWhere classical statistics focuses on learning information about a\npopulation from a (representative) sample,  Machine Learning \nfocuses on out-of-sample prediction accuracy. \n\nFor example, given a large set of medical records, the natural\ninstinct of a statistician is to find the indicators of cancer in\norder assess them via some sort of follow-on genetic study, while a\nML practitioner will typically start by building a predictive\nalgorithm to predict who will be diagnosed with cancer. The\nstatistician will perform calculations with $p$-values, test\nstatistics, and the like to make sure that any discovered\nrelationship is accurate, while the ML practitioner will verify\nthe performance by finding a new (hopefully similar) set of medical\nrecords to test algorithm performance. \n\nClearly, this difference is more one of style than substance: the\nstatistician might see what features are important in the ML model to\ndecide what to investigate, while the ML modeler will use statistical\ntools to make sure the model is finding something real and not just\nfitting to noise. In this course, the distinction may be even\nblurrier as our focus is *statistical machine learning* - that little\nniche right on the boundary between the two fields. \n\nIn brief, \"statistics vs ML\" is a bit of a meaningless distinction as\nboth fields draw heavily from each other. I tend to say one is *doing\nstatistics* whenever the end-goal is to *better understand* something\nabout the real world (*ie.*, the end product is *knowledge*), while\none is *doing ML* whenever one is building a system to be used in an\nautomated fashion (*ie.*, the end product is *software*), but\ndefinitions vary.[^culture]\n\n[^culture]: There are some cultural differences that come from ML's\nCS history vs Statistics' \"Science-Support\" history: notably, ML\nfolks think of (binary) classification while statisticians think of\nregression as the first task to teach.\n\nIn this course we will use the following taxonomy borrowed from the\nML literature: \n\n- Supervised Learning: Tasks with a well-defined target variable (output)\nthat we aim to predict\n\n  Examples: \n  - Given an image of a car running a red-light, read its license plate.\n  - Given attendance records, predict which students will not pass a course.\n  - Given a cancer patient's medical records, predict whether a certain drug\n    will have a beneficial impact on their health outcomes. \n  \n- Unsupervised Learning: Given a whole set of variables, none of which is\n  considered an output, learn useful underling structure. \n\n  Examples: \n  - Given a set of climate simulations, identify the general trends of a \n    climate intervention.\n  - Given a set of students' class schedules, group them into sets. (You might\n    imagine these sets correspond to majors, but without that sort of \"label\" \n    (output variable) this is only speculative, so we're unsupervised)\n  - Given a social network, identify the most influential users.\n\nThere are other types of learning tasks: *e.g.* semi-supervised, online,\nreinforcement, but the Supervised/Unsupervised distinction is the main one\nwe will use in this course. \n\nWithin Supervised Learning, we can further subdivide into two major categories:\n\n- Regression Problems: problems where the response (label) is a\n  real-valued number\n- Classification Problems: problems where the response (label) is a\n  category label.\n  \n  - Binary classification: there are only two categories\n  - Multi-way or Multinomial classification: multiple categories\n  \nLinear Regression, which we will study more below, is the canonical\nexample of a *regression* tool for *supervised* learning.\n\nAt this point, you can already foresee one of the (many) terminology\ninconsistencies will will encounter in this course: _logistic regression_\nis a tool for _classification_, not regression. As modern ML is the intersection\nof many distinct intellectual traditions, the terminology is rarely \nconsistent.[^uk]\n\n[^uk]: You may recall the famous quip about the US and the UK: \"Two countries,\nseparated by a common language.\"\n\n## Test and Training Error\n\nAs we think about measuring a model's predictive performance, it becomes\nincreasingly important to distinguish between _in-sample_ and _out-of-sample_\nperformance, also called training (in-sample) and testing (out-of-sample)\nperformance. \n\nIn previous courses, you likely have assessed model fit by seeing how well\nyour model fits the data it was trained on: statistics like $R^2$, SSE, SSR in\nregression or $F$-tests for model comparison do just this. As you have used them,\nthey are primarily useful for comparison of _similar_ models, *e.g.*, OLS with\ndifferent numbers of predictor variables. But it's worth reflecting on this\nprocess a bit more: didn't the model with more predictors always fit the data\nbetter? If so, why don't we always just include all of our predictors? \n\nOf course you know, the answer is that we want to avoid overfitting. Just\nbecause a model fit the data a bit better doesn't mean it is actually better. \nIf you need 1,000 variables to get a 0.1% reduction in MSE, do you really\nbelieve those features are doing much? No! \n\nYou likely have a sense that a feature needs to \"earn its keep\" to be worth\nincluding in a model. Statisticians have formalized this idea very well\nin some contexts: quantities like _degrees of freedom_ or _adjusted_ $R^2$\nattemps to measure whether a variable provides a _statistically significant_\nimprovement in performance. These calculations typically rely on subtle\ncalculations involving nice properties of the multivariate normal distribution\nand ordinary least squares, or things that can be (asymptotically) considered\nessentially equivalent. \n\nIn this class, we don't want to make those sorts of strong distributional and\nmodeling assumptions. So what can we do instead? Well, if we want to see if \nModel A predicts more accurately than Model B on new data, why don't we just\ndo that? Let's get some new data and compare the MSEs of Model A and Model B:\nwhichever one does better is the one that does better.[^sig]\n\nThis is a pretty obvious idea, so it's worth asking why it's not the baseline\nand why statisticians bothered with all the degrees of freedom business to start\nwith. As always, you have to know your history: statistics comes from a lineage\nof scientific experimentation where data is limited and often quite expensive to\nget. If you are running a medical trial, you can't just toss a few hundred extra\nparticipants in - this costs money! If you are doing an agricultural experiment,\nit may take several years to see whether a new seed type actually has higher\nyield than the previous version. It's also not clear how one should separate\ndata into training and test sets: if you are studying, *e.g.*, friendship\ndynamics on Facebook, you don't have an (obvious) \"second Facebook\" that you can\nuse to assess model accuracy. \n\nBy contrast, CS-tradition Machine Learning comes from a world of \"internet-scale\"\nwhere data is plentiful, cheap, and is continuously being collected.[^anec] Not\nall problems fall in this regime but, as we will see, enough do that it's worth \nthinking about what we should do in this scenario. Excitingly, if we don't\ndemand a full and exhaustive mathematical characterization of a method before we\nactually apply it, we can begin to explore much more complex and interesting\nmodels.\n\n::: {.callout-tip title=\"Advice\"}\n\nA good rule of thumb for applied statistical and data science work: begin by\nasking yourself what you would do if you had access to the whole population\n(or an infinitely large sample) and then adapt that answer to the limited data\nyou actually have. You always want to make sure you are asking the right \nquestion, even if you are only able to give an approximate finite-data answer,\nrather than giving an 'optimal' answer to a question you don't actually care\nabout.\n\n:::\n\nSo, for the first two units of this course, we will put this idea front and\ncenter: we will fit our models to a _training set_ and then see how well they\nperform on a _test set_. Our goal is to not to find find models which perform\nwell on the test set *per se*: we really want to find models that perform well\non the all future data, not just one test set. But this training/test split will\ncertainly get us going in the right direction. \n\nLooking ahead, let's note some of the key questions we will come back to again\nand again: \n\n- If we don't have an explicit test set, where can we get one? Can we 'fake it\n  to make it'? \n- What types of models have small \"test-training\" gap, *i.e.* do about as well\n  on the test and training sets, and what models have a large gap? \n\n[^anec]: Anecdotally, a Google Data Scientist once mentioned to me that they\nrarely bother doing $p$-value or significance calculations. At the scale of\nGoogle's A/B testing on hundreds of billions of ad impressions, _everything_\nis statistically significant. \n\n[^sig]: You might still worry about the randomness of this comparison process:\nif we had a slightly different sample of our new data, would the better model\nstill look better? You aren't wrong to worry about this, but we have at a minimum\nmade the problem much easier: now we are just comparing the means of two error\ndistributions - classic $t$-test stuff - as opposed to comparing two (potentially\ncomplex) models.\n\n\n## Generalization and Model Complexity \n\nSo far, we have two useful concepts:\n\n- Training Error\n- Test Error\n\nwith a third we can set up a trivial, but surprisingly useful, inequality: \n\n$$\\text{Test Error} = \\text{Training Error} + \\text{Generalization Gap}$$\n\nHere, the \"Generalization Gap\" is defined as the difference between the training\nerror and the test error.[^testerror] Essentially, the Generalization Gap measures\nthe \"optimism\" bias obtained by measuring accuracy on the original training data.\nIf the Generalization Gap is large, the model will look much better on the \ntraining data then when we actually go to put it into practice. Conversely, if\nthe Generalization Gap is small, the performance we estimate from the training\ndata will continue when we deploy our model. \n\nThis is a all a bit circular, but it lets us break our overarching goal \n(small test error) into two parts: \n\n- We want a model with small training error; and\n- We want a model with small generalization gap. \n\nWe can only be confident that we'll have a small test error when these *both* \nof these are true. Again - and just to be clear - having a small training error\n*is important*, but it is only *necessary* and not *sufficient* to have a small\ntest error. \n\nWe can simply observe training error, so much of our theoretical analysis\nfocuses on understanding the generalization gap. For now, let's think about\nthe generalization gap of plain linear regression (OLS). It is not hard to show\n(and we might show in class next week) that, if the OLS model is true, the\nexpected training MSE is: \n\n$$\\mathbb{E}[\\text{Training MSE}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n(y_i - \\sum_{j=1}^p x_{ij}\\hat{\\beta}_j)^2\\right] = \\frac{\\sigma^2(n-p)}{n} = \\sigma^2\\left(1-\\frac{p}{n}\\right)$$\n\nHere $\\sigma^2$ is the 'noise variance' of the OLS model. (We will review the OLS\nmodel in much more detail next week.) \n\nThis is somewhat remarkable: if we knew the exact true model $\\beta_*$, our MSE\nwould be $\\sigma^2$, but our training MSE is less than that. How can we do better\nthan the optimal and exactly correct model? **Overfitting** - our OLS fits our\ntraining data a bit 'too well' and it manages to capture the true signal *and* \nthe noise. Whatever noise is in the data doesn't carry into the test set, so\nwe get a bit of overfitting. \n\nEven a simple model like OLS is vulnerable to a bit of overfitting. It isn't too\nbig in this context and our formula above actually lets us see how it behaves: \n\n- As $n\\to\\infty$, the amount of overfitting goes down. This is of course the\n  behavior we want: as we get more data, we should stop fitting noise and only\n  fit signal.\n- As $p \\to n$, the amount of overfitting goes up. That is, as we add more \n  features (covariates) to our regression, we expect more over fitting. When\n  we supply OLS with more 'degrees of freedom', some of those wind up fitting\n  noise. \n  \nThe $n\\to\\infty$ behavior shouldn't surprise you: the theme of \"more data yields\nbetter estimation and smaller error\" is ubiquitous in statistics. The behavior\nas $p$ increases may not be something you have seen before. Later in this course,\nwe will actually ask what happens if $p > n$. Clearly, our formula from above \ncan't hold as it predicts _negative_ MSE! But we'll get to that later...\n\nAs $p$ gets larger, OLS is more prone to overfitting. It turns out that this is\nnot a special property of OLS - basically all methods will have this property\nto one degree or another. While there are many ways to justify this, perhaps\nthe simplest is a story about \"complexity\": with more features, and hence more\ncoefficients, OLS becomes a more complex model and more able to fit both the \nsignal and the noise in the training data. Clearly, complexity is not necessarily\nbad - we want to be able to capture all of the signal in our data - but it is\ndangerous. \n\nThis _complexity_ story is one we will follow through the rest of this course. \nA more _complex_ model is one which is able to fit its training data easily. \nMathematically, we actually measure complexity by seeing how well a model fits\npure noise: if it doesn't fit it well at all (because there is no signal!), \nwe can usually assume it won't overfit on signal + noise. But if it fits\npure noise perfectly, it has _by definition_ overfit the training data. \n\nI like to think of complexity as \"superstitious\" or \"gullibility\": the model\nwill believe (fit to) anything we tell it (training data), whether it is true\n(signal) or not (noise). \n\nWe don't want a model that is too complex, but we also don't want a model that\nis too simple. If it can't fit signal, it is essentially useless. In our metaphor,\nan overly simple (low complexity) model is like a person who simply doesn't believe\nanything at all: they are never tricked, but they also can't really understand\nthe world. \n\nLet us now take a quick detour into a flexible _family_ of models and see how\nperformance relates to the complexity story. \n\n[^testerror]: I'm being a bit sloppy here. This is more precisely the \n*population test error*, not the *test set test error*. \n\n\n## Nearest Neighbor Methods\n\nLet's now see how complexity plays out for a very simple classifier, \n$K$-Nearest Neighbors (KNN). KNN formalizes the intuition of \n\"similar inputs -> similar outputs.\" KNN looks at the $K$ most similar points\nin its training data  (\"nearest neighbors\" if you were to plot the data) and\ntakes the average label to make its prediction.[^details]\n\n[^details]: There are details here about how we measure similarity, but for\nnow we will restrict our attention to simple Euclidean distance. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(class) # Provides a KNN function for classification\nargs(knn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) \nNULL\n```\n\n\n:::\n:::\n\n\n\nYou can see here that KNN requires access to the full training set at\nprediction time: this is different than something like OLS where we reduce our\ndata to a set of regression coefficients (parameters).[^np]\n\n[^np]: KNN is a *non-parameteric* method, but not all *non-parametric* methods\nrequire access to the training data at test time. We'll cover some of those\nlater in the course.\n\nWe'll also need some data to play with. For now, we'll use synthetic data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Make two interleaving half-circles\n#'\n#' @param n_samples Number of points (will be divided equally among the circles)\n#' @param shuffle Whether to randomize the sequence\n#' @param noise Standard deviation of Gaussian noise applied to point positions\n#'\n#' @description Imitation of the Python \\code{sklearn.datasets.make_moons} function.\n#' @return a \\code{list} containining \\code{samples}, a matrix of points, and \\code{labels}, which identifies the circle from which each point came.\n#' @export\nmake_moons <- function(n_samples=100, shuffle=TRUE, noise=0.25) {\n  n_samples_out = trunc(n_samples / 2)\n  n_samples_in = n_samples - n_samples_out\n  \n  points <- matrix( c(\n    cos(seq(from=0, to=pi, length.out=n_samples_out)),  # Outer circle x\n    1 - cos(seq(from=0, to=pi, length.out=n_samples_in)), # Inner circle x\n    sin(seq(from=0, to=pi, length.out=n_samples_out)), # Outer circle y\n    1 - sin(seq(from=0, to=pi, length.out=n_samples_in)) - 0.5 # Inner circle y \n  ), ncol=2) \n  \n  if (!is.na(noise)) points <- points + rnorm(length(points), sd=noise)\n  \n  labels <- c(rep(1, n_samples_out), rep(2, n_samples_in))\n  \n  if (!shuffle) {\n    list(\n      samples=points, \n      labels=labels\n    )\n  } else {\n    order <- sample(x = n_samples, size = n_samples, replace = F)\n    list(\n      samples=points[order,],\n      labels=as.factor(ifelse(labels[order] == 1, \"A\", \"B\"))\n    )\n  }\n}\n```\n:::\n\n\n\nThis function comes from the `clusteringdatasets` R package, but \nthe underlying idea comes from a function in `sklearn`, a popular Python ML \nlibrary.\n\nLet's take a look at this sort of data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTRAINING_DATA <- make_moons()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyverse)\ndata.frame(TRAINING_DATA$samples, \n           labels=TRAINING_DATA$labels) |>\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=3)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nWe can make this a bit more attractive:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMY_THEME <- theme_bw(base_size=20) + theme(legend.position=\"bottom\")\ntheme_set(MY_THEME)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %>%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=2) + \n  ggtitle(\"Sample Realization of the Moons Dataset\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nMuch better!\n\nLet's try making a simple prediction at the point (0, 0):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn(TRAINING_DATA$samples, \n    cl=TRAINING_DATA$labels, \n    test=data.frame(X1=0, X2=0), k=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] B\nLevels: A B\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nDoes this match what you expect from the plot above? Why or why not?\nThe following image might help: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggforce)\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %>%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point() + \n  ggtitle(\"Sample Realization of the Moons Dataset\") + \n  geom_circle(aes(x0=0, y0=0, r=0.2), linetype=2, color=\"red4\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nWhy is `R` returning a `factor` response here? What does that tell us about\nthe type of ML we are doing? \n\n:::\n\nWe can also visualize the output of KNN at every point in space:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize_knn_boundaries <- function(training_data, k=NULL){\n    xrng <- c(min(training_data$samples[,1]), max(training_data$samples[,1]))\n    yrng <- c(min(training_data$samples[,2]), max(training_data$samples[,2]))\n    \n    xtest <- seq(xrng[1], xrng[2], length.out=101)\n    ytest <- seq(yrng[1], yrng[2], length.out=101)\n    \n    test_grid <- expand.grid(xtest, ytest)\n    colnames(test_grid) <- c(\"X1\", \"X2\")\n    \n    pred_labels = knn(training_data$samples, \n                      cl=training_data$labels, \n                      test_grid, \n                      k=k)\n    \n  ggplot() + \n  geom_point(data=data.frame(TRAINING_DATA$samples, \n                             labels=TRAINING_DATA$labels), \n             aes(x=X1, y=X2, color=labels), \n             size=3) + \n  geom_point(data=data.frame(test_grid, pred_labels=pred_labels), \n             aes(x=X1, y=X2, color=pred_labels), \n             size=0.5) + \n  ggtitle(paste0(\"KNN Prediction Boundaries with K=\", k))\n}\n\nvisualize_knn_boundaries(TRAINING_DATA, k=1)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nIf we raise $K$, we get smoother boundaries: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize_knn_boundaries(TRAINING_DATA, k=5)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nAnd if we go all the way to $K$ near to the size of the training data, \nwe get very boring boundaries indeed:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize_knn_boundaries(TRAINING_DATA, k=NROW(TRAINING_DATA$samples)-1)\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nWhat does this tell us about the _complexity_ of KNN as a function of $K$?\n\n:::\n\nIn the terminology we introduced above, we see that increasing $K$ \ndecreases model complexity (wiggliness). \n\nLet's now see how training error differs as we change $K$: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTEST_DATA <- make_moons()\n\nTRAINING_ERRORS <- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train <- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TRAINING_DATA$samples, k=k)\n    true_labels_train <- TRAINING_DATA$labels\n    err <- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the training (in-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TRAINING_ERRORS <- rbind(TRAINING_ERRORS, data.frame(k=k, training_error=err))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAt k = 1, the training (in-sample) error of KNN is 0%\nAt k = 2, the training (in-sample) error of KNN is 9%\nAt k = 3, the training (in-sample) error of KNN is 6%\nAt k = 4, the training (in-sample) error of KNN is 4%\nAt k = 5, the training (in-sample) error of KNN is 5%\nAt k = 6, the training (in-sample) error of KNN is 9%\nAt k = 7, the training (in-sample) error of KNN is 5%\nAt k = 8, the training (in-sample) error of KNN is 6%\nAt k = 9, the training (in-sample) error of KNN is 7%\nAt k = 10, the training (in-sample) error of KNN is 4%\nAt k = 11, the training (in-sample) error of KNN is 6%\nAt k = 12, the training (in-sample) error of KNN is 5%\nAt k = 13, the training (in-sample) error of KNN is 6%\nAt k = 14, the training (in-sample) error of KNN is 6%\nAt k = 15, the training (in-sample) error of KNN is 7%\nAt k = 16, the training (in-sample) error of KNN is 9%\nAt k = 17, the training (in-sample) error of KNN is 8%\nAt k = 18, the training (in-sample) error of KNN is 8%\nAt k = 19, the training (in-sample) error of KNN is 9%\nAt k = 20, the training (in-sample) error of KNN is 8%\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(TRAINING_ERRORS, aes(x=k, y=training_error)) + \n    geom_point() + \n    ggtitle(\"Training (In-Sample) Error of KNN\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nCompare this to the test error: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTESTING_ERRORS <- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train <- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TEST_DATA$samples, k=k)\n    true_labels_train <- TEST_DATA$labels\n    err <- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the test (out-of-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TESTING_ERRORS <- rbind(TESTING_ERRORS, data.frame(k=k, test_error=err))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAt k = 1, the test (out-of-sample) error of KNN is 9%\nAt k = 2, the test (out-of-sample) error of KNN is 10%\nAt k = 3, the test (out-of-sample) error of KNN is 8%\nAt k = 4, the test (out-of-sample) error of KNN is 6%\nAt k = 5, the test (out-of-sample) error of KNN is 8%\nAt k = 6, the test (out-of-sample) error of KNN is 7%\nAt k = 7, the test (out-of-sample) error of KNN is 7%\nAt k = 8, the test (out-of-sample) error of KNN is 6%\nAt k = 9, the test (out-of-sample) error of KNN is 5%\nAt k = 10, the test (out-of-sample) error of KNN is 5%\nAt k = 11, the test (out-of-sample) error of KNN is 5%\nAt k = 12, the test (out-of-sample) error of KNN is 5%\nAt k = 13, the test (out-of-sample) error of KNN is 5%\nAt k = 14, the test (out-of-sample) error of KNN is 5%\nAt k = 15, the test (out-of-sample) error of KNN is 5%\nAt k = 16, the test (out-of-sample) error of KNN is 6%\nAt k = 17, the test (out-of-sample) error of KNN is 5%\nAt k = 18, the test (out-of-sample) error of KNN is 5%\nAt k = 19, the test (out-of-sample) error of KNN is 5%\nAt k = 20, the test (out-of-sample) error of KNN is 6%\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(TESTING_ERRORS, aes(x=k, y=test_error)) + \n    geom_point() + \n    ggtitle(\"Test (Out-of-Sample) Error of KNN\")\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nThe difference between the two is clearer if we put them on the same figure:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nERRS <- inner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |>\n        pivot_longer(-k) |>\n        rename(Error=value, Type=name)\nggplot(ERRS, aes(x=k, y=Error, color=Type)) + \n  geom_point() + geom_line()\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nWe notice a few things here: \n\n1) Training Error increases in $K$[^rev], with 0 training error \n   at $K=1$. (Why?)\n2) Test Error is basically always higher than test error\n3) The best training error does not have the best test error\n\n[^rev]: It is easier to understand this 'in reverse': as \n$K \\downarrow 1$, training error decreases to 0, so as \n$K \\uparrow n$, training error increases. \n\nWe can also look at the gap between training and test error: this is called\n`generalization error` or `optimism`: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |>\n    mutate(optimism=test_error - training_error) |>\n    ggplot(aes(x=k, y=optimism)) + \n    geom_point() + \n    geom_line()\n```\n\n::: {.cell-output-display}\n![](notes01_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nConsider the following questions:\n\n- What is the relationship between optimism and model complexity\n- What is the best value of $K$ for this data set?\n- How should we pick the best value of $K$?\n- How might that change if we increase the number of training samples?\n- How might that change if we increase the number of test samples? \n\n:::\n\n## Model Complexity (Redux)\n\nLet us understand this using the tools of complexity we discussed above. \n\n- For large $K$, the model became quite simple, barely fit the training data, but\n  did just as well on the training data as the test data (poorly on both). This\n  is indicative of a *small generalization gap* and a *low complexity* model. \n- For python $K$, the model became rather complex, fit the training data well, \n  but didn't perform as well as we would like on the test data. This is indicative\n  of a *large generalization gap* and a *high complexity* model. \n  \nIn this case, we could see the complexity visually by looking at the decision\nboundaries (the lines separating predictions of class \"A\" from class \"B\"). This\nisn't universally true[^olscmplx], but the intuition of \"high complexity = more\nwiggles\" is usually pretty good.\n\nThe classical story of ML and complexity is given by something like this:[^fig]\n\n![](https://mlstory.org/assets/gen-ushaped.svg)\n\nFor now, you can interpret \"risk\" as \"test error\" and\n\"empirical risk\" as \"training error\". \n\n::: {.callout-caution title=\"A more complex story of complexity\" collapse=\"true\"}\n\nRecent research has suggested that the story is not\nquite so simple. Some methods exhibit a \"single descent\"\ncurve\n\n![](https://mlstory.org/assets/gen-single.svg)\n\nand some even have a \"double descent\" curve:\n\n![](https://mlstory.org/assets/gen-double.svg)\n\nThe story of these curves is still an active topic\nof research, but it's pretty clear that _very large_ \nand _very deep_ neural networks exhibit something\nlike a double descent curve. My own guess is that we're\nnot quite measuring 'complexity' correctly for these\nincredibly complex models and that the classical story \nholds if we go measure complexity in the right way, but\nthis is far from a universally held belief. \n\n:::\n\nSo how do we measure complexity? For OLS, it seems\nto be proportional to $p$, while for KNN it seems to\nbe inverse to $K$. In this class, we won't really \nfocus too much on the actual measurements. For most of\nthe methods we study, it is usually pretty clear what\ndrives complexity up or down, even if we can't quite\nput a number on it. \n\n### Stability and Generalization\n\nIt turns out there is a deep connection between\ngeneralization and a suitable notion of \"stability\". \nA model is said to be relatively stable if changes\nto an input point do not change the output predictions\nsignificantly.[^be]\n\nAt an intuitive level, this makes sense: if the model\nis super sensitive to individual inputs, it must be\nvery flexible and hence quite complex. (A very simple\nmodel cannot be sensitive to all of its inputs.) \n\nWe can apply this idea to understand some rules-of-thumb\nand informal practices you might have seen in previous\nstatistics courses: \n\n- Regression leverage: [Leverage](https://en.wikipedia.org/wiki/Leverage_(statistics))\n  measures how much a single data point can change \n  regression coefficients. This is stability!\n  \n- Removing outliers: we often define outliers as\n  observations which have a major (and assumed\n  corrupting) influence on our inferences. By removing\n  outliers, we guarantee that the resulting inference\n  is not too sensitive to any of the remaining data\n  points. Here, the 'pre-step' of outlier removal\n  increases stability (by changing sensitivity of those\n  observations to zero) and hopefully makes our\n  inferences more accurate (better generalization)\n  \n- Use of robust statistics, *e.g.* medians instead of\n  means. These explicitly control the stability of\n  our process.\n\nPerhaps most importantly: this justifies why the \nlarge $n$ (big sample) limit seems to avoid overfitting.\nIf our model is fit to many distinct data points, it\ncan't be too sensitive to any of them. At least, that's\nthe hope...\n\nThe sort of statistical models you have seen to date --\nso-called *parametric* models -- have a 'stabilizing' \neffect. By reducing lots of data to only a few\nparameters, those parameters (and hence the model\noutput) can't depend too much on any individual\ninput point.[^thirty_rule] This 'bottleneck' in the\nparameter space seems to improve performance. \n\nOther methods, like $1$-Nearest-Neighbor, become\nincreasingly more complex as we get more data and do\nnot benefit from this sort of 'bottleneck' effect. \n\nAt this point, you might think that stability and\nvariance are closely related concepts - you are not \nwrong and we will explore the connection in more \ndetail next week. \n\n[^be]: For details, see [O. Bousquet and A. Elisseeff, \n*Stability and Generalization* in **Journal of Machine\nLearning Research 2**,\npp.499-526.](https://jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf)\n\n[^thirty_rule]: Some regression textbooks advocate a\nrule that you have 30 data points for every variable\nin your model. This is essentially guaranteeing that\nthe $p/n$ ratio that controls the generalization of OLS\n(see above!) stays quite small.\n\n\n## Key Terms and Concepts\n\n- Supervised vs Unsupervised\n- Regression vs Classification\n- Training Error\n- Test Error\n- Generalization Gap\n- Complexity\n- Overfitting\n- Stability\n- $K$-Nearest Neighbors\n- Decision Boundaries\n\n[^fig]: Figures from HR Chapter 6, *Generalization*.\n\n[^olscmplx]: In OLS, the complexity doesn't make the model 'wigglier' in a normal\nsense - it's still linear after all - but you can think of it as the additional\ncomplexity of a 3D 'map' (*i.e.*, a to-scale model) vs a standard 2D map. \n",
    "supporting": [
      "notes01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}