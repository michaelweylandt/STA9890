[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "",
    "text": "In lieu of a final exam, STA 9890 has a prediction competition worth approximately one-third of your final grade. Details about this prediction competition will be posted here."
  },
  {
    "objectID": "reports/report02.html",
    "href": "reports/report02.html",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "Released to Students: 2025-03-11\nSubmission: 2025-04-18 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report02.html#research-report",
    "href": "reports/report02.html#research-report",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "text": "Research Report #02: Ensemble Learning Techniques for Fair Classification\nIn this Research Report, you will explore concepts of Machine Learning Fairness and see how ensemble learning techniques can be used to create a “fair” ensemble from “unfair” base learners.1 Along the way, you will\n\nUse a black-box optimizer to implement a form of regularized logistic regression\nDesign and implement a custom ensemble learning strategy\nImplement and Assess Various ML Fairness Metrics\n\nProject Skeleton\nYour report should be divided into three sections, covering the following:\n\nBackground on ML Fairness\nComputation - Implementing Regularized Logistic Regression with CVXR or cvxpy\n\nImplementation and Assessment of the FairStacks Ensembling Method\n\nAt a minimum, these should include the following elements:\n\nBackground on ML Fairness\n\nSocial background, including two examples of bias in automated decision systems\nOverview of definitions used to measure ML Fairness\nIdentification and import of a real data set on which we can evaluate the proposed methodology. Be sure to describe both the “raw” data and the fairness metric you are interested in. If there is relevant social context, be sure to describe that as well.\n\n\nComputation\n\nIntroduction to the CVX optimization framework\nImplementation of the following classification methods as base learners to be used in construction of the FairStacks ensemble:\n\nNaive Bayes (no CVX)\nLinear Discriminant Analysis (no CVX)\nSupport Vector Machines (use CVX)\nLogistic Regression (use CVX): plain, ridge, and lasso variants\nDecision trees (you may use an existing implementation - use individual tree(s) here, not a full random forest)\n\n\n\n\nImplementation and Assessment of FairStacks\n\nDefine the FairStacks Ensembling Problem, taking care to describe both the choice of loss function (logistic loss)2, regularization and any constraints used\nImplement the FairStacks Problem using CVX\n\nImplement a full model building (train-validation/ensembling-test split) pipeline to implement FairStacks robustly\nApply the FairStacks pipeline to your real data set and chosen fairness metric\nCompute the fairness and accuracy obtained by the FairStacks ensemble and compare it with the fairness and accuracy of the individual base learners. Does FairStacks succeed in its goal of improving both accuracy and fairness?\n\n\nAdditional Background\nML Fairness\nAs ML systems continue to permate our society, increasing interest has been paid to their effects on society. ML systems are highly effective at replicating the characteristics of their training data - both good (accuracy) and bad (bias).3 The book BHN reviews this emerging area in some detail.\nFor our purposes, we can restrict our attention to ‘fair classification’, i.e., making sure that ML systems treat different groups ‘equally’ (BHN §3). There are many definitions of ‘equally’ that can be appropriate, depending on the problem. In the simplest, demographic parity, the ML system should give the same fraction of positive (+1) labels to all demographic groups; this is the type of fairness embodied by phrases like “looks like America”. We quantify this with measures like deviation from demographic parity. If we divide our population into two groups \\(\\mathcal{G}_1, \\mathcal{G}_2\\), the deviation from demographic parity associated with a classifier \\(f: \\mathbb{R}^p \\to \\{0, 1\\}\\) is given by:\n\\[\\text{DDP}(f) = \\left|\\frac{1}{|\\mathcal{G}_1|}\\sum_{i \\in \\mathcal{G}_1} f(\\mathbf{x}_i) - \\frac{1}{|\\mathcal{G}_2|}\\sum_{i \\in \\mathcal{G}_2} f(\\mathbf{x}_i)\\right| = \\left|\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{G}_1} f(\\mathbf{x}) - \\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{G}_2} f(\\mathbf{x}) \\right|\\]\nThe 2018 FAccT Tutorial “21 Fairness Definitions and Their Politics” covers this and other fairnesss metrics. For purposes of this project, you should consider the fairness metric that is most appropriate to the social context surrounding your dataset of interest. Note that, if your interest is in a non-US data set, the relevant subgroups and legal doctrines may be significantly different than those (implicitly) assumed by most of the FairML literature.\nFairStacks\nFairStacks is a novel ensemble learning method which attempts to create a fair ensemble from a set of unfair base learners. At the highest level, FairStacks generalizes the following idea: if Approach A is biased against a group and Approach B is biased in favor of the same group, then Approach (A + B) will be approximately unbiased.\nTo put FairStacks into practice, we modify our basic ensembling procedure: in standard ensemble learning, we create a “model of models”, finding weights \\(\\hat{\\mathbf{w}}\\) such that\n\\[ \\hat{\\mathbf{w}} = \\text{arg min}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\text{Loss}\\left(\\mathbf{y}, \\sum_j w_j \\hat{f}_j(\\mathbf{x}_i)\\right) \\]\nwhere the inner sum is taken over the base learners \\(\\{\\hat{f}_j\\}\\). We add ‘fairness’ in the form of a regularizer (penalty). If \\(\\hat{b}_j\\) is the bias of base learner \\(\\hat{f}_j\\), FairStacks solves\n\\[ \\hat{\\mathbf{w}} = \\text{arg min}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\text{Loss}\\left(\\mathbf{y}, \\sum_j w_j \\hat{f}_j(\\mathbf{x}_i)\\right) + \\underbrace{\\lambda \\left|\\sum_j w_j \\hat{b}_j\\right|}_{\\text{Fairness Penalty}}\\]\nBy setting \\(\\lambda\\) large, we force the ‘average bias’ down to zero, resulting in a fairer ensemble. See the paper for more details.\nCVX\nThe CVX family of libraries attempt to make convex optimization easy and accesible to a wide range of users. Implementations of CVX can be found in:\n\nMatlab: the original CVX\n\nPython: cvxpy\n\nR: CVXR\n\nJulia: Convex.jl\n\n\nand more. While the details vary according to the host language, the essential structure is unchanged. I demonstrate the use of CVXR here to implement Lasso regression.\n\nset.seed(100)  # For reproducibility\nlibrary(CVXR)  # Load CVXR\nn &lt;- 400       # Set up a moderately high-dimensional problem\np &lt;- 1000\ns &lt;- 5\nsigma &lt;- 2\n\n# Lasso works best for IID Gaussian data\nX &lt;- matrix(rnorm(n * p), \n            nrow=n, ncol=p)\n\n# 'True' coefficients are mainly sparse with 5 non-zero values\nbeta_star &lt;- matrix(rep(0, p), ncol=1)\nbeta_star[1:s] &lt;- 3\n\n# Generate observed response from OLS DGP\ny &lt;- X %*% beta_star + rnorm(n, sd=sigma)\n\n## We are now ready to apply CVXR\n####  Also see discussion at\n####  https://www.cvxpy.org/examples/machine_learning/lasso_regression.html\n\nbeta &lt;- Variable(p) # Create 'beta' as a CVX _variable_ to be optimized\n\n# Per theory, about the right level of regularization to be used here\nlambda  &lt;- sigma * sqrt(s * log(p) / n) \nloss    &lt;- 1/(2 * n) * sum((y - X %*% beta)^2) # MSE Loss\npenalty &lt;- lambda * sum(abs(beta))\n\nobjective &lt;- Problem(Minimize(loss + penalty))\nbeta_hat  &lt;- solve(objective)$getValue(beta)\n\nWe can see that this correctly finds the non-zero elements:\n\nplot(beta_hat, \n     xlab=\"Coefficient Number\", \n     ylab=\"Lasso Estimate\", \n     main=\"CVX Lasso Solution\", \n     col=\"red4\", \n     pch=16)\n\n\n\n\n\n\n\nIf we look more closely, we see two important properties of the solution:\n\n\nNon-specialized solvers like CVXR cannot achieve exact zeros, so it is useful to do a bit of post-processing (e.g., zapsmall()).\nIf we need exact zeros, other approaches should be used\n\nThe lasso solution exhibits shrinkage, as the estimate are systematically smaller than the true values\n\n\nhead(beta_hat, n=10)\n\n               [,1]\n [1,]  2.435849e+00\n [2,]  2.498692e+00\n [3,]  2.376080e+00\n [4,]  2.418007e+00\n [5,]  2.424720e+00\n [6,]  1.756240e-21\n [7,]  1.604964e-21\n [8,] -1.193091e-21\n [9,] -9.722483e-22\n[10,] -1.320523e-21\n\n\nCVX is never the optimal approach for a given problem, but it is an incredibly useful tool for prototyping and trying out interesting new approaches without putting in the effort to derive and implement a specialized algorithm. (Tools like tensorflow and pytorch take this idea even further, but require significantly more effort to learn. If you are interested in working in ML after this course ends, they are a great place to invest your time.)\nThe documentation for the CVX packages has several examples which will be of use for you.\nPossible Topic(s) for Additional Research\nAlgorithms for Regularized Logistic Regression\nFor this report, you are not required to fit the FairStacks problem “by hand” and can use CVX instead. While CVX is flexible and easy to use, it rarely performs as efficiently as a hand-coded tailored algorithm. You may want to explore the use of convex optimization algorithms for fitting logistic regression problems generally and the FairStacks problem specifically.\nWhile the gradient descent method you used in Research Report #01 can be applied here, it is likely to be unbearably slow. You may achieve better performance using a Newton or Quadratic Approximation approach. These approaches use a (second-order) Taylor expansion to approximate the objective function by a quadratic function; that quadratic function has a closed-form minimizer (essentially the OLS solution) which is used to update the iterate.4 A new approximation is created at the new iterate, solved again, etc. See BV §9.5 for details. This approach is widely used to fit generalized linear models such as logistic regression. In the statistics literature, you may see it described as Iteratively Reweighted Least Squares or Fisher Scoring; see, e.g., the output of summary on a glm fit in R.\nAlternative Fair ML Proposals\nThe FairStacks approach is far from the only Fair ML proposal in the literature. If this is a topic that interest you, you may want to compare FairStacks to other proposals and see which performs the best on your data set and which achieves an acceptable level of fairness. (Not all methods can achieve all levels of fairness). Even though FairStacks worked the best on the problems we considered, it may not be the best for your data set - there is no free lunch after all!\nAdditional Randomization\nAs discussed in class, ensemble methods work best when the base learners exhibit relatively low degrees of correlation. (If all base learners make the same prediction, it does not matter how they are weighted in the ensemble.) This is often particularly difficult to guarantee when applying powerful ML techniques to not-too-difficult prediction problems, as any technique worth knowing will perform reasonably well. In the FairStacks paper, we used a novel technique called mini-patching5 to increase the variance of the base learner ensemble: mini-patching works by taking small subsets of both rows (observations) and columns (features) and fitting base learners to these subsets. We then fit the mini-patched learners using the FairStacks ensembling approach.\nDoes this technique improve the performance of FairStacks on your data set? Are there other techniques you can use to create alternative / better base learners to use in your ensemble?\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.6 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report02.html#footnotes",
    "href": "reports/report02.html#footnotes",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis Research Report is adapted from my ongoing research collaboration with Camille Little (Rice University) and Genevera Allen (Columbia University). A now-somewhat-dated manuscript describing our work in this space can be found online at ArXiv 2206.00074.↩︎\nIn the FairStacks paper, we used OLS as a classification method for somewhat technical reasons. For this Research Report, you should use logistic loss as your primary loss function. It may be an interesting extension to compare against OLS.↩︎\nThe earliest entries in the modern era of chatbots were marked by several flaws reflective their training data. Early iterations of the chat engine built into the Bing search engine were often rather overly-sexual and not-so-subtly malicious, perhaps reflecting the darker corners of the internet on which it was trained.↩︎\nFor this approach, you should use the matrix-analytic closed-form OLS solution instead of using ‘internal’ gradient descent. The aim of Newton methods is to avoid gradient descent.↩︎\nMini-Patching is related to, but should not be confused with the more common technique of Mini-Batching.↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "",
    "text": "There are no required textbooks for this course. Supplemental readings and suggested practice will be assigned from the following readily-available resources:\n\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nConvex Optimization by Boyd and Vandenberghe (BV)\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (ISL)\nElements of Statistical Learning by Hastie Tibshirani, and Freedman (ESL)\nStatistical Learning with Sparsity by Hastie, Tibshirani, and Wainwright (SLS)\nPattern Recognition and Machine Learning by Bishop (PRML)\nProbabilistic Machine Learning: An Introduction by Murphy (PML-1)\nProbabilistic Machine Learning: Advanced Topics by Murphy (PML-2)\nGaussian Processes for Machine Learning by Rasmussen and Williams (GPML)\nReinforcement Learning by Sutton and Hutto (RL)\nBoosting: Foundations and Algorithms by Schapire and Freund (SF)\nFoundations of Machine Learning by Mohri, Rostamizadeh, and Talwalkar (FML)\nUnderstanding Machine Learning: From Theory to Algorithms by Shalev-Shwartz and Ben-David (UML)\nPatterns, Predictions, and Actions by Hardt and Recht (HR)\nFairness and Machine Learning by Barocas, Hardt, and Narayanan (BHN)\nLearning Theory from First Principles by Bach (LTFP)\n\n\n\nSTA 9890 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nThe instructor will provide a formula sheet for use during in-class exams. No alternative resources may be used during these activities.\nStudents may use online resources, such as blog posts, discussion fora, etc. for the Research Reports and Course Competition but Generative AI tools are specifically disallowed.\n\nException: Coding Assistance Tools, e.g., GitHub CoPilot, may be used for the Research Reports and Course Competition. Their usage is limited to coding your ideas. They may not be used as idea generation tools.\n\nThe era of Generative AI is challenging and fast-paced. If you are unsure whether a tool is allowed in this course, please reach out to the instructor before using it. In matters of Academic Integrity, it is far better to ask permission than forgiveness."
  },
  {
    "objectID": "resources.html#course-resources",
    "href": "resources.html#course-resources",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "",
    "text": "There are no required textbooks for this course. Supplemental readings and suggested practice will be assigned from the following readily-available resources:\n\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nConvex Optimization by Boyd and Vandenberghe (BV)\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (ISL)\nElements of Statistical Learning by Hastie Tibshirani, and Freedman (ESL)\nStatistical Learning with Sparsity by Hastie, Tibshirani, and Wainwright (SLS)\nPattern Recognition and Machine Learning by Bishop (PRML)\nProbabilistic Machine Learning: An Introduction by Murphy (PML-1)\nProbabilistic Machine Learning: Advanced Topics by Murphy (PML-2)\nGaussian Processes for Machine Learning by Rasmussen and Williams (GPML)\nReinforcement Learning by Sutton and Hutto (RL)\nBoosting: Foundations and Algorithms by Schapire and Freund (SF)\nFoundations of Machine Learning by Mohri, Rostamizadeh, and Talwalkar (FML)\nUnderstanding Machine Learning: From Theory to Algorithms by Shalev-Shwartz and Ben-David (UML)\nPatterns, Predictions, and Actions by Hardt and Recht (HR)\nFairness and Machine Learning by Barocas, Hardt, and Narayanan (BHN)\nLearning Theory from First Principles by Bach (LTFP)\n\n\n\nSTA 9890 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nThe instructor will provide a formula sheet for use during in-class exams. No alternative resources may be used during these activities.\nStudents may use online resources, such as blog posts, discussion fora, etc. for the Research Reports and Course Competition but Generative AI tools are specifically disallowed.\n\nException: Coding Assistance Tools, e.g., GitHub CoPilot, may be used for the Research Reports and Course Competition. Their usage is limited to coding your ideas. They may not be used as idea generation tools.\n\nThe era of Generative AI is challenging and fast-paced. If you are unsure whether a tool is allowed in this course, please reach out to the instructor before using it. In matters of Academic Integrity, it is far better to ask permission than forgiveness."
  },
  {
    "objectID": "resources.html#aipolicy",
    "href": "resources.html#aipolicy",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Academic Integrity Policy",
    "text": "Academic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nIn this course, expectations for academic integrity for in-class graded activities are straightforward: no use of unauthorized materials on weekly quizzes or mid-semester tests. Unless explicitly stated otherwise by the instructor in writing, the only authorized materials allowed during in class assessment are the instructor-provided formula sheets.\nThe Research Reports are “open peers”, “open book”, “open internet” but you may not use paid services or generative AI services. You must cite all sources used and include an acknowledgement section listing all peers with whom you collaborated. You do not need to acknowledge the instructor or any Piazza discussions, but you must acknowledge other faculty members or other Baruch students who help you complete your reports, even if they are not enrolled in this course.\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity."
  },
  {
    "objectID": "resources.html#course-accomodations",
    "href": "resources.html#course-accomodations",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Course Accomodations",
    "text": "Course Accomodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services (SDS). Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed."
  },
  {
    "objectID": "resources.html#personal-resources2",
    "href": "resources.html#personal-resources2",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Personal Resources1",
    "text": "Personal Resources1\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).2\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\nOn campus, you can also access the Bearcat Food Pantry.\n\n\nFinancial Security\nBaruch students experiencing heightened financial stress have access to Student Emergency Grants administered through the Office of the Dean of Students.\nNote that funds are also available for students experiencing immigration-related financial stress.\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during business hours for more information."
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎"
  },
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "",
    "text": "This course applies multiple regression techniques to the increasingly important study of very large data sets. Those techniques include linear and logistic model fitting, inference, and diagnostics. Methods with special applicability for Big Data will be emphasized, such as lasso and ridge regression. Issues of model complexity, the bias-variance tradeoff, and model validation will be studied in the context of large data sets. Methods that rely less on distributional assumptions are also introduced, including cross-validation, bootstrap resampling, and nonparametric methods. Students will learn dimension reduction methods, correlation analysis, and random forests."
  },
  {
    "objectID": "objectives.html#official-course-description--",
    "href": "objectives.html#official-course-description--",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "",
    "text": "This course applies multiple regression techniques to the increasingly important study of very large data sets. Those techniques include linear and logistic model fitting, inference, and diagnostics. Methods with special applicability for Big Data will be emphasized, such as lasso and ridge regression. Issues of model complexity, the bias-variance tradeoff, and model validation will be studied in the context of large data sets. Methods that rely less on distributional assumptions are also introduced, including cross-validation, bootstrap resampling, and nonparametric methods. Students will learn dimension reduction methods, correlation analysis, and random forests."
  },
  {
    "objectID": "objectives.html#course-learning-objectives",
    "href": "objectives.html#course-learning-objectives",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9890 will be able to:\n\nIdentify Key ML Tasks and Trade-Offs\nUse Regression and Classification Tools to Develop Interpretable and Accurate Predictive Models\nDevelop and Apply Ensemble Learning Strategies\nAccurately Assess Model Performance Using CV, Hold-Out, Stability, and Bootstrap Techniques\nUse Unsupervised Methods to Find Meaningful Structure in Data\nApply Statistical Machine Learning Methods to Novel Data Structures and Types\nGenerate and Communicate Insights via Statistical Machine Learning Methods\n\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\nLearning Goal 6\nLearning Goal 7\n\n\n\n\nTest #01\n✓\n✓\n\n✓\n\n\n\n\n\nResearch Report #01\n✓\n✓\n\n✓\n\n\n\n\n\nTest #02\n✓\n✓\n✓\n✓\n\n\n\n\n\nResearch Report #02\n✓\n✓\n✓\n✓\n\n\n\n\n\nTest #03\n✓\n\n✓\n✓\n✓\n\n\n\n\nResearch Report #03\n✓\n\n✓\n✓\n✓\n\n✓\n\n\nCourse Competition\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\nIn-Class Presentation\n\n\n\n\n✓\n✓\n✓"
  },
  {
    "objectID": "objectives.html#program-learning-goals",
    "href": "objectives.html#program-learning-goals",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course is intended to contribute to the following Program Learning Goals for the MS in Business Analytics:\n\n\n\n Learning Goal\nMS BA Learning Goal\nDescription\n\n\n\n\n\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n✓\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non-technical/lay audiences.\n\n\n✓\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course is intended to contribute to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\n\n\n Learning Goal\nMS QMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g., deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n✓\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course is intended to contribute to the following Program Learning Goals for the MS in Statistics:\n\n\n\n Learning Goal\nMS Statistics Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems form business and the other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n✓\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "STA 9890 - Handouts and Additional Notes",
    "section": "",
    "text": "Supplemental course notes and links to useful external resources will be posted here.\n\nSupplemental Lecture Notes\n\nWeek 1 - Course Overview & Introduction to ML (2025-01-28)\n\nInstructor Notes\n\n\n\nWeek 2 - Regression I (2025-02-04)\n\nInstructor Notes\n\nThe YouTube channel “3Blue1Brown” makes excellent videos explaining mathematical concepts. A recent entry discusses gradient descent in the context of Neural Networks. At this point in the course, our focus is still on simpler (convex) methods, so not all of this will be directly applicable, but it is still a useful summary and gives helpful background on how gradient methods remain at the heart of all modern ML.\n\nYou may also find value in 3Blue1Brown videos on Linear Algebra and on Probability.\nOf these, the following are likely to be particularly useful in this course:\n\nVectors\nLinear Transformations\nMatrix Multiplication\nInverses, Rank, and Null Space\nDot Products and Duality\nEigenvectors and Eigenvalues\nBayes’ Theorem\n\nthough you don’t need to watch all of these immediately.\nIn this course, we will apply calculus techniques (mainly differentiation) to functions \\(\\mathbb{R}^{p} \\to \\mathbb{R}\\). The website matrixcalculus.org/ is helpful for this work.\n\n\nWeek 3 - Regression II (2025-02-11)\n\nInstructor Notes\n\n\n\nWeek 4 - Regression III (2025-02-25)\n\nInstructor Notes\n\n\n\nWeek 5 - Introduction to Classification (2025-03-04)\n\nInstructor Notes\n\n\n\nWeek 6 - Classification I (2025-03-11)\n\nInstructor Notes\n\n\n\nWeek 7 - Classification II (2025-03-18)\n\nInstructor Notes\n\n\n\nWeek 8 - Ensemble Learning & Resampling Methods (2025-03-25)\n\nInstructor Notes\n\n\n\nWeek 9 - Tree-Based Methods (2025-04-01)\n\nInstructor Notes\n\n\n\nWeek 10 - Introduction to Unsupervised Learning (2025-04-08)\n\nInstructor Notes\n\n\n\nWeek 11 - Unsupervised Learning I (2025-04-22)\n\nInstructor Notes\n\n\n\nWeek 12 - Unsupervised Learning II (2025-04-29)\n\nInstructor Notes\n\n\n\nWeek 13 - Introduction to Generative Models (2025-05-06)\n\nInstructor Notes"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "STA 9890 - Research Reports",
    "section": "",
    "text": "In lieu of traditional homework, STA 9890 has three “research reports” (one for each unit of the course). These research reports are intended to help you develop your skills in the computational and methodological aspects of Statistical Machine Learning. Specifically, these reports are designed to help you learn to deal with novel machine learning developments published as academic research papers, to assess whether so-called developments are actually able to do what they claim, to determine whether a proposed method can actually solve the problem you care about and, if appropriate, to call shenanigans on the ‘puffery’ that is pervasive in the ML literature.\nEach Research Report must be submitted as a fully-typed PDF using the course Brightspace. (No handwritten work will be graded.) Each report must include all code used and should have several figures. Reports should be 6-8 pages, double spaced in a legible 10 to 12 point font.\n\nResearch Reports\n\nResearch Report #01: Bias and Variance in Linear Regression\nDue Dates:\n\nReleased to Students: 2025-02-04\nSubmission Deadline: 2025-03-07 11:45pm ET\n\nIn Research Report #01, you will dig into the oft-cited claim that Ordinary Least Squares is a Best Linear Unbiased Estimator (BLUE). In classical statistics, the BLUE property is often used as an argument of optimality, implying that we can’t beat OLS, so we shouldn’t even try. As you will see, this optimality of OLS is quite overstated: OLS can be beaten quite easily whenever its assumptions are violated, whenever non-linear estimators are allowed, or whenever bias is permitted (taking “Best” to mean “minimum MSE” instead of “minimum variance”).\nThese findings may seem a bit abstract, but they get at the heart of almost every method and principle we will cover in this course. In this project, in addition to getting a better understanding of what BLUE does and does not mean, you will learn to:\n\nimplement gradient descent methods\ndesign Monte Carlo simulations to assess bias and variance\nfind optimal values of tuning parameters using cross-validation\n\n\n\nResearch Report #02: Ensemble Learning Techniques for Fair Classification\nDue Dates:\n\nReleased to Students: 2025-03-11\nSubmission Deadline: 2025-04-18 11:45pm ET\n\nIn Research Report #02, you will apply some of the tools we have developed to the problem of fairness in machine learning (FairML). While not a core topic for this course, this exercise is useful to see how the core idea of this course–regularization, optimization, etc.–can be applied to interesting and novel questions. In this project, you will also engage critically with a newly proposed ML method and investigate i) whether it truly does what it claims to; ii) whether it can be efficiently and reliably implemented; and iii) the degree to which (if any!) it solves your problem of interest. As working Data Scientists and Business Analysts, you may not think of yourselves as researchers, but knowing how to read and critically evaluate cutting-edge work will let you maintain and enhance your skills throughout your career.\n\n\nResearch Report #03: Sparse Principal Components Analysis\nDue Dates:\n\nReleased to Students: 2025-04-22\nSubmission Deadline: 2025-05-09 11:45pm ET\n\nIn Research Report #03, you will explore sparse PCA and apply it to a data set of interest. As you do so, you will see how a modern machine learning principle (sparsity) can be used to improve a classical statistical technique like PCA to get ‘the best of both worlds.’ Because our focus here is on an unsupervised method, this report should be careful to consider interpretation and validation of the resulting PCs, as standard validation techniques for supervised methods cannot be applied."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 9890 - Statistical Learning for Data Mining",
    "section": "",
    "text": "Welcome to the course website for STA 9890 - Statistical Learning for Data Mining (Spring 2025)!\nSTA 9890 is a survey of Statistical Machine Learning targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives. Official copies of these documents can be found on CUNY Brightspace.\nThis site also hosts descriptions of the Course Competition and Research Reports (Homework), as well as selected Handouts and Additional Notes that will be useful.\nThere are quite a few moving parts to this course, so this key dates file or the list of upcoming course activities below may be useful:\n\n\n\n\n\n\n\nA CSV file suitable for import into Google Calendar with all assignment deadlines can be found here.\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 9890 - Course Syllabus",
    "section": "",
    "text": "All syllabus and course schedule provisions subject to change with suitable advance notice."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA 9890 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎\nThough ubiquitous, email is a remarkably ‘flaky’ service, providing the sender no way to guarantee their message arrives untampered and providing the recipient no way to guarantee the providence of a message received. (This is not quite true: there are tools for more secure email but they are somewhat more difficult to use and are not supported at CUNY.) Brightspace is integrated with CUNY’s Identity Verification Services and allows students to guarantee correct submission. Note that Brightspace does not, by default, send students an email confirming submission, but I believe this is an option that can be enabled on the student’s end.↩︎\nHaoyu Chen and Jiongjiong Yang. “Multiple Exposures Enhance Both Item Memory and Contextual Memory over Time”. Frontiers in Psychology 11. November 2020. DOI:10.3389/fpsyg.2020.565169↩︎\nFor this course, an average student is a student who enters the course with:\n\nFluency with statistical and numerical software at the level of (at least) STA 9750;\nFluency with univariate and multivariate regression at the level of (at least) STA 9700; and\nFamiliarity with probability and linear algebra;\n\nand is earning a B-range grade. If you have less background or are aiming for a higher grade, you should expect to commit proportionally more time to this course.\nIf you lack the prerequisite background listed above or simply wish to review it before the semester begins in earnest, please reach out to the instructor and I will be more than happy to provide supplementary readings.↩︎\nThe CUNY Graduate Center has a useful summary of these expectations. Baruch courses follow the same standards. See also CUNY Central Policy.↩︎\nAs a student, you have free access to GitHub CoPilot once you create a student GitHub account and register for the Student Developer Pack.↩︎\nIn theory, you can work with these frameworks at this sort of “low-level”, but they are likely to be far more cumbersome than using base R or standard numpy for these tasks. If you are interested in working with these frameworks in this manner in order to better familiarize yourself with the core computational abstractions, you may request instructor permission to do so.↩︎"
  },
  {
    "objectID": "reports/report03.html",
    "href": "reports/report03.html",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "",
    "text": "Released to Students: 2025-04-22\nSubmission: 2025-05-09 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report03.html#research-report",
    "href": "reports/report03.html#research-report",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "Research Report #03: Sparse Principal Components Analysis",
    "text": "Research Report #03: Sparse Principal Components Analysis\nIn the final Research Report of this course, you will investigate the intersection of sparsity, a form of interpretability, with unsupervised learning, the branch of ML focused on finding meaningful patterns in data.\nClassically, principal components analysis (PCA) is the cornerstone of dimension reduction methods. By reducing a large number of features to a smaller number of linearly independent (uncorrelated) components, PCA is often said to increase interpretability. This assumes a degree of “reification” - is the PC something meaningful on its own?\nIf the PC captures a true underlying factor, then perhaps we have found something worth interpreting; but PCA always finds some sort of factorization, even when there is no “true” underlying factor. In some circumstances, this PC becomes a thing in itself, e.g., IQ, but that cannot always be assumed. If we do not assume or define a meaning to the PC, it can only be understood as a combination of all of the features used to create it, hardly a paragon of interpretability.\nSparse PCA takes a different approach: rather than finding an interpretation of the linear combination of all features that captures the most variance, it seeks a linear combination of only a few features that explain nearly-the-most variance. Because only a few features are used, the resulting PC is more interpretable a priori than the output of classical PCA.\nAs background, you should read the paper by Weylandt and Swiler1 and review the Supporting Materials to see how Sparse PCA can be applied in a scientific setting. For more methodological background, see the references cited therein, especially the paper by Witten et al., as well as ISL §6.3.1 and §12.2 and SLS §8.1-8.2.2.\nProject Skeleton\nYour report should be divided into three sections, covering the following:\n\nBackground on PCA and Sparse PCA\nComputation - Implementing the Power Method and the Sparse Power Method for PCA\nImplementation and Assessment of Sparse PCA\n\nAt a minimum, these should include the following elements:\n\nBackground on PCA and Sparse PCA\n\nDerivation of PCA from Variance Maximization to Singular Value Decomposition\nProof of Convergence of the Power Method for SVD Computations\nModification of Classical Power Method for Sparsity\n\n\nComputation\n\nImplementation of Classical and Sparsified Power Method\nDiscussion of Convergence for Power Methods2\n\nHow might one tune the sparsity level used?\n\n\nImplementation and Assessment\n\nIn-Simulation: Construct simulations to compare the accuracy of classical and sparse PCA. Which does better when the “true” PCs are dense? When they are sparse? Does this depend on the sample size? How can we measure accuracy of the estimated PCs?\nOn Real Data: Identify a real data set where PCA can be applied. Apply both classical and sparse PCA and compare the results. Which PCA is better (in whatever sense)? Which PCA is more interepretable? Is there a way to validate your interpretation?\n\n\nAdditional Background\nThe Power Method is a classical approach to computing eigenvectors and, by extension, singular vectors. For this research report, you will build upon the singular vector variant, so I review the eigenvector variant here.\nSuppose \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{p \\times p}_{\\succ 0}\\) is a strictly positive definite \\(p \\times p\\) matrix. By the spectral theorem, it has an eigendecomposition:\n\\[\\mathbf{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\]\nwhere the eigenvalues \\(\\{\\lambda_i\\}\\) are decreasing and strictly positive: (\\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\\)). If the eigenvalues are distinct (as they are with probability 1 for continuous-valued data observed noisly), then this decomposition is unique up to sign.3\nThe power method for computing the leading (highest eigenvalue) eigenvector of \\(\\mathbf{\\Sigma}\\) proceeds as follows:\n\nInitialize: select \\(\\mathbf{v}^{(0)}\\) as a random unit vector\nRepeat Until Convergence:\n\nMultiply: \\(\\tilde{\\mathbf{v}}^{(k+1)} = \\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\)\n\nNormalize: \\(\\mathbf{v}^{(k+1)} = \\tilde{\\mathbf{v}}^{(k+1)} / \\|\\tilde{\\mathbf{v}}^{(k+1)}\\|_2\\)\n\nIterate: \\(k \\leftarrow k + 1\\)\n\n\n\nReturn: At convergence, return the eigenvector \\(\\mathbf{v}^{(k)}\\) and the eigenvalue \\(\\lambda = \\|\\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\|_2\\)\n\n\nAdditional eigenvectors can be found by “deflating” \\(\\mathbf{\\Sigma}\\) and applying the power method to \\[\\mathbf{\\Sigma}' = \\mathbf{\\Sigma} - \\lambda \\hat{\\mathbf{v}}\\hat{\\mathbf{v}}^{\\top}.\\]\nSo long as the eigenvalues of \\(\\mathbf{\\Sigma}\\) are distinct, the power method converges to the leading eigenvector as long as the initial guess is not perfectly orthogonal to the true eigenvector. If we pick our initial guess randomly, this is a very reasonable assumption. To see why this is the case, note that:\n\\[\\begin{align*}\n\\mathbf{v}^{(k)} &\\propto \\mathbf{\\Sigma}^k \\mathbf{v}^{(0)} \\\\\n                 &= \\left(\\sum_{i=1}^p \\lambda_i^k \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\right)\\mathbf{v}^{(0)} \\\\\n                 &= \\sum_{i=1}^p \\lambda_i^k \\langle\\mathbf{v}_i, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_i\n\\end{align*}\\]\nViewed here, you can see why the power method has its name. As \\(k \\to \\infty\\), the first term in this series becomes much much larger than the others, so we get\n\\[\\mathbf{v}^{(k)} \\buildrel \\sim \\over \\propto \\lambda_1^k \\langle\\mathbf{v}_1, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_1\\] After normalizing and dropping the scalar terms, we have\n\\[\\mathbf{v}^{(k)} \\to \\mathbf{v}_1\\]\nas desired. This argument also clarifies the two possible failure modes of the power method:\n\nIf the initial guess is unlucky and orthogonal to the true eigenvector, we have \\(\\langle \\mathbf{v}_1, \\mathbf{v}^{(0)} \\rangle = 0\\) so the \\(\\mathbf{v}_1\\) term drops out of the answer and we converge to the next eigenvector (unless we are orthogonal to that one as well).\nIf the eigenvalues are not distinct, then multiple terms become large and we get a strange ‘superposition’ of multiple eigenvectors. (Alternatively, there isn’t a ‘true’ right answer, but we get one possible eigenvector.)\nPotential Topics for Additional Research\nWhile the main focus of this report is on Sparse PCA, a suitably sparsified SVD can be used for a wide variety of multivariate analysis problems. You may wish to extend your analysis to other sparse multivariate methods, such as Sparse CCA or Sparse PLS.\nAlternatively, a huge number of approaches to Sparse PCA have been proposed, not all of which are based on the power method. You may wish to compare the approach I prefer (projected power method) with other approaches: be sure to consider i) accuracy; ii) interpretability; and iii) computational ease / efficiency in your comparison.\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.4 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report03.html#footnotes",
    "href": "reports/report03.html#footnotes",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nM. Weylandt and L.P. Swiler. “Beyond PCA: Additional Dimension Reduction Techniques to Consider in the Development of Climate Fingerprints”. Journal of Climate 37(5), p.1723-1735. 2024. DOI: 10.1175/JCLI-D-23-0267.1. Direct Link↩︎\nRecall that PCs are only defined “up to sign”. If \\(\\hat{\\mathbf{u}}\\) is a valid PC, so is \\(-\\hat{\\mathbf{u}}\\). You will need to modify your usual convergence checks to account for this.↩︎\nNote that we can substitute \\(\\mathbf{v}_i \\to -\\mathbf{v}_i\\) without changing the result since the minus signs will cancel in \\((-\\mathbf{v}_i)(-\\mathbf{v}_i)^{\\top} = \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\).↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "reports/report01.html",
    "href": "reports/report01.html",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "",
    "text": "Released to Students: 2025-02-04\nSubmission: 2025-03-07 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report01.html#research-report",
    "href": "reports/report01.html#research-report",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "Research Report #01: Bias and Variance in Linear Regression",
    "text": "Research Report #01: Bias and Variance in Linear Regression\nIn this research report, you will dive deeply into the bias-variance trade-off and the BLUE-ness (or lack thereof) of ordinary least squares.\nProject Skeleton\nYour report should be divided into four sections, covering the following:\n\nTheoretical background\nComputation - Gradient Descent and Weight Decay\nBias and Variance Under Linear Data Generating Processes (DGP)\nBias and Variance Under Non-Linear DGP\n\nAt a minimum, these should include the following elements:\n\nTheoretical Background\n\n\nRigorous Statement and Proof of the Bias-Variance Decomposition\n\\[\\mathbb{E}[\\text{MSE}] = \\text{Bias}^2 + \\text{Variance} (+ \\text{Irreducible Noise})\\]\nDiscuss how this statement is to be interpreted in the context of parameter estimation (when the underlying DGP is linear) and in the context of (possibly misspecified) prediction. Be sure to clarify when the “Irreducible Noise” term is needed.\n\nProof of the “BLUE” property of OLS with clear statement of the relevant assumptions. Take care to differentiate which assumptions are required for “B” and which are required for “U”.\n\n\nComputation\n\nDerivation and implementation of the ‘closed-form’ matrix expression for OLS\nDerivation and implementation of the ‘closed-form’ matrix expression for Ridge Regression\nDerivation and implementation of a gradient descent method for OLS\nDerivation and implementation of a ‘weight decay’ modification for OLS gradient descent.\nEmpirical demonstration of the equivalance of OLS+Weight Decay with Ridge Regression\n\n\nBias and Variance Under Linear DGP\n\nPosit a Linear DGP\nUsing Monte Carlo simulations:\n\nShow that OLS is unbiased under this DGP\nCompute the variance of a given regression coefficient\nCompute the in- and out-of-sample prediction MSE\nShow how the variance, in-sample, and out-of-sample MSE change with the sample size, \\(n\\)\n\n\n\nCompare against ridge regression\n\nShow that RR is biased under this DGP\nCompare the variance of OLS and RR\nDemonstrate the MSE Existence Theorem for Ridge Regression\nCompare the MSE of RR and OLS as a function of the sample size \\(n\\)\n\n\n\n\n\nBias and Variance Under Non-Linear DGP\n\nPosit a Non-Linear DGP\nUsing simulation, determine the “best approximate” linear regression coefficients: that is, find the OLS coefficients minimizing MSE\nShow that the estimated OLS coefficients converge to the “best approximate” coefficients as the sample size \\(n\\) increases\nCompute the MSE of RR in simulation and then use 5-fold cross-validation to determine the optimal regularization level\nCompare the MSE of RR and OLS in the non-linear setting\n\n\n\nFinally, note that because we are interested in bias, variance, and MSE – each of which is defined with respect to a (known) DGP – we only use statistical simulation for this project. In Report #02 and Report #03, you will apply methods to real (non-simulated) data.\nAdditional Background\nGradient Descent methods fit models by taking small steps in the direction opposite the gradient of the loss function. (Recall the gradient points in the direction of steepest increase, so moving in the opposite direction leads to a decrease in error.)\nAbstractly, given a differentiable loss function, \\(\\mathcal{L}(\\beta)\\), gradient descent proceeds as follows:\n\nSelect initial guess \\(\\beta^{(0)}\\) and set \\(k=0\\)\n\nRepeat until convergence:\n\n\nCompute gradient \\(\\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}\\)\n\nUpdate guess using a gradient step \\[\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}\\]\n\nIncrement \\(k := k+1\\)\n\nCheck convergence:\n\nParameter convergence: if \\(\\beta^{(k+1)} \\approx \\beta^{(k)}\\), we have converged.\nObjective convergence: if \\(\\mathcal{L}(\\beta^{(k+1)}) \\approx \\mathcal{L}(\\beta^{(k)})\\), we have converged\n\n\n\n\nAt convergence, return \\(\\beta^{(k+1)}\\)\n\n\nThis scheme is pretty easy to implement, but there are a few points where it behooves you to be careful:\n\nHow do we check the approximate equality in the convergence check?\nWe will almost never get exact equality, so you need to pick a norm and a tolerance.\nHow do we choose the step-size \\(c\\)? Much has been written about this; for our purposes it suffices to take a very small value of \\(c\\) and keep it constant.\nWhat if we never reach convergence? Should we stop after some (large) number of iterations?\n\nThe method of weight-decay modifies the gradient update as follows:\n\\[\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}} - \\omega \\beta^{(k)}\\]\nHere \\(\\omega\\) is another small constant scalar value. By subtracting off a fraction of \\(\\beta^{(k)}\\) at each step, weight decay prevents the estimate from ever getting to be too big (hence the name). In the case of OLS, this can be shown to be equivalent to a form of ridge regression.1\nThe MSE Existence Theorem for Ridge Regression comes in two forms:\n\n\nEstimation Variant. Let \\[\\text{MSE}_i(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}}[(\\hat{\\beta_i} - \\beta_i^*)^2]\\] denote the mean squared estimation error in the \\(i^\\text{th}\\) component of \\(\\hat{\\beta}\\) as an estimator of the true regression coefficients \\(\\beta^*\\). Here the expectation is taken over random realizations of both the design matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{y}\\), as well as the estimator \\(\\hat{\\beta}\\) which is a function of \\(\\mathbf{X}, \\mathbf{y}\\).\nThen, there exists some positive value \\(\\lambda &gt; 0\\) such that\n\\[\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) &gt; \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})\\]\nwhere \\(\\hat{\\beta}_{\\text{RR}, \\lambda}\\) is the ridge regression estimate with regularization parameter \\(\\lambda\\).\n\n\nPrediction Variant. Let \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}, \\mathbf{y}'}[\\|\\mathbf{X}\\hat{\\beta} - \\mathbf{y}'\\|_2^2\\] denote the mean squared prediction error associated with the estimator \\(\\hat{\\beta}\\). Here the expectation is taken over random realizations of both the design matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{y}\\) used for training, as well as the test data vector \\(\\mathbf{y}'\\) which must be drawn from the same DGP.\nThen, there exists some positive value \\(\\lambda &gt; 0\\) such that\n\\[\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) &gt; \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})\\]\nwhere \\(\\hat{\\beta}_{\\text{RR}, \\lambda}\\) is the ridge regression estimate with regularization parameter \\(\\lambda\\).\n\n\nIt is important to note that both of these are statements in expectation: OLS may perform better than RR on a single realization, but in average - over many realizations - suitably-tuned RR will perform better.\nAlso, we note that the MSE Existence Theorem does not give any practical advice on selecting \\(\\lambda\\); we have to fall back on our standard approaches, such as cross-validation, for doing so.\nFinally, note that the prediction version of the MSE Existence Theorem only holds when we look at test (out-of-sample) prediction accuracy. OLS is the optimal linear method for in-sample MSE always and forever.\nPossible Topic(s) for Additional Research\nRecent ML research has focused on the case of overparameterized models: that is, models with more parameters than data points used for training. We have already seen ridge regression as one way to deal with this problem. Another approach is to simply use an iterative method, such as gradient descent, and stop it when the training error reaches zero. (This is a non-unique global minimum for the convex OLS problem.) How does this procedure compare? Does it complicate our understanding of the bias-variance trade-off?\nAnother recent line of research generalizes the “BLUE” property, showing that OLS is in an appropriate sense “BUE” - it is still optimal even if we allow for (certain types of) non-linear estimators as well. Can you show this?\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.2 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report01.html#footnotes",
    "href": "reports/report01.html#footnotes",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nA somewhat remarkable finding of much recent research is that many ‘new’ regularization methods pioneered for fitting neural networks have essentially the same effect as ridge (\\(\\ell_2^2\\) or Tikhonov) regularization: weight decay, drop out, early stopping, mini-batching and mini-patching. Sometimes it’s hard to beat the classics.↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "notes/notes01.html",
    "href": "notes/notes01.html",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "",
    "text": "Where classical statistics focuses on learning information about a population from a (representative) sample, Machine Learning focuses on out-of-sample prediction accuracy.\nFor example, given a large set of medical records, the natural instinct of a statistician is to find the indicators of cancer in order assess them via some sort of follow-on genetic study, while a ML practitioner will typically start by building a predictive algorithm to predict who will be diagnosed with cancer. The statistician will perform calculations with \\(p\\)-values, test statistics, and the like to make sure that any discovered relationship is accurate, while the ML practitioner will verify the performance by finding a new (hopefully similar) set of medical records to test algorithm performance.\nClearly, this difference is more one of style than substance: the statistician might see what features are important in the ML model to decide what to investigate, while the ML modeler will use statistical tools to make sure the model is finding something real and not just fitting to noise. In this course, the distinction may be even blurrier as our focus is statistical machine learning - that little niche right on the boundary between the two fields.\nIn brief, “statistics vs ML” is a bit of a meaningless distinction as both fields draw heavily from each other. I tend to say one is doing statistics whenever the end-goal is to better understand something about the real world (ie., the end product is knowledge), while one is doing ML whenever one is building a system to be used in an automated fashion (ie., the end product is software), but definitions vary.1\nIn this course we will use the following taxonomy borrowed from the ML literature:\n\nSupervised Learning: Tasks with a well-defined target variable (output) that we aim to predict\nExamples:\n\nGiven an image of a car running a red-light, read its license plate.\nGiven attendance records, predict which students will not pass a course.\nGiven a cancer patient’s medical records, predict whether a certain drug will have a beneficial impact on their health outcomes.\n\nUnsupervised Learning: Given a whole set of variables, none of which is considered an output, learn useful underling structure.\nExamples:\n\nGiven a set of climate simulations, identify the general trends of a climate intervention.\nGiven a set of students’ class schedules, group them into sets. (You might imagine these sets correspond to majors, but without that sort of “label” (output variable) this is only speculative, so we’re unsupervised)\nGiven a social network, identify the most influential users.\n\n\nThere are other types of learning tasks: e.g. semi-supervised, online, reinforcement, but the Supervised/Unsupervised distinction is the main one we will use in this course.\nWithin Supervised Learning, we can further subdivide into two major categories:\n\nRegression Problems: problems where the response (label) is a real-valued number\nClassification Problems: problems where the response (label) is a category label.\n\nBinary classification: there are only two categories\nMulti-way or Multinomial classification: multiple categories\n\n\nLinear Regression, which we will study more below, is the canonical example of a regression tool for supervised learning.\nAt this point, you can already foresee one of the (many) terminology inconsistencies will will encounter in this course: logistic regression is a tool for classification, not regression. As modern ML is the intersection of many distinct intellectual traditions, the terminology is rarely consistent.2"
  },
  {
    "objectID": "notes/notes01.html#a-taxonomy-of-machine-learning",
    "href": "notes/notes01.html#a-taxonomy-of-machine-learning",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "",
    "text": "Where classical statistics focuses on learning information about a population from a (representative) sample, Machine Learning focuses on out-of-sample prediction accuracy.\nFor example, given a large set of medical records, the natural instinct of a statistician is to find the indicators of cancer in order assess them via some sort of follow-on genetic study, while a ML practitioner will typically start by building a predictive algorithm to predict who will be diagnosed with cancer. The statistician will perform calculations with \\(p\\)-values, test statistics, and the like to make sure that any discovered relationship is accurate, while the ML practitioner will verify the performance by finding a new (hopefully similar) set of medical records to test algorithm performance.\nClearly, this difference is more one of style than substance: the statistician might see what features are important in the ML model to decide what to investigate, while the ML modeler will use statistical tools to make sure the model is finding something real and not just fitting to noise. In this course, the distinction may be even blurrier as our focus is statistical machine learning - that little niche right on the boundary between the two fields.\nIn brief, “statistics vs ML” is a bit of a meaningless distinction as both fields draw heavily from each other. I tend to say one is doing statistics whenever the end-goal is to better understand something about the real world (ie., the end product is knowledge), while one is doing ML whenever one is building a system to be used in an automated fashion (ie., the end product is software), but definitions vary.1\nIn this course we will use the following taxonomy borrowed from the ML literature:\n\nSupervised Learning: Tasks with a well-defined target variable (output) that we aim to predict\nExamples:\n\nGiven an image of a car running a red-light, read its license plate.\nGiven attendance records, predict which students will not pass a course.\nGiven a cancer patient’s medical records, predict whether a certain drug will have a beneficial impact on their health outcomes.\n\nUnsupervised Learning: Given a whole set of variables, none of which is considered an output, learn useful underling structure.\nExamples:\n\nGiven a set of climate simulations, identify the general trends of a climate intervention.\nGiven a set of students’ class schedules, group them into sets. (You might imagine these sets correspond to majors, but without that sort of “label” (output variable) this is only speculative, so we’re unsupervised)\nGiven a social network, identify the most influential users.\n\n\nThere are other types of learning tasks: e.g. semi-supervised, online, reinforcement, but the Supervised/Unsupervised distinction is the main one we will use in this course.\nWithin Supervised Learning, we can further subdivide into two major categories:\n\nRegression Problems: problems where the response (label) is a real-valued number\nClassification Problems: problems where the response (label) is a category label.\n\nBinary classification: there are only two categories\nMulti-way or Multinomial classification: multiple categories\n\n\nLinear Regression, which we will study more below, is the canonical example of a regression tool for supervised learning.\nAt this point, you can already foresee one of the (many) terminology inconsistencies will will encounter in this course: logistic regression is a tool for classification, not regression. As modern ML is the intersection of many distinct intellectual traditions, the terminology is rarely consistent.2"
  },
  {
    "objectID": "notes/notes01.html#footnotes",
    "href": "notes/notes01.html#footnotes",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some cultural differences that come from ML’s CS history vs Statistics’ “Science-Support” history: notably, ML folks think of (binary) classification while statisticians think of regression as the first task to teach.↩︎\nYou may recall the famous quip about the US and the UK: “Two countries, separated by a common language.”↩︎\nYou might still worry about the randomness of this comparison process: if we had a slightly different sample of our new data, would the better model still look better? You aren’t wrong to worry about this, but we have at a minimum made the problem much easier: now we are just comparing the means of two error distributions - classic \\(t\\)-test stuff - as opposed to comparing two (potentially complex) models.↩︎\nAnecdotally, a Google Data Scientist once mentioned to me that they rarely bother doing \\(p\\)-value or significance calculations. At the scale of Google’s A/B testing on hundreds of billions of ad impressions, everything is statistically significant.↩︎\nI’m being a bit sloppy here. This is more precisely the population test error, not the test set test error.↩︎\nThere are details here about how we measure similarity, but for now we will restrict our attention to simple Euclidean distance.↩︎\nKNN is a non-parameteric method, but not all non-parametric methods require access to the training data at test time. We’ll cover some of those later in the course.↩︎\nIn OLS, the complexity doesn’t make the model ‘wigglier’ in a normal sense - it’s still linear after all - but you can think of it as the additional complexity of a 3D ‘map’ (i.e., a to-scale model) vs a standard 2D map.↩︎\nFigures from HR Chapter 6, Generalization.↩︎\nFor details, see O. Bousquet and A. Elisseeff, Stability and Generalization in Journal of Machine Learning Research 2, pp.499-526.↩︎\nSome regression textbooks advocate a rule that you have 30 data points for every variable in your model. This is essentially guaranteeing that the \\(p/n\\) ratio that controls the generalization of OLS (see above!) stays quite small.↩︎"
  },
  {
    "objectID": "notes/notes01.html#test-and-training-error",
    "href": "notes/notes01.html#test-and-training-error",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Test and Training Error",
    "text": "Test and Training Error\nAs we think about measuring a model’s predictive performance, it becomes increasingly important to distinguish between in-sample and out-of-sample performance, also called training (in-sample) and testing (out-of-sample) performance.\nIn previous courses, you likely have assessed model fit by seeing how well your model fits the data it was trained on: statistics like \\(R^2\\), SSE, SSR in regression or \\(F\\)-tests for model comparison do just this. As you have used them, they are primarily useful for comparison of similar models, e.g., OLS with different numbers of predictor variables. But it’s worth reflecting on this process a bit more: didn’t the model with more predictors always fit the data better? If so, why don’t we always just include all of our predictors?\nOf course you know, the answer is that we want to avoid overfitting. Just because a model fit the data a bit better doesn’t mean it is actually better. If you need 1,000 variables to get a 0.1% reduction in MSE, do you really believe those features are doing much? No!\nYou likely have a sense that a feature needs to “earn its keep” to be worth including in a model. Statisticians have formalized this idea very well in some contexts: quantities like degrees of freedom or adjusted \\(R^2\\) attemps to measure whether a variable provides a statistically significant improvement in performance. These calculations typically rely on subtle calculations involving nice properties of the multivariate normal distribution and ordinary least squares, or things that can be (asymptotically) considered essentially equivalent.\nIn this class, we don’t want to make those sorts of strong distributional and modeling assumptions. So what can we do instead? Well, if we want to see if Model A predicts more accurately than Model B on new data, why don’t we just do that? Let’s get some new data and compare the MSEs of Model A and Model B: whichever one does better is the one that does better.3\nThis is a pretty obvious idea, so it’s worth asking why it’s not the baseline and why statisticians bothered with all the degrees of freedom business to start with. As always, you have to know your history: statistics comes from a lineage of scientific experimentation where data is limited and often quite expensive to get. If you are running a medical trial, you can’t just toss a few hundred extra participants in - this costs money! If you are doing an agricultural experiment, it may take several years to see whether a new seed type actually has higher yield than the previous version. It’s also not clear how one should separate data into training and test sets: if you are studying, e.g., friendship dynamics on Facebook, you don’t have an (obvious) “second Facebook” that you can use to assess model accuracy.\nBy contrast, CS-tradition Machine Learning comes from a world of “internet-scale” where data is plentiful, cheap, and is continuously being collected.4 Not all problems fall in this regime but, as we will see, enough do that it’s worth thinking about what we should do in this scenario. Excitingly, if we don’t demand a full and exhaustive mathematical characterization of a method before we actually apply it, we can begin to explore much more complex and interesting models.\n\n\n\n\n\n\nAdvice\n\n\n\nA good rule of thumb for applied statistical and data science work: begin by asking yourself what you would do if you had access to the whole population (or an infinitely large sample) and then adapt that answer to the limited data you actually have. You always want to make sure you are asking the right question, even if you are only able to give an approximate finite-data answer, rather than giving an ‘optimal’ answer to a question you don’t actually care about.\n\n\nSo, for the first two units of this course, we will put this idea front and center: we will fit our models to a training set and then see how well they perform on a test set. Our goal is to not to find find models which perform well on the test set per se: we really want to find models that perform well on the all future data, not just one test set. But this training/test split will certainly get us going in the right direction.\nLooking ahead, let’s note some of the key questions we will come back to again and again:\n\nIf we don’t have an explicit test set, where can we get one? Can we ‘fake it to make it’?\nWhat types of models have small “test-training” gap, i.e. do about as well on the test and training sets, and what models have a large gap?"
  },
  {
    "objectID": "notes/notes01.html#model-complexity",
    "href": "notes/notes01.html#model-complexity",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Model Complexity",
    "text": "Model Complexity"
  },
  {
    "objectID": "notes/notes01.html#nearest-neighbor-methods",
    "href": "notes/notes01.html#nearest-neighbor-methods",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Nearest Neighbor Methods",
    "text": "Nearest Neighbor Methods\nLet’s now see how complexity plays out for a very simple classifier, \\(K\\)-Nearest Neighbors (KNN). KNN formalizes the intuition of “similar inputs -&gt; similar outputs.” KNN looks at the \\(K\\) most similar points in its training data (“nearest neighbors” if you were to plot the data) and takes the average label to make its prediction.6\n\nlibrary(class) # Provides a KNN function for classification\nargs(knn)\n\nfunction (train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE) \nNULL\n\n\nYou can see here that KNN requires access to the full training set at prediction time: this is different than something like OLS where we reduce our data to a set of regression coefficients (parameters).7\nWe’ll also need some data to play with. For now, we’ll use synthetic data:\n\n#' Make two interleaving half-circles\n#'\n#' @param n_samples Number of points (will be divided equally among the circles)\n#' @param shuffle Whether to randomize the sequence\n#' @param noise Standard deviation of Gaussian noise applied to point positions\n#'\n#' @description Imitation of the Python \\code{sklearn.datasets.make_moons} function.\n#' @return a \\code{list} containining \\code{samples}, a matrix of points, and \\code{labels}, which identifies the circle from which each point came.\n#' @export\nmake_moons &lt;- function(n_samples=100, shuffle=TRUE, noise=0.25) {\n  n_samples_out = trunc(n_samples / 2)\n  n_samples_in = n_samples - n_samples_out\n  \n  points &lt;- matrix( c(\n    cos(seq(from=0, to=pi, length.out=n_samples_out)),  # Outer circle x\n    1 - cos(seq(from=0, to=pi, length.out=n_samples_in)), # Inner circle x\n    sin(seq(from=0, to=pi, length.out=n_samples_out)), # Outer circle y\n    1 - sin(seq(from=0, to=pi, length.out=n_samples_in)) - 0.5 # Inner circle y \n  ), ncol=2) \n  \n  if (!is.na(noise)) points &lt;- points + rnorm(length(points), sd=noise)\n  \n  labels &lt;- c(rep(1, n_samples_out), rep(2, n_samples_in))\n  \n  if (!shuffle) {\n    list(\n      samples=points, \n      labels=labels\n    )\n  } else {\n    order &lt;- sample(x = n_samples, size = n_samples, replace = F)\n    list(\n      samples=points[order,],\n      labels=as.factor(ifelse(labels[order] == 1, \"A\", \"B\"))\n    )\n  }\n}\n\nThis function comes from the clusteringdatasets R package, but the underlying idea comes from a function in sklearn, a popular Python ML library.\nLet’s take a look at this sort of data:\n\nTRAINING_DATA &lt;- make_moons()\n\n\nlibrary(ggplot2)\nlibrary(tidyverse)\ndata.frame(TRAINING_DATA$samples, \n           labels=TRAINING_DATA$labels) |&gt;\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=3)\n\n\n\n\n\n\n\n\nWe can make this a bit more attractive:\n\nMY_THEME &lt;- theme_bw(base_size=20) + theme(legend.position=\"bottom\")\ntheme_set(MY_THEME)\n\n\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %&gt;%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point(size=2) + \n  ggtitle(\"Sample Realization of the Moons Dataset\")\n\n\n\n\n\n\n\n\nMuch better!\nLet’s try making a simple prediction at the point (0, 0):\n\nknn(TRAINING_DATA$samples, \n    cl=TRAINING_DATA$labels, \n    test=data.frame(X1=0, X2=0), k=3)\n\n[1] B\nLevels: A B\n\n\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nDoes this match what you expect from the plot above? Why or why not? The following image might help:\n\nlibrary(ggforce)\ndata.frame(TRAINING_DATA$samples, labels=TRAINING_DATA$labels) %&gt;%\n  ggplot(aes(x=X1, y=X2, color=labels)) + \n  geom_point() + \n  ggtitle(\"Sample Realization of the Moons Dataset\") + \n  geom_circle(aes(x0=0, y0=0, r=0.2), linetype=2, color=\"red4\")\n\n\n\n\n\n\n\n\nWhy is R returning a factor response here? What does that tell us about the type of ML we are doing?\n\n\n\nWe can also visualize the output of KNN at every point in space:\n\nvisualize_knn_boundaries &lt;- function(training_data, k=NULL){\n    xrng &lt;- c(min(training_data$samples[,1]), max(training_data$samples[,1]))\n    yrng &lt;- c(min(training_data$samples[,2]), max(training_data$samples[,2]))\n    \n    xtest &lt;- seq(xrng[1], xrng[2], length.out=101)\n    ytest &lt;- seq(yrng[1], yrng[2], length.out=101)\n    \n    test_grid &lt;- expand.grid(xtest, ytest)\n    colnames(test_grid) &lt;- c(\"X1\", \"X2\")\n    \n    pred_labels = knn(training_data$samples, \n                      cl=training_data$labels, \n                      test_grid, \n                      k=k)\n    \n  ggplot() + \n  geom_point(data=data.frame(TRAINING_DATA$samples, \n                             labels=TRAINING_DATA$labels), \n             aes(x=X1, y=X2, color=labels), \n             size=3) + \n  geom_point(data=data.frame(test_grid, pred_labels=pred_labels), \n             aes(x=X1, y=X2, color=pred_labels), \n             size=0.5) + \n  ggtitle(paste0(\"KNN Prediction Boundaries with K=\", k))\n}\n\nvisualize_knn_boundaries(TRAINING_DATA, k=1)\n\n\n\n\n\n\n\n\nIf we raise \\(K\\), we get smoother boundaries:\n\nvisualize_knn_boundaries(TRAINING_DATA, k=5)\n\n\n\n\n\n\n\n\nAnd if we go all the way to \\(K\\) near to the size of the training data, we get very boring boundaries indeed:\n\nvisualize_knn_boundaries(TRAINING_DATA, k=NROW(TRAINING_DATA$samples)-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhat does this tell us about the complexity of KNN as a function of \\(K\\)?\n\n\n\nIn the terminology we introduced above, we see that increasing \\(K\\) decreases model complexity (wiggliness).\nLet’s now see how training error differs as we change \\(K\\):\n\nTEST_DATA &lt;- make_moons()\n\nTRAINING_ERRORS &lt;- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train &lt;- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TRAINING_DATA$samples, k=k)\n    true_labels_train &lt;- TRAINING_DATA$labels\n    err &lt;- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the training (in-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TRAINING_ERRORS &lt;- rbind(TRAINING_ERRORS, data.frame(k=k, training_error=err))\n}\n\nAt k = 1, the training (in-sample) error of KNN is 0%\nAt k = 2, the training (in-sample) error of KNN is 9%\nAt k = 3, the training (in-sample) error of KNN is 6%\nAt k = 4, the training (in-sample) error of KNN is 4%\nAt k = 5, the training (in-sample) error of KNN is 5%\nAt k = 6, the training (in-sample) error of KNN is 9%\nAt k = 7, the training (in-sample) error of KNN is 5%\nAt k = 8, the training (in-sample) error of KNN is 6%\nAt k = 9, the training (in-sample) error of KNN is 7%\nAt k = 10, the training (in-sample) error of KNN is 4%\nAt k = 11, the training (in-sample) error of KNN is 6%\nAt k = 12, the training (in-sample) error of KNN is 5%\nAt k = 13, the training (in-sample) error of KNN is 6%\nAt k = 14, the training (in-sample) error of KNN is 6%\nAt k = 15, the training (in-sample) error of KNN is 7%\nAt k = 16, the training (in-sample) error of KNN is 9%\nAt k = 17, the training (in-sample) error of KNN is 8%\nAt k = 18, the training (in-sample) error of KNN is 8%\nAt k = 19, the training (in-sample) error of KNN is 9%\nAt k = 20, the training (in-sample) error of KNN is 8%\n\nggplot(TRAINING_ERRORS, aes(x=k, y=training_error)) + \n    geom_point() + \n    ggtitle(\"Training (In-Sample) Error of KNN\")\n\n\n\n\n\n\n\n\nCompare this to the test error:\n\nTESTING_ERRORS &lt;- data.frame()\nfor(k in seq(1, 20)){\n    pred_labels_train &lt;- knn(TRAINING_DATA$samples, cl=TRAINING_DATA$labels, TEST_DATA$samples, k=k)\n    true_labels_train &lt;- TEST_DATA$labels\n    err &lt;- mean(pred_labels_train != true_labels_train)\n    cat(paste0(\"At k = \", k, \", the test (out-of-sample) error of KNN is \", round(100 * err, 2), \"%\\n\"))\n    TESTING_ERRORS &lt;- rbind(TESTING_ERRORS, data.frame(k=k, test_error=err))\n}\n\nAt k = 1, the test (out-of-sample) error of KNN is 9%\nAt k = 2, the test (out-of-sample) error of KNN is 10%\nAt k = 3, the test (out-of-sample) error of KNN is 8%\nAt k = 4, the test (out-of-sample) error of KNN is 6%\nAt k = 5, the test (out-of-sample) error of KNN is 8%\nAt k = 6, the test (out-of-sample) error of KNN is 7%\nAt k = 7, the test (out-of-sample) error of KNN is 7%\nAt k = 8, the test (out-of-sample) error of KNN is 6%\nAt k = 9, the test (out-of-sample) error of KNN is 5%\nAt k = 10, the test (out-of-sample) error of KNN is 5%\nAt k = 11, the test (out-of-sample) error of KNN is 5%\nAt k = 12, the test (out-of-sample) error of KNN is 5%\nAt k = 13, the test (out-of-sample) error of KNN is 5%\nAt k = 14, the test (out-of-sample) error of KNN is 5%\nAt k = 15, the test (out-of-sample) error of KNN is 5%\nAt k = 16, the test (out-of-sample) error of KNN is 6%\nAt k = 17, the test (out-of-sample) error of KNN is 5%\nAt k = 18, the test (out-of-sample) error of KNN is 5%\nAt k = 19, the test (out-of-sample) error of KNN is 5%\nAt k = 20, the test (out-of-sample) error of KNN is 6%\n\nggplot(TESTING_ERRORS, aes(x=k, y=test_error)) + \n    geom_point() + \n    ggtitle(\"Test (Out-of-Sample) Error of KNN\")\n\n\n\n\n\n\n\n\nThe difference between the two is clearer if we put them on the same figure:\n\nERRS &lt;- inner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |&gt;\n        pivot_longer(-k) |&gt;\n        rename(Error=value, Type=name)\nggplot(ERRS, aes(x=k, y=Error, color=Type)) + \n  geom_point() + geom_line()\n\n\n\n\n\n\n\n\nWe notice a few things here:\n\nTraining Error decreases in \\(K\\), with 0 training error at \\(K=1\\) (Why?)\nTest Error is basically always higher than test error\nThe best training error does not have the best test error\n\nWe can also look at the gap between training and test error: this is called generalization error or optimism:\n\ninner_join(TRAINING_ERRORS, TESTING_ERRORS, by=\"k\") |&gt;\n    mutate(optimism=test_error - training_error) |&gt;\n    ggplot(aes(x=k, y=optimism)) + \n    geom_point() + \n    geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nConsider the following questions:\n\nWhat is the relationship between optimism and model complexity\nWhat is the best value of \\(K\\) for this data set?\nHow should we pick the best value of \\(K\\)?\nHow might that change if we increase the number of training samples?\nHow might that change if we increase the number of test samples?"
  },
  {
    "objectID": "notes/notes01.html#generalization-and-model-complexity",
    "href": "notes/notes01.html#generalization-and-model-complexity",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Generalization and Model Complexity",
    "text": "Generalization and Model Complexity\nSo far, we have two useful concepts:\n\nTraining Error\nTest Error\n\nwith a third we can set up a trivial, but surprisingly useful, inequality:\n\\[\\text{Test Error} = \\text{Training Error} + \\text{Generalization Gap}\\]\nHere, the “Generalization Gap” is defined as the difference between the training error and the test error.5 Essentially, the Generalization Gap measures the “optimism” bias obtained by measuring accuracy on the original training data. If the Generalization Gap is large, the model will look much better on the training data then when we actually go to put it into practice. Conversely, if the Generalization Gap is small, the performance we estimate from the training data will continue when we deploy our model.\nThis is a all a bit circular, but it lets us break our overarching goal (small test error) into two parts:\n\nWe want a model with small training error; and\nWe want a model with small generalization gap.\n\nWe can only be confident that we’ll have a small test error when these both of these are true. Again - and just to be clear - having a small training error is important, but it is only necessary and not sufficient to have a small test error.\nWe can simply observe training error, so much of our theoretical analysis focuses on understanding the generalization gap. For now, let’s think about the generalization gap of plain linear regression (OLS). It is not hard to show (and we might show in class next week) that, if the OLS model is true, the expected training MSE is:\n\\[\\mathbb{E}[\\text{Training MSE}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n(y_i - \\sum_{j=1}^p x_{ij}\\hat{\\beta}_j)^2\\right] = \\frac{\\sigma^2(n-p)}{n} = \\sigma^2\\left(1-\\frac{p}{n}\\right)\\]\nHere \\(\\sigma^2\\) is the ‘noise variance’ of the OLS model. (We will review the OLS model in much more detail next week.)\nThis is somewhat remarkable: if we knew the exact true model \\(\\beta_*\\), our MSE would be \\(\\sigma^2\\), but our training MSE is less than that. How can we do better than the optimal and exactly correct model? Overfitting - our OLS fits our training data a bit ‘too well’ and it manages to capture the true signal and the noise. Whatever noise is in the data doesn’t carry into the test set, so we get a bit of overfitting.\nEven a simple model like OLS is vulnerable to a bit of overfitting. It isn’t too big in this context and our formula above actually lets us see how it behaves:\n\nAs \\(n\\to\\infty\\), the amount of overfitting goes down. This is of course the behavior we want: as we get more data, we should stop fitting noise and only fit signal.\nAs \\(p \\to n\\), the amount of overfitting goes up. That is, as we add more features (covariates) to our regression, we expect more over fitting. When we supply OLS with more ‘degrees of freedom’, some of those wind up fitting noise.\n\nThe \\(n\\to\\infty\\) behavior shouldn’t surprise you: the theme of “more data yields better estimation and smaller error” is ubiquitous in statistics. The behavior as \\(p\\) increases may not be something you have seen before. Later in this course, we will actually ask what happens if \\(p &gt; n\\). Clearly, our formula from above can’t hold as it predicts negative MSE! But we’ll get to that later…\nAs \\(p\\) gets larger, OLS is more prone to overfitting. It turns out that this is not a special property of OLS - basically all methods will have this property to one degree or another. While there are many ways to justify this, perhaps the simplest is a story about “complexity”: with more features, and hence more coefficients, OLS becomes a more complex model and more able to fit both the signal and the noise in the training data. Clearly, complexity is not necessarily bad - we want to be able to capture all of the signal in our data - but it is dangerous.\nThis complexity story is one we will follow through the rest of this course. A more complex model is one which is able to fit its training data easily. Mathematically, we actually measure complexity by seeing how well a model fits pure noise: if it doesn’t fit it well at all (because there is no signal!), we can usually assume it won’t overfit on signal + noise. But if it fits pure noise perfectly, it has by definition overfit the training data.\nI like to think of complexity as “superstitious” or “gullibility”: the model will believe (fit to) anything we tell it (training data), whether it is true (signal) or not (noise).\nWe don’t want a model that is too complex, but we also don’t want a model that is too simple. If it can’t fit signal, it is essentially useless. In our metaphor, an overly simple (low complexity) model is like a person who simply doesn’t believe anything at all: they are never tricked, but they also can’t really understand the world.\nLet us now take a quick detour into a flexible family of models and see how performance relates to the complexity story."
  },
  {
    "objectID": "notes/notes01.html#model-complexity-redux",
    "href": "notes/notes01.html#model-complexity-redux",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Model Complexity (Redux)",
    "text": "Model Complexity (Redux)\nLet us understand this using the tools of complexity we discussed above.\n\nFor large \\(K\\), the model became quite simple, barely fit the training data, but did just as well on the training data as the test data (poorly on both). This is indicative of a small generalization gap and a low complexity model.\nFor python \\(K\\), the model became rather complex, fit the training data well, but didn’t perform as well as we would like on the test data. This is indicative of a large generalization gap and a high complexity model.\n\nIn this case, we could see the complexity visually by looking at the decision boundaries (the lines separating predictions of class “A” from class “B”). This isn’t universally true8, but the intuition of “high complexity = more wiggles” is usually pretty good.\nThe classical story of ML and complexity is given by something like this:9\n\nFor now, you can interpret “risk” as “test error” and “empirical risk” as “training error”.\n\n\n\n\n\n\nA more complex story of complexity\n\n\n\n\n\nRecent research has suggested that the story is not quite so simple. Some methods exhibit a “single descent” curve\n\nand some even have a “double descent” curve:\n\nThe story of these curves is still an active topic of research, but it’s pretty clear that very large and very deep neural networks exhibit something like a double descent curve. My own guess is that we’re not quite measuring ‘complexity’ correctly for these incredibly complex models and that the classical story holds if we go measure complexity in the right way, but this is far from a universally held belief.\n\n\n\nSo how do we measure complexity? For OLS, it seems to be proportional to \\(p\\), while for KNN it seems to be inverse to \\(K\\). In this class, we won’t really focus too much on the actual measurements. For most of the methods we study, it is usually pretty clear what drives complexity up or down, even if we can’t quite put a number on it.\n\nStability and Generalization\nIt turns out there is a deep connection between generalization and a suitable notion of “stability”. A model is said to be relatively stable if changes to an input point do not change the output predictions significantly.10\nAt an intuitive level, this makes sense: if the model is super sensitive to individual inputs, it must be very flexible and hence quite complex. (A very simple model cannot be sensitive to all of its inputs.)\nWe can apply this idea to understand some rules-of-thumb and informal practices you might have seen in previous statistics courses:\n\nRegression leverage: Leverage measures how much a single data point can change regression coefficients. This is stability!\nRemoving outliers: we often define outliers as observations which have a major (and assumed corrupting) influence on our inferences. By removing outliers, we guarantee that the resulting inference is not too sensitive to any of the remaining data points. Here, the ‘pre-step’ of outlier removal increases stability (by changing sensitivity of those observations to zero) and hopefully makes our inferences more accurate (better generalization)\nUse of robust statistics, e.g. medians instead of means. These explicitly control the stability of our process.\n\nPerhaps most importantly: this justifies why the large \\(n\\) (big sample) limit seems to avoid overfitting. If our model is fit to many distinct data points, it can’t be too sensitive to any of them. At least, that’s the hope…\nThe sort of statistical models you have seen to date – so-called parametric models – have a ‘stabilizing’ effect. By reducing lots of data to only a few parameters, those parameters (and hence the model output) can’t depend too much on any individual input point.11 This ‘bottleneck’ in the parameter space seems to improve performance.\nOther methods, like \\(1\\)-Nearest-Neighbor, become increasingly more complex as we get more data and do not benefit from this sort of ‘bottleneck’ effect.\nAt this point, you might think that stability and variance are closely related concepts - you are not wrong and we will explore the connection in more detail next week."
  },
  {
    "objectID": "notes/notes01.html#key-terms-and-concepts",
    "href": "notes/notes01.html#key-terms-and-concepts",
    "title": "STA 9890 - Introduction to Machine Learning",
    "section": "Key Terms and Concepts",
    "text": "Key Terms and Concepts\n\nSupervised vs Unsupervised\nRegression vs Classification\nTraining Error\nTest Error\nGeneralization Gap\nComplexity\nOverfitting\nStability\n\\(K\\)-Nearest Neighbors\nDecision Boundaries"
  },
  {
    "objectID": "notes/notes02.html#bias-variance-tradeoff",
    "href": "notes/notes02.html#bias-variance-tradeoff",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff"
  },
  {
    "objectID": "notes/notes02.html#bias-variance-decomposition",
    "href": "notes/notes02.html#bias-variance-decomposition",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Bias-Variance Decomposition",
    "text": "Bias-Variance Decomposition\nIn Report #01, you will show that, under MSE loss, our expected test error can be decomposed as\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\]\nLet’s show how we can analyze these quantities for a KNN regression problem. Here, we’re using the ‘regression’ version of KNN since it plays nicely with MSE.1\n\n# Don't attach the package to avoid weird aliasing issues\nknn.reg &lt;- FNN::knn.reg \nargs(knn.reg)\n\nfunction (train, test = NULL, y, k = 3, algorithm = c(\"kd_tree\", \n    \"cover_tree\", \"brute\")) \nNULL\n\n\nWe also need a ‘true’ function which we’re trying to estimate. Let’s use the following model:\n\\[\\begin{align*}\nX &\\sim \\mathcal{U}([0, 1]) \\\\\nY &\\sim \\mathcal{N}(4\\sqrt{X} + 0.5 * \\sin(4\\pi * X), 0.25)\n\\end{align*}\\]\nThat is, \\(X\\) is uniform on the unit interval and \\(Y\\) is a non-linear function of \\(X\\) plus some Gaussian noise.\nFirst let’s plot \\(X\\) vs \\(\\E[X]\\) - under MSE loss this is our ‘best’ (Bayes-optimal) possible guess.\n\nyfun &lt;- function(x) 4 * sqrt(x) + 0.5 * sinpi(4 * x)\n\nx &lt;- seq(0, 1, length.out=101)\ny_mean &lt;- yfun(x)\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     main=\"True Regression Function\", cex.lab=1.5)\n\n\n\n\n\n\n\n\nTo generate training data from this model, we simply implement the PRNG components:\n\nx_train &lt;- runif(25, 0, 1) # 25 training points\ny_train &lt;- rnorm(25, mean=yfun(x_train), sd=0.5)\n\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\npoints(x_train, y_train)\n\n\n\n\n\n\n\n\nWe have some variance of course, but we can still “squint” to get the right shape of our function. Let’s see how KNN looks on this data.\nWe start with \\(K=1\\):\n\nX_train &lt;- matrix(x_train, ncol=1)\nplot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\nY_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=plot_grid)$pred\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\npoints(x_train, y_train)\n\nlines(plot_grid, Y_hat, col=\"red4\", lwd=2)\n\n\n\n\n\n\n\n\nThis is not a great fit - what happens if we repeat this process may times?\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\nfor(i in seq(1, 20)){\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    test_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    Y_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=test_grid)$pred\n    lines(test_grid, Y_hat, col=\"#FFAA0099\", lwd=0.5)\n}\n\n\n\n\n\n\n\n\nClearly we have some variance!\nIf we repeat with a higher value of \\(K\\), we see far less variance:\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\nfor(i in seq(1, 20)){\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    Y_hat &lt;- knn.reg(train=X_train, y=y_train, k=10, test=test_grid)$pred\n    lines(test_grid, Y_hat, col=\"#FFAA0099\", lwd=0.5)\n}\n\n\n\n\n\n\n\n\nHow well does KNN do on average?\nThat is, if we could repeat this process (infinitely) many times, how well would it recover the true regression function? Let’s try \\(K=1\\) and \\(K=10\\):\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\n\n\nKNN_AVERAGE_PRED_K1 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=1, test=test_grid)$pred\n}))\n\nlines(test_grid, KNN_AVERAGE_PRED_K1, col=\"red4\")\n\n\nKNN_AVERAGE_PRED_K10 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=10, test=test_grid)$pred\n}))\n\nlines(test_grid, KNN_AVERAGE_PRED_K10, col=\"blue4\")\n\n\n\n\n\n\n\n\nWe see here that, on average, KNN with \\(K=1\\) (red) basically gets the function just right - no bias!\nOn the other hand, because KNN with \\(K=10\\) smooths out the function, we see systematic errors (here oversmoothing). That’s some bias.\nSo which is better? - \\(K=1\\) - High variance, but low bias - \\(K=10\\) - Low variance, but high bias\nWe’ll have to look at some test error to see. For now, we’ll generate our test data exactly the same way as we generate our training data:\n\nKNN_K1_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    \n    # Generate from same model as before\n    X_test &lt;- matrix(runif(25, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K1_MSE &lt;- mean(rowMeans(KNN_K1_ERROR^2))\n\nKNN_K10_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    \n    # Generate from same model as before\n    X_test &lt;- matrix(runif(25, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=10, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K10_MSE &lt;- mean(rowMeans(KNN_K10_ERROR^2))\n\ncbind(K1_MSE = KNN_K1_MSE, \n      K10_MSE = KNN_K10_MSE)\n\n       K1_MSE K10_MSE\n[1,] 1.954541 1.46504\n\n\n\\(K=10\\) does better overall!\nBut does it do better everywhere or are some parts of the problem better for \\(K=1\\)?\nNow we’ll be systematic in our test data - spacing it equally on the grid and computing ‘pointwise’ MSE:\n\nKNN_K1_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    # Generate from same model as before\n    test_grid &lt;- seq(0, 1, length.out=101)\n    X_test &lt;- matrix(runif(test_grid, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(test_grid, mean=yfun(X_test), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=1, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K1_MSE &lt;- rowMeans(KNN_K1_ERROR^2)\n\nKNN_K10_ERROR &lt;- replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    test_grid &lt;- seq(0, 1, length.out=101)\n    X_test &lt;- matrix(runif(test_grid, 0, 1), ncol=1) \n    y_test &lt;- matrix(rnorm(101, mean=yfun(X_test), sd=0.5), ncol=1)\n    \n    y_hat &lt;- knn.reg(train=X_train, y=y_train, k=10, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K10_MSE &lt;- rowMeans(KNN_K10_ERROR^2)\n\nplot(KNN_K1_MSE, col=\"blue4\", pch=16, ylim=c(0, 1))\npoints(KNN_K10_MSE, col=\"red4\", pch=16)\n\n\n\n\n\n\n\n\nIt looks like - for this set up at least - \\(K=10\\) is better everywhere but that’s not always the case.\nPlay around with the size of the training data, the noise in the samples, and the data generating function (yfun) to see if you can get different behavior.\nAlso - is \\(K=10\\) really the optimal choice here? What would happen if we changed \\(n\\)?\nSo, now that we have a good sense of (average) test error, can we verify our MSE decomposition?\nRecall \\[\\begin{align*}\n\\E[\\text{MSE}] &= \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} \\\\\n\\text{ where } \\text{Bias}^2 &= \\E\\left[\\left(\\E[\\hat{y}] - \\E[y]\\right)^2\\right] \\\\\n&= \\left(\\E[\\hat{y}] - \\E[y]\\right)^2 \\text{ (Why can I drop the outer expectation?)} \\\\\n\\text{Variance} &= \\E\\left[\\left(\\hat{y} - \\E[y]\\right)^2\\right] \\\\\n\\text{Irreducible Error} &= \\E\\left[\\left(y - \\E[y]\\right)^2\\right]\n\\end{align*}\\]\n(Make sure you understand these definitions and how they work together!)\nLet’s work these out using all the tools we built before.\nFirst, for the bias: - we already have \\(\\E[y]\\) - this is just the yfun we selected - we can compute \\(\\E[\\hat{y}]\\) by running KNN many times and averaging the result\n\nsample_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\nKNN_AVERAGE_PRED_K1 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=1, test=sample_grid)$pred\n}))\n\nKNN_BIAS_K1 &lt;- KNN_AVERAGE_PRED_K1 - yfun(sample_grid)\nplot(sample_grid, KNN_BIAS_K1^2, col=\"red4\", \n     type=\"l\", main=\"Squared Bias of KNN with K=1\")\n\n\n\n\n\n\n\n\nNot too much bias, but things do go a bit off the rails near the end points.\nNext, we can compute variance pointwise:\n\nsample_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\nKNN_VARIANCE_K1 &lt;- rowMeans(replicate(500, {\n    X_train &lt;- matrix(runif(25, 0, 1), ncol=1)\n    y_train &lt;- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    (knn.reg(train=X_train, y=y_train, k=1, test=sample_grid)$pred - KNN_AVERAGE_PRED_K1)^2\n}))\n\nplot(sample_grid, KNN_VARIANCE_K1, col=\"red4\", \n     type=\"l\", main=\"Variance of KNN with K=1\")\n\n\n\n\n\n\n\n\nFor this data at least, the variance term is generally larger than the bias term: this is what we expect with a very flexible (high variance + low bias) model like \\(1\\)-NN.\nFinally, irreducible error is just 0.25 everywhere (recall \\(y \\sim \\mathcal{N}(\\E[y], 0.25)\\)).\n\nKNN_IE &lt;- rowMeans(replicate(500, {\n    sample_grid &lt;- matrix(seq(0, 1, length.out=101), ncol=1)\n    X_test &lt;- matrix(sample_grid, ncol=1)\n    y_test &lt;- matrix(rnorm(sample_grid, mean=yfun(X_test), sd=0.5), ncol=1)\n    y_best_pred &lt;- matrix(yfun(X_test), ncol=1)\n    (as.vector(y_best_pred - y_test))^2\n}))\n\nplot(KNN_IE)\n\n\n\n\n\n\n\n\nPut these together and we see the decomposition in action:\n\nmean(KNN_BIAS_K1)^2 + mean(KNN_VARIANCE_K1) + mean(KNN_IE)\n\n[1] 0.5290861\n\n\nas compared to\n\nmean(KNN_K1_MSE)\n\n[1] 0.5242858\n\n\nVisually,\n\nlibrary(dplyr)\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.1\n\nDECOMP_DATA &lt;- data.frame(\n   sample_grid = sample_grid, \n   KNN_K1_BIAS2 = KNN_BIAS_K1^2, \n   KNN_K1_VARIANCE=KNN_VARIANCE_K1, \n   KNN_K1_IE = KNN_IE, \n   KNN_K1_MSE = KNN_K1_MSE) |&gt; \n   pivot_longer(-sample_grid) |&gt;\n   mutate(Error=value,\n          Type=case_when(\n            name==\"KNN_K1_BIAS2\" ~ \"Bias^2\", \n            name==\"KNN_K1_IE\" ~ \"Irreducible Error\", \n            name==\"KNN_K1_VARIANCE\" ~ \"Variance\", \n            name==\"KNN_K1_MSE\" ~ \"Total Error\")) \n\n\nggplot() + \n  geom_bar(data=DECOMP_DATA |&gt; filter(Type != \"Total Error\"), \n           mapping=aes(x=sample_grid, y=Error, color=Type), \n           stat=\"identity\") + \n  geom_line(data=DECOMP_DATA |&gt; filter(Type == \"Total Error\"), \n            mapping=aes(x=sample_grid, y=Error), \n            color=\"red4\", linewidth=2) + \n  xlab(\"X\") + ylab(\"Test Error\")\n\n\n\n\n\n\n\n\nSo a bit of weirdness at the left end point - but holds up as well as we might expect for \\(N=25\\) samples.\nHow do the relative magnitudes of these terms change as you adjust the parameters of the simulation?"
  },
  {
    "objectID": "notes/notes02.html#linear-algebra-review",
    "href": "notes/notes02.html#linear-algebra-review",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Linear Algebra Review",
    "text": "Linear Algebra Review\nIn mathematics, a vector - random or otherwise - is a fixed-length ordered collection of numbers. When we want to be precise about the size of a vector, we often call it a “tuple”, e.g., a length-three vector is a “triple”, a length-four vector is a “4-tuple”, a length-five vector is a “5-tuple” etc..\nSo, these are all vectors:\n\n\\((3, 4)\\)\n\\((1, 1, 1)\\)\n\\((1, 5, 6, 10)\\)\n\nWhen we want to talk about the set of vectors of a given size, we use the Cartesian product of sets. For two sets, \\(A, B\\), the product set \\(A \\times B\\) is the set of all pairs, with the first element from \\(A\\) and the second from \\(B\\). In mathematical notation,\n\\[A \\times B = \\left\\{(a, b): a \\in A, b \\in B\\right\\} \\]\nThis set-builder notation is read as follows: “The Cartesian Product of \\(A\\) and \\(B\\) is the set of all pairs \\((a, b)\\) such that \\(a\\) is in \\(A\\) and \\(b\\) is in \\(B\\).”\nIf \\(A\\) and \\(B\\) are the same set, we define a Cartesian power as follows:\n\\[A^2 = A \\times A = \\left\\{(a_1, a_2): a_1 \\in A, a_2 \\in A\\right\\}\\]\nNote that even though the sets \\(A\\) and \\(A\\) in this product are the same, the elements in each pair may vary. For example, if \\(A = \\{1, 2, 3\\}\\), we have\n\\[A^2 = \\left\\{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3,2), (3, 3)\\right\\}\\]\nNote that vectors are ordered pairs so \\((2, 1) \\neq (1, 2)\\). From here, it should be pretty easy to convince yourself that set sizes play nicely with Cartesian products:\n\n\\(|A \\times B| = |A| |B|\\)\n\\(|A^k| = |A|^k\\)\n\nThe most common set of vectors we use are those where each element is an arbitrary real number. The set of vectors of length \\(n\\) (\\(n\\)-tuples) is thus \\(\\R^n\\). We rarely mix vectors of different lengths, so we don’t really have a name or notation for the “combo pack” \\(\\R^2 \\cup \\R^3 \\cup \\R^4\\).\nConventionally, vectors are written in bold (if on a computer) or with a little arrow on top (hand written): so a vector called “x” would be denoted \\(\\mathbf{x}\\) or \\(\\vec{x}\\). The elements of \\(\\bx\\) are denoted by subscripts \\(\\bx = (x_1, x_2, \\dots, x_n)\\).\n\nVector Arithmetic\nWe have three arithmetic operations we can perform on general vectors. The simplest is scalar multiplication. A scalar is a non-vector number, i.e., a ‘regular’ number. Scalar multiplication consists of applying the scalar independently to each element of a vector.\n\\[\\alpha \\bx = \\alpha(x_1, x_2, \\dots, x_n) = (\\alpha x_1, \\alpha x_2, \\dots, \\alpha x_n)\\]\nFor example, if \\(\\bx = (3, 4)\\) and \\(\\alpha = 2\\), we have \\[\\alpha \\bx = (6, 8)\\]\nNote that the output of scalar multiplication is always a vector of the same length as the input.\nWe also have the ability to add vectors. This again is performed element-wise.\n\\[\\bx + \\by = (x_1, \\dots, x_n) + (y_1, \\dots, y_n) = (x_1 + y_1, \\dots, x_n + y_n) \\]\nNote that we can’t add vectors of different lengths (recall our “no mixing” rule) and the output length is always the same as the input lengths.\nFinally, we have the vector inner product, defined as:\n\\[\\langle \\bx, \\by \\rangle = x_1y_1 + x_2y_2 + \\dots + x_ny_n \\]\nYou might have seen this previously as the “dot” product. The inner product takes two length-\\(n\\) vectors and gives back a scalar. This structure might seem a bit funny, but as we’ll see below, it’s actually quite useful.\nYou might ask if there’s a “vector-out” product: there is one, with the fancy name “Hadamard product”, but it doesn’t play nicely with other tools, so we don’t use it very much.\nThese tools play nicely together:\n\n\\(\\alpha(\\bx + \\by) = \\alpha \\bx + \\alpha \\by\\) (Distributive)\n\\(\\langle \\alpha \\bx, \\by \\rangle = \\alpha \\langle \\bx, \\by \\rangle\\) (Associative)\n\\(\\langle \\bx, \\by \\rangle = \\langle \\by, \\bx \\rangle\\) (Commutative)\n\n\n\nVector Length and Angle\nWe sometimes want to think about the “size” of a vector, analogous to the absolute value of a scalar. In scalar-world, we say “drop the sign” but there’s not an obvious analogue to a sign for a vector. For instance, if \\(\\bx = (3, -4)\\) is \\(\\bx\\) “positive”, “negative” or somewhere in beetween?\nWe note a trick from scalar-land: \\(|x| = \\sqrt{x^2}\\). We can use the same idea for vectors:\n\\[ \\|\\bx\\| = \\sqrt{\\langle \\bx, \\bx\\rangle} = \\sqrt{\\sum_{i=1}^n x_i^2}\\]\nThis quantity, \\(\\|\\bx\\|\\), is called the norm or length of a vector. We use the double bars to distinguish it from the absolute value of a scalar, but it’s fundamentally the same idea.\nIn \\(\\R^2\\), we recognize this formula for length as the Pythagorean theorem:\n\\[ \\|(3, 4)\\| = \\sqrt{3^2 + 4^2} = \\sqrt{25} = 5 \\]\nWe also sometimes want to define the angle between two vectors. We can define this as:\n\\[ \\cos \\angle(\\bx, \\by) = \\frac{\\langle \\bx, \\by\\rangle}{\\|\\bx\\|\\|\\by\\|} \\Leftrightarrow \\angle(\\bx, \\by) = \\cos^{-1}\\left(\\frac{\\langle \\bx, \\by\\rangle}{\\|\\bx\\|\\|\\by\\|}\\right)\\]\nWe won’t use this formula too often for implementation, but it’s good to have it for intuition. In particular, we note that angle is a proxy for sample correlation, justifying the common vernacular of “orthogonal”, meaning “at right angles”, for “uncorrelated” or “unrelated.”\n\n\nMatrices\nAn \\(n\\)-by-\\(p\\) array of numbers is called a matrix; here the first dimension is the number of rows while the second is the number of columns. So \\[\\bA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix}\\] is a 3-by-2 matrix. We denote the set of matrices of a given size as \\(\\R^{n \\times p}\\), extending slightly the notation we use for vectors.\nIn this course, we will use matrices for two closely-related reasons:\n\nTo organize our data\nTo specify and manipulate linear models\n\nSpecifically, if we have \\(n\\) training samples, each of \\(p\\) covariates (predictors), we will arrange them in a matrix traditionally called \\(\\bX \\in \\R^{n \\times p}\\). Here, each row of \\(\\bX\\) corresponds to an observation. Statisticians tend to call this matrix a design matrix because (historically) it was something designed as part of an experiment; the name got carried forward into the observational (un-designed) setting. You may also hear it called the ‘data matrix’.\nSuppose we have a design matrix \\(\\bX \\in \\R^{n \\times p}\\) and a vector of regression coefficients \\(\\mathbf{\\beta} \\in \\R^p\\). We can use matrix multiplication to make predictions about all observations simultaneously.\nSpecifically, recall that the standard (multivariate) linear model looks like:\n\\[\\hat{y} = \\sum_{i=1}^p x_i\\beta_i\\]\nFor a single observation, this can be written in vector notation as\n\\[\\hat{y} = \\bx^{\\top}\\bbeta\\]\nIf we have \\(n\\) observations, we can stack them in a vector as:\n\\[\\begin{pmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{pmatrix} =\n\\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\\\bx_n^{\\top}\\bbeta \\end{pmatrix}\\]\nWe can connect this with our design matrix \\(\\bX\\) by using the above as a definition of matrix-vector multiplication:\n\\[\\hat{\\by} = \\bX\\bbeta =  \n\\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\\\bx_n^{\\top}\\bbeta \\end{pmatrix}\\]\nHere, matrix multiplication proceeds by taking the inner product of each row with the (column) vector \\(\\bbeta\\). This may feel a bit unnatural at first, but with a bit of practice, it will become second nature. Note that we don’t need a transpose on \\(\\bX\\): the multiplication ‘auto-transposes’ in some sense.\nIt is always helpful to keep track of dimensions when doing matrix multiplication: checking that matrices and vectors have the right size is a useful way to make sure you haven’t done anything too wrong. In general, we can only multiply a \\(m\\)-by-\\(n\\) matrix with a \\(n\\)-by-\\(p\\) matrix and the result is a \\(m\\)-by-\\(p\\) matrix (the \\(n\\)-dimension gets reduced to a scalar by the inner product). Formally, we have something like\n\\[ \\R^{m \\times n} \\times \\R^{n \\times p} \\to \\R^{m \\times p}\\]\nFor purposes of this, you can always think of an \\(n\\)-vector as a \\(n\\)-by-1 “column” matrix, giving us:\n\\[\\underbrace{\\bX}_{\\R^{n \\times p}} \\underbrace{\\bbeta}_{\\R^p} = \\underbrace{\\hat{\\by}}_{\\R^{n \\times 1}}\\]\n\n\nSpectral Properties of Symmetric Matrices\nAn important class of matrices we will consider are symmetric matrices, which are just what the name sounds like. These come up in several key places in statistics, none more important than the covariance matrix, typically denoted \\(\\Sigma\\). Recall that the covariance operator \\(\\mathbb{C}\\) is symmetric (\\(\\mathbb{C}[X, Y] = \\mathbb{C}[Y, X]\\)) so the covariance matrix of a random vector turns out to be symmetric as well.\nAnother common source of symmetric matrices is when a matrix is multiplied by its transpose: you should convince yourself that \\(\\bX^{\\top}\\bX \\in \\R^{p \\times p}\\) is a symmetric matrix."
  },
  {
    "objectID": "notes/notes02.html#introduction-to-convex-optimization",
    "href": "notes/notes02.html#introduction-to-convex-optimization",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Introduction to Convex Optimization",
    "text": "Introduction to Convex Optimization\nIn this course, we will frequently want to find the minimizer of certain functions. Typically, these will arise as ERMs and the function will take a \\(p\\)-vector of inputs, e.g. regression coefficients, and produce a scalar loss output. Let us generally call this function \\(f: \\R^p \\to \\R\\).\nIn some circumstances, we can take the derivative of \\(f\\), typically called the gradient in this context, and set it equal to zero. For example, suppose we want to minimize an expression of the form:\n\\[f(\\bx) = \\frac{1}{2}\\bx^{\\top}\\bA\\bx + \\bb^{\\top}\\bx + c\\]\nwhere \\(\\bA\\) is a symmetric strictly positive-definite matrix, \\(\\bb\\) is an arbitrary \\(p\\)-vector, and \\(c\\) is a constant. Taking the gradient, we find\n\\[ \\frac{\\partial f}{\\partial \\bx} = \\bA\\bx + \\bb\\]\nWe set this to zero and find a crticial point at:\n\\[\\begin{align*}\n\\mathbf{0} &= \\bA\\bx + \\bb \\\\\n- \\bb &= \\bA\\bx \\\\\n\\implies \\bx &= -\\bA^{-1}\\bb\n\\end{align*}\\]\nassuming that \\(\\bA\\) is invertible. (Here, invertibility is implied by assuming \\(\\bA\\) is strictly positive-definite.) As usual, we are not quite done here, as we must also check that this is a minimizer and not a maximizer or a saddle point. To do so, we take the second derivative to find\n\\[ \\frac{\\partial}{\\partial \\bx}\\frac{\\partial f}{\\partial \\bx} = \\bA\\]\nIn this multivariate context, we need the second derivative, a.k.a the Hessian, to be strictly positive-definite to guarantee that we have found a minimizer and this is indeed exactly what we assumed above. As discussed above, ‘definiteness’ plays the role of sign for many applications of matrix-ness. Here, by assuming strictly positive definite, we are essentially treating the matrix as strictly positive (\\(&gt;0\\)), which is exactly the condition we need to guarantee a minimizer in the scalar case as well.\nHence, we have that the one minimizer of \\(f\\) is found at \\(\\bx_* = \\bA^{-1}\\bb\\). Compare this to the scalar case of minimizing a quadratic \\(\\frac{1}{2}ax^2 + bx + c\\) with minimizer at \\(x = -b/a\\).2\nThe above analysis works well, but it is essentially the only minimization we will be able to do in closed form in this course.3\nFor other functions, we will need to apply optimization: the mathematical toolkit for finding minimizers (or maximizers) of functions. Fortunately for us, many of the methods we begin this course with fall in the realm of convex optimization, a particularly nice branch of optimization.\nConvex Optimization refers to the problem of minimizing convex functions over convex sets. Let’s define both of these:\n\nA convex function is one which satisfies this inequality at all points: \\[f(\\lambda \\bx + (1-\\lambda)\\by) \\leq \\lambda f(\\bx) + (1-\\lambda) f(\\by)\\]\nfor any \\(\\bx, \\by\\) and any \\(\\lambda \\in [0, 1]\\).\nThis definition is a bit non-intuitive, but it basically implies that we have a “bowl-like” function. This definition captures the idea that the actual function value is always less than we might get from linear interpolation. A picture is helpful here:\n\n\n\n\n\n\n\n\n\n\nHere, we see that the actual value of the function (blue dot) is less than what we would get if we interpolated the two red points (red line).\nAn alternative definition is that if \\(f\\) is twice-differentiable, its Hessian (2nd derivative matrix) is positive semi-definite.\n\nA convex set is one that allows us to look “between” points. Specifically, a set \\(\\mathcal{C} \\subseteq \\R^p\\) is convex if \\[\\bx, \\by \\in \\mathcal{C} \\implies \\lambda \\bx + (1-\\lambda)\\by \\in \\mathcal{C}\\]\nfor all \\(\\bx, \\by \\in \\mathcal{C}\\) and any \\(\\lambda \\in [0, 1]\\).\n\nClearly these are related by the idea of looking “between” two points:\n\nConvex functions guarantee that this will produce something lower than naive interpolation; and\nConvex sets guarantee that the midpoint will be “allowable”.\n\nTo tie these together, note another alternative characterization of a convex function: one whose epigraph (the area above the curve in the plot) is a convex set.\nFor optimization purposes, these two properties imply a rather remarkable fact:\nIf \\(\\bx_0\\) is a local minimum of \\[\\min_{\\bx \\in \\mathcal{C}} f(\\bx)\\], then it is a global minimum.\nThis is quite shocking: if we find a point where we can’t improve by going in any direction, we are guaranteed to have found a global minimum and no point could be better. (It is possible to have multiple minimizers however: consider \\(f(\\bx) = 0\\). Any choice of \\(\\bx\\) is a global minimizer.) This lets us turn the “global” search problem into a “local” one.\nOf course, this only helps us if we can find a local minimizer of \\(f\\). How might we do so? Let’s recall a basic calculus idea: the gradient of a function is a vector that points in the direction of greatest increase (the “steepest” uphill). So if we go in the opposite direction of the gradient, we actually will go “downhill”. In fact, this is basically all we need to start applying gradient descent.\nGradient Descent: Given a convex function \\(f\\):\n\nInitialize at (arbitrary) starting point \\(\\bx_0\\).\nInitialize step counter \\(k=0\\).\nRepeat until convergence:\n\nCompute the gradient of \\(f\\) at \\(\\bx_k\\): \\(\\left.\\nabla f\\right.|_{\\bx=\\bx_k}\\)\nSet \\(\\bx_{k+1} = \\bx_k - c \\left.\\nabla f\\right.|_{\\bx=\\bx_k}\\)\n\n\nRepeated infinitely many times, this will converge to a local, and hence global, minimizer of \\(f\\). There are many variants of this basic idea4, mostly related to selecting the optimal step size \\(c\\), but this is the most important algorithm in convex optimization and it is important to understand it deeply.\nWe can apply it here to the 3D function \\[f(\\bx) = (x_1-2)^2 + (x_2-3)^2 + (x_3 - 4)^2.\\] Clearly, we can see that the minimizer has to be at \\((2, 3, 4)\\), but our algorithm won’t use that fact.\nBefore we can implement this algorithm, we need the gradient, which is given by\n\\[f(\\bx) = \\left\\|\\bx - (2,3,4)^{\\top}\\right\\|_2^2 \\implies \\nabla f = 2[\\bx - (2, 3, 4)^{\\top}]\\]\n\nx &lt;- matrix(c(0, 0, 0), ncol=1) # We want to work exclusively with column vecs\nconverged &lt;- FALSE\nc &lt;- 0.001 # Small step size\nf &lt;- function(x) sum((x - c(2, 3, 4))^2)\ngrad &lt;- function(x) 2 * (x - matrix(c(2, 3, 4), ncol=1))\n\nwhile(!converged){\n    g &lt;- grad(x)\n    x_new &lt;- x - c * g\n    \n    if(sum(abs(x - x_new)) &lt; 1e-5){\n        converged &lt;- TRUE\n    }\n    \n    x &lt;- x_new\n}\n\nx\n\n         [,1]\n[1,] 1.998893\n[2,] 2.998340\n[3,] 3.997787\n\n\nWe don’t get the exact minimizer, but we certainly get something ‘close enough’ for our purposes. In Report #01, you will use gradient descent and some variants to study the least squares problem.\nOften, we will want to see how the value of \\(f(\\bx_k)\\) changes over the course of the optimization. We expect that it will go down monotonically, but it may not be worth continuing the optimization if we have reached a point of ‘diminishing returns.’ You can do this by hand (evaluating \\(f\\) after each update and storing the results), but many optimizers will often track this automatically for you: e.g. TensorBoard.5"
  },
  {
    "objectID": "notes/notes02.html#footnotes",
    "href": "notes/notes02.html#footnotes",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Bias-Variance decomposition (and tradeoff) holds approximately for other loss functions, though the math is only this nice for MSE.↩︎\nNote that we can only minimize this quadratic in the case where \\(a\\) is strictly positive so the parabola is upward facing. This is the scalar equivalent of the strict positive-definiteness condition we put on \\(\\bA\\).↩︎\nOLS and some basic variants (ridge regression) can be written in this ‘quadratic’ style. Can you see why?↩︎\nFor instance, of the 14 optimization algorithms included in base pytorch, all but one are advanced versions of gradient descent. The exception is LBFGS which attempts to (approximately) use both the gradient and the Hessian (second derivative); computing the Hessian is normally quite expensive, so LBFGS uses some clever tricks to approximate the Hessian.↩︎\nModern ML toolkits like pytorch or TensorFlow are (at heart) fancy systems to do two things automatically that we did ‘by hand’ in this example:\n\nCompute gradients via a process known as ‘back-propogation’ or ‘autodiff’\nImplement (fancy) gradient descent\n\n↩︎"
  },
  {
    "objectID": "notes/notes02.html",
    "href": "notes/notes02.html",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "",
    "text": "\\[\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}}\\]"
  },
  {
    "objectID": "notes/notes02.html#ordinary-least-squares",
    "href": "notes/notes02.html#ordinary-least-squares",
    "title": "STA 9890 - Review of OLS, Linear Algebra, and Convex Optimization",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\nSuppose we want to fit a linear model to a given data set (for any of the reasons we discuss in more detail below): how can we choose which line to fit? (There are infinitely many!)\nSince our goal is minimizing test error, and we hope training error is at least a somewhat helpful proxy for training error, we can pick the line that minimizes training error. To do this, we need to commit to a specific measure of error. As the name Ordinary Least Squares suggests, OLS uses (mean) squared error as its target.\nWhy is MSE the right choice here? It turns out that MSE is very nice computationally, but the reason is actually more fundamental: given a random variable \\(Z\\), suppose we want to minimize the \\((Z - \\mu)\\) for some \\(\\mu\\): it can be shown that\n\\[ \\E[Z] = \\text{argmin}_{\\mu \\in \\R} \\E[(Z - \\mu)^2]\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nProve this for yourself.\nHint: Differentiate with respect to \\(\\mu\\).\n\n\n\nThat is, the quantity that minimizes the MSE is the mean. So when we fit a line to some data by OLS, we are implicitly trying to fit to \\(\\E[y]\\) - a very reasonable thing to do!\nSpecifically, given training data \\(\\mathcal{D} = \\{(\\bx_i, y_i)\\}_{i=1}^n\\) where each \\(\\bx_i \\in \\R^p\\), OLS finds \\(\\bbeta \\in \\R^p\\) such that \\[ \\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^p \\beta_jx_j\\right)^2 = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\bx^{\\top}\\bbeta\\right)^2\\]\nSome possibly new notation here:\n\n\\(\\bx^{\\top}\\bbeta\\) is the (inner) product of two vectors: defined as the sum of their elementwise products.\nOptimization problems:\n\n\\[\\hat{x} = \\text{argmin}_{x \\in \\mathcal{C}} f(x)\\]\n\\[f_* = \\text{min}_{x \\in \\mathcal{C}} f(x)\\]\n\nThese problems say: find the value of \\(x\\) in the set \\(\\mathcal{C}\\) that minimizes the function \\(f\\). \\(\\text{argmin}\\) says ‘give me the minimizer’ while \\(\\min\\) says give me the minimum value’. These are related by \\(f_* = f(\\hat{x})\\)\nThe function \\(f\\) is called the objective; the set \\(\\mathcal{C}\\) is called the constraint set.\n\nOrdinary Least Squares refers to the use of an MSE objective without any additional constraints.\nNote the general structure of our approach here:\n\nDefine the loss we care about\nSet up an optimization problem to minimize loss on the training set\nSolve optimization problem\n\nML folk call this empirical risk minimization (ERM) since we’re minimizing the risk (average loss) on the data we can see (the training data). Statisticians call this \\(M\\)-estimation, since it defines an estimator by Minimization of a measure of ‘fit’. Whatever you call it, it’s a very useful ‘meta-method’ for coming up with ML methods.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\n\nHow does this compare with Maximum Likelihood Estimation?\nWe set up this ERM method using mean squared error - what happens with other errors? Specifically, formulate this ERM for\n\nmean absolute error\nmean percent error\n\nand compare to OLS.\n\n\n\n\nSo far we’ve set up OLS as \\[ \\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\bx^{\\top}\\bbeta\\right)^2\\]\nWe can clean this up to make additional analysis easier:\n\nLet \\[\\by = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}\\] be the (vertically stacked) vector of responses.\nNext look at our predictions: \\[\\hat{\\by} = \\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\bx_n^{\\top}\\bbeta \\end{pmatrix} = \\begin{pmatrix} \\bx_1^{\\top} \\\\ \\bx_2^{\\top} \\\\ \\vdots \\\\ \\bx_n^{\\top} \\end{pmatrix}\\bbeta = \\bX\\bbeta\\]\n\nHence, OLS is just \\[\\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n} \\|\\by - \\bX\\bbeta\\|_2^2\\] Here \\(\\|\\cdot\\|_2^2\\) is the (squared Euclidean or \\(L_2\\)) norm of a vector: defined by \\(\\|\\bz\\|_2^2 = \\sum z_i^2\\)\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\n\nDo we actually need the \\(1/n\\) part?\nWhy is this called linear?\n\n\n\n\nWe’ve formulated OLS as \\[\\hat{\\beta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n} \\|\\by - \\bX\\bbeta\\|_2^2.\\] In order to solve this, it will be useful to modify it slightly to \\[\\hat{\\beta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{2} \\|\\by - \\bX\\bbeta\\|_2^2\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhy is this ok to do?\n\n\n\nYou will show in Report #01 that the solution is given by\n\\[\\hat{\\beta} = (\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\by\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nProve this for yourself. What conditions are required on \\(\\bX\\) for the inverse in the above expression to exist?\n\n\n\nWith this estimate, our in-sample predictions are given by:\n\\[\\hat{\\by} = \\bX\\hat{\\bbeta} = \\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\by \\]\nThis says that our predictions are, in some sense, just a linear function of the original data \\(\\by\\). The matrix\n\\[\\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top} = \\bH\\]\nis sometimes called the ‘hat’ matrix because it puts a hat on \\(\\by\\) (\\(\\hat{\\by}=\\bH\\by\\)).\n\\(\\bH\\) can be shown to be a special type of matrix called a projector, meaning that it has eigenvalues all 0 or 1. For the hat matrix specifically, we can show that eigenvalues of \\(\\bH\\) are \\(p\\) zeros and \\(n - p\\) ones.\nThis in turn implies that it is idempotent, meaning \\(\\bH^2 = \\bH\\bH = \\bH\\). (To show this, simply express \\(\\bH\\) in terms of its eigendecomposition.) We can use this property to finally justify the in-sample MSE of OLS we have cited several times.\nThe in-sample MSE is given by:\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{n}\\|\\by - \\hat{\\by}\\|_2^2 \\\\\n           &= \\frac{1}{n}\\left\\|\\by - \\bH\\by\\right\\|_2^2 \\\\\n           &= \\frac{1}{n}(\\by - \\bH\\by)^{\\top}(\\by - \\bH\\by) \\\\\n           &= \\frac{1}{n}(\\by^{\\top} - \\by^{\\top}\\bH)(\\by - \\bH\\by) \\\\\n           &= \\frac{\\by^{\\top}\\by - \\by^{\\top}\\bH\\by - \\by^{\\top}\\bH\\by + \\by^{\\top}\\bH\\bH\\by}{n} \\\\\n           &= \\frac{\\by^{\\top}\\by - \\by^{\\top}\\bH\\by}{n} \\\\\n           &= \\frac{\\by^{\\top}(\\bI - \\bH)\\by}{n} \\\\\n\\implies \\E[\\text{MSE}] &= \\E\\left[\\frac{\\by^{\\top}(\\bI - \\bH)\\by}{n}\\right] \\\\\n&= \\frac{1}{n}\\E\\left[\\by^{\\top}(\\bI - \\bH)\\by\\right]\n\\end{align*}\\]\nTo finish this, we need to know that the expectation of a symmetric quadratic form satisfies\n\\[\\bx \\sim (\\mu, \\Sigma) \\implies \\E[\\bx^{\\top}\\bA\\bx] = \\text{Tr}(\\bA\\Sigma) + \\mu^{\\top}\\bA\\mu\\]\nfor any random vector \\(\\bx\\) with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\).\nTo apply this above, we note \\(\\by \\sim (\\bX\\beta_*, \\sigma^2 \\bI)\\), so we get\n\\[\\begin{align*}\n\\E\\left[\\by^{\\top}(\\bI - \\bH)\\by\\right] &= \\text{Tr}((\\bI - \\bH) \\sigma^2 \\bI) + (\\bX\\bbeta_*)^{\\top}(\\bI - \\bH)(\\bX\\bbeta_*) \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}\\bX^{\\top}(\\bI - \\bH)\\bX\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bH\\bX)\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\bX)\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bX)\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top} \\mathbf{0}\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) \\\\\n&= \\sigma^2 (\\text{Tr}(\\bI) - \\text{Tr}(\\bH))\n\\end{align*}\\]\nRecall that the trace is simply the sum of the eigenvalues, so this last term becomes \\(\\sigma^2(n - p)\\), finally giving us:\n\\[\\E[\\text{MSE}] = \\frac{\\sigma^2(n-p)}{n}  = \\sigma^2\\left(1 - \\frac{p}{n}\\right)\\]\nWhew! That was a lot of work! But can you imagine how much more work this would have been without all of these matrix tools?\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nMake sure you can justify every step in the derivation above. This is a particularly long computation, but we will use the individual steps many more times in this course."
  },
  {
    "objectID": "notes/notes03.html",
    "href": "notes/notes03.html",
    "title": "STA 9890 - Regularized Regression",
    "section": "",
    "text": "\\[\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}} \\newcommand{\\V}{\\mathbb{V}} \\newcommand{\\argmin}{\\text{arg min}}\\]\nLast week we showed the following:\nWe begin this week by asking if we can do better than OLS. To keep things simple, we begin by assuming we are under a linear DGP (so no ‘model error’) but that’s only a mathematical niceity. It’s not something you should always assume - in fact, it is really more important to think about how models do on non-linear DGPs. As we will see, it may still be useful to use linear models…\nBecause OLS is BLUE under our assumptions, we know that we need to relax one or more of our assumptions to beat it. For now, we will focus on relaxing the U - unbiasedness; non-linear methods come later in this course.\nRecalling our decomposition:\n\\[\\text{MSE} = \\text{Bias}^2 + \\text{Variance}\\]\nOur gambit is that we can find an alternative estimator with a bit more bias, but far less variance. Before we attempt to do so for linear regression, let’s convince ourselves this is possible for a much simpler problem - estimating means."
  },
  {
    "objectID": "notes/notes03.html#estimating-normal-means",
    "href": "notes/notes03.html#estimating-normal-means",
    "title": "STA 9890 - Regularized Regression",
    "section": "Estimating Normal Means",
    "text": "Estimating Normal Means\nSuppose we have data from a distribution \\[X_i \\buildrel \\text{iid} \\over \\sim\n\\mathcal{N}(\\mu, 1)\\] for some unknown \\(\\mu\\) that we seek to estimate. Quite reasonably, we might use the sample mean \\[\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\] to estimate \\(\\mu\\). Clearly, this is an unbiased estimator and it has variance given by \\(1/n\\), which isn’t bad. In general, it’s pretty hard to top this.\nWe can verify all of this empirically:\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(dplyr)\n\n\ncompute_mse_sample_mean &lt;- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R &lt;- replicate(1000, {\n        X &lt;- rnorm(n, mean=mu, sd=1)\n        mean(X)\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nMU_GRID &lt;- seq(-5, 5, length.out=501)\nN &lt;- 10\n\nSIMRES &lt;- map(MU_GRID, compute_mse_sample_mean, n=N) |&gt; list_rbind()\n\nOur bias is essentially always zero:\n\nggplot(SIMRES, aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(slope=0, \n                intercept=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Constant Zero Bias of Sample Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nSimilarly, our bias is small, and constant. Specifically, it is around \\(1/n\\) as predicted by standard theory:\n\nggplot(SIMRES, aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Variance of Sample Mean\") + \n    ggtitle(\"Constant Variance of Sample Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nAs expected, the MSE is the sum of bias and variance, so it’s basically just variance here:\n\nggplot(SIMRES, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Sample Mean MSE \") + \n    ggtitle(\"Constant MSE of Sample Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nSo far, it looks like the sample mean is hard to beat. In particular, this curve is But… what if we know, e.g., that \\(\\mu\\) is positive. We might still use the sample mean, but with the additional step that we set it to zero if the sample mean looks negative. That is, our new estimator is \\[\\hat{\\mu} = (\\overline{X}_n)_+ \\text{ where } z_+ = \\begin{cases} z & z &gt; 0 \\\\ 0 & z \\leq 0 \\end{cases}\\]\nThe \\((\\cdot)_+\\) operator is known as the positive-part. How does this \\(\\hat{\\mu}\\) do?\n\npospart &lt;- function(x) ifelse(x &gt; 0, x, 0)\ncompute_mse_positive_mean &lt;- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the positive part of the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R &lt;- replicate(1000, {\n        X &lt;- rnorm(n, mean=mu, sd=1)\n        pospart(mean(X))\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nSIMRES_POSPART &lt;- map(MU_GRID, compute_mse_positive_mean, n=N) |&gt; \n    list_rbind()\n\n\nggplot(SIMRES_POSPART, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nNot surprisingly, we do very poorly if we are estimating a negative \\(\\mu\\) but we assume it is positive. Let’s zoom in on the area near 0 however.\n\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= -0.5, \n           mu &lt;= 1) |&gt;\nggplot(aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nInteresting! For some of these values, we do better than the sample mean.\nIn particular, we do better in the following scenario:\n\nTrue mean is positive\nSample mean is negative\nPositive part of sample mean is zero, so closer than pure sample mean\n\nThe probability of step 2 (sample mean is negative) is near zero for large \\(\\mu\\), but for \\(\\mu\\) in the neighborhood of zero, it can happen.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nReview Question: As a function of \\(\\mu\\), what is the probability that \\(\\overline{X}_n\\) is negative? You can leave your answer in terms of the standard normal CDF \\(\\Phi(\\cdot)\\).\n\n\n\nThis is pretty cool. We have made an additional assumption and, when that assumption holds, it helps us or, worst case, doesn’t really hurt us much. Of course, when the assumption is wrong (\\(\\mu &lt; 0\\)), we do much worse, but we can’t really hold that against \\((\\overline{X}_n)_+\\).\nLooking more closely, we can look at the bias of \\((\\overline{X}_n)_+\\):\n\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= -0.5, \n           mu &lt;= 1) |&gt;\nggplot(aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Bias of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nWe see here that our improvement came at the cost of some bias, particularly in the \\(\\mu \\in [0, 1]\\) range. But for that bias, we see a good reduction in variance:\n\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= -0.5, \n           mu &lt;= 1) |&gt;\nggplot(aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(Variance^2)) + \n    ggtitle(\"Non-Constant Variance of Positive Part Mean Estimator\") + \n    theme_bw()\n\n\n\n\n\n\n\n\nHere, we see that the variance is less than \\(1/n\\) from \\(\\mu \\approx 0.5\\) and down. Let’s plot variance and bias against each other:\n\nlibrary(geomtextpath)\nSIMRES_POSPART |&gt;\n    filter(mu &gt;= 0, \n           mu &lt;= 1) |&gt;\n    ggplot(aes(x=bias, y=variance)) + \n       geom_point() + \n       geom_textline(aes(x=bias, y=1/n - bias),\n                     lty=2, color=\"red4\", \n                     label=\"Breakeven\") + \n       ylim(c(0, 0.1)) + \n       theme_bw() + \n       xlab(expression(\"Bias\"^2)) + \n       ylab(\"Variance\") + \n       ggtitle(\"Bias-Variance Tradeoff for Positive Part Sample Mean\")\n\n\n\n\n\n\n\n\nHere, all of the values of \\(\\mu\\) corresponding to points below this line are points where the positive part estimator does better than the standard sample mean.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nSee if you can compute the bias and variance of \\((\\overline{X}_n)_+\\) in closed form. The moments of the Rectified Normal Distribution may be of use.\n\n\n\nOk - now let’s start to generalize. Clearly, the first step is to change the ‘positive’ assumption. The easiest generalization is to restrict \\(\\mu\\) to an interval \\([a, b]\\). In this case, it makes sense to replace the positive part operator with a ‘clamp’ operator:\n\\[(x)_{[a, b]} = \\begin{cases} a & x \\leq a \\\\ x & x\\in(a, b) \\\\ b & x \\geq b \\end{cases}\\]\nThe positive part operator we applied before is \\((x)_+ = (x)_{[0, \\infty)}\\).\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nExtend the simulation above to characterize the estimation performance (bias and variance) of \\((\\overline{X}_n)_[a, b]\\).\n\n\n\nA particularly useful version of this bound is taking \\((\\overline{X}_n)[-\\beta, +\\beta]\\); that is, we don’t know the sign of \\(\\mu\\), but we know it is less than \\(\\beta\\) in magnitude. This is not an improbable assumption - we often have a good sense of the plausible magnitude of a parameter (Bayesian priors anyone?) - but it feels a bit ‘firm’. Can we relax this sort of assumption? We want \\(\\mu\\) to be ‘not too big’, but we’re willing to go big if the data takes us there.\nWe can implement this as follows:\n\\[\\hat{\\mu}_{\\alpha} = \\frac{\\overline{X}_n}{1+\\alpha}\\]\nClearly, setting \\(\\alpha = 0\\) gets us back to the standard sample mean. Can this be better than the sample mean? Let’s do the calculations by hand. First we note that \\(\\E[\\hat{\\mu}_{\\alpha}] = \\frac{\\mu}{1+\\alpha}\\) giving a bias of \\[\\text{Bias} = \\E[\\hat{\\mu}_{\\alpha}] - \\mu = \\mu\\left(1 - \\frac{1}{1+\\alpha}\\right) \\implies \\text{Bias}^2 = \\mu^2\\left(1 - \\frac{1}{1+\\alpha}\\right)^2\\] and\n\\[\\text{Variance} = \\V[\\hat{\\mu}_{\\alpha}] = \\frac{1}{(1+\\alpha)^2}\\V[\\overline{X}_n] = \\frac{1}{n(1+\\alpha)^2}\\]\nso the total MSE is given by\n\\[\\begin{align*}\n\\text{MSE} &= \\E[(\\hat{\\mu}_{\\alpha} - \\mu)^2] \\\\\n           &= \\text{Bias}^2 + \\text{Variance} \\\\\n           &= \\mu^2\\left(1 - \\frac{1}{1+\\alpha}\\right)^2 + \\frac{1}{n(1+\\alpha)^2}\n\\end{align*}\\]\nFor suitable \\(\\alpha, n\\) this can be less than the standard MSE of \\(1/n\\). For instance, at \\(\\mu = 5\\) and \\(n = 10\\),\n\nshrunk_mean_mse &lt;- function(mu, n, alpha){\n    mu^2 * (1 - 1/(1+alpha))^2 + 1/(n * (1+alpha)^2)\n}\n\nshrunk_mean_mse(5, 10, 1e-4)\n\n[1] 0.09998025\n\n\nNot great - but an improvement! It’s actually pretty hard to beat the sample mean with an estimator of this form in the univariate case, but it can be incredibly useful in more general settings."
  },
  {
    "objectID": "notes/notes03.html#james-stein-estimation-of-multivariate-normal-means",
    "href": "notes/notes03.html#james-stein-estimation-of-multivariate-normal-means",
    "title": "STA 9890 - Regularized Regression",
    "section": "James-Stein Estimation of Multivariate Normal Means",
    "text": "James-Stein Estimation of Multivariate Normal Means\nTODO"
  },
  {
    "objectID": "notes/notes03.html#ridge-regression",
    "href": "notes/notes03.html#ridge-regression",
    "title": "STA 9890 - Regularized Regression",
    "section": "Ridge Regression",
    "text": "Ridge Regression\nAbove, we saw that \\((\\overline{X}_n)_{[-\\beta, \\beta]}\\) could outperform \\(\\overline{X}_n\\) if the true parameter is ‘not too big’. Can we extend this idea to regression?\nRecall that we formulated OLS as:\n\\[\\hat{\\bbeta}_{\\text{OLS}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|\\]\nWe can apply a ‘clamp’ as:\n\\[\\hat{\\bbeta}_{\\tau-\\text{Clamped}} = \\argmin_{\\bbeta \\in \\R: \\|\\bbeta\\| &lt; \\tau} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\| \\leq {\\tau}\\]\nHere, we have some choice in measuring the ‘size’ of \\(\\hat{\\bbeta}\\): in fact, we can theoretically use any of our \\(\\ell_p\\)-norms. As with the squared loss, it turns out to be mathematically easiest to start with the \\(\\ell_2\\) (Euclidean) norm. This gives us the ridge regression problem:\n\\[\\hat{\\bbeta}_{\\text{Ridge}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2 \\leq {\\tau} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2^2 \\leq {\\tau}^2\\]\nBefore we discuss solving this, let’s confirm it is convex. Recall that for a problem to be convex, it needs to have a convex objective and a convex feasible set. Clearly the objective here is convex - we still have the MSE loss from OLS unchanged. So now we need to convince ourselves that \\(\\|\\bbeta\\|_2^2 \\leq \\tau^2\\) defines a convex set. We can look at this in 2D first:\n\\[\\|\\bbeta\\|_2^2 \\leq \\tau^2 \\implies \\beta_1^2 + \\beta_2^2 \\leq \\tau\\]\nBut this is just the equation defining a circle and its interior (in math speak a 2D ‘ball’) so it has to be convex!\nIn fact, constraints of the form\n\\[\\|\\bbeta\\|_p \\leq \\tau \\Leftrightarrow \\|\\bbeta\\|_p^2 \\leq \\tau^p\\]\nare convex for all \\(\\ell_p\\) norms (\\(p \\geq 1\\)). We will use this fact many times in this course. The sets that this constraint defines are called \\(\\ell_p\\) ‘balls’ by analogy with the \\(\\ell_2\\) figure.\n\n\n\n\n\n\n\n\n\nSo our ridge problem\n\\[\\hat{\\bbeta}_{\\text{Ridge}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2^2 \\leq {\\tau}^2\\] is indeed convex. While we can solve this without too much trouble, it turns out to be easier to use an optimization trick known as Lagrange Multipliers to change this to something easier to solve.\nThe Method of Lagrange Multipliers lets us turn constrained optimization problems that look like:\n\\[\\argmin_x f(x) \\text{ such that } g(x) \\leq \\tau\\]\ninto unconstrained problems like:\n\\[\\argmin_x f(x) + \\lambda g(x)\\]\nHere, the constant \\(\\lambda\\) is known as the Lagrange multiplier. Instead of forcing \\(g(x)\\) to be bounded, we simply penalize any \\(x\\) that makes \\(g(\\cdot)\\) large. For large enough penalties (large \\(\\lambda\\)), we will eventually force \\(g(x)\\) to be small enough that we satisfy the original constraint. In fact, there is a one-to-one relationship between \\(\\tau\\) and \\(\\lambda\\), but it’s usually not possible to work it out in any meaningful useful manner; all we can say is that larger \\(\\lambda\\) correspond to smaller \\(\\tau\\).\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhy is this true? Why does small \\(\\tau\\) imply large \\(\\lambda\\) and vice versa?\n\n\n\nIn this form, it’s straightforward to pose ridge regression as:\n\\[\\argmin_{\\beta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 + \\frac{\\lambda}{2}\\|\\bbeta\\|_2^2\\]\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nWhy were we allowed to put an extra \\(\\frac{1}{2}\\) in front of the penalty term?\n\n\n\nYou will show in Report #01 that the solution to this problem is given by:\n\\[\\hat{\\bbeta}_{\\lambda} = (\\bX^{\\top}\\bX + \\lambda \\bI)^{-1}\\bX^{\\top}\\by\\]\nThere are several important things to note about this expression:\n\nThere are actually many ridge regression solutions, one for each value of \\(\\lambda\\). It is a bit improper to refer to “the” ridge regression solution. We will discuss selecting \\(\\lambda\\) below.\nIf you are a bit sloppy, this looks something like:\n\\[\\hat{\\beta} \\approx \\frac{SS_{XY}}{SS_{XX} + \\lambda}\\]\nso we ‘shrink’ the standard OLS solution towards zero, by an amount depending on \\(\\lambda\\).\nThe penalized form (as opposed to the constraint form) allows for \\(\\hat{\\beta}\\) to be arbitrarily large, if the data supports it.\nIf we set \\(\\lambda = 0\\), we recover standard OLS.\nUnlike the OLS solution, the ridge regression exists even when \\(p &gt; n\\).\n\nWhile this isn’t as simple to analyze as some of the expressions we considered above, this looks similar enough to our ‘shrunk’ mean estimator that it’s plausible it will improve on OLS. In fact, you will show in Report #01 that there is always some value of \\(\\lambda\\) that guarantees improvement over OLS.\nTo wit,\n\nn &lt;- 100\np &lt;- 80\nZ &lt;- matrix(rnorm(n * p), nrow=n)\nL &lt;- chol(toeplitz(0.6^(1:p))) # Add 'AR(1)' correlation\nX &lt;- Z %*% L\nbeta &lt;- runif(p, 2, 3)\n\neye &lt;- function(n) diag(1, n, n)\ncalculate_ridge_mse &lt;- function(lambda, nreps = 1000){\n    MSE &lt;- mean(replicate(nreps, {\n        y &lt;- X %*% beta + rnorm(n, sd=1)\n        beta_hat &lt;- solve(crossprod(X) + lambda * eye(p), crossprod(X, y))\n        sum((beta - beta_hat)^2)\n    }))\n    \n    data.frame(lambda=lambda, MSE=MSE)\n}\n\nlambda_grid &lt;- 10^(seq(-2, 2, length.out=41))\n\nRIDGE_MSE &lt;- map(lambda_grid, calculate_ridge_mse) |&gt; list_rbind()\nOLS_MSE &lt;- calculate_ridge_mse(0)$MSE # Ridge at lambda = 0 =&gt; OLS\n\nggplot(RIDGE_MSE, aes(x=lambda, y=MSE)) + \n    geom_point() + \n    geom_line() + \n    geom_abline(intercept=OLS_MSE, slope=0, lwd=2, lty=2) + \n    xlab(expression(lambda)) + \n    ylab(\"Estimation Error (MSE)\") + \n    ggtitle(\"Ridge Regularization Improves on OLS\") + \n    theme_bw() + \n    scale_x_log10()\n\n\n\n\n\n\n\n\nClearly, for smallish \\(\\lambda\\), we are actually outperforming OLS, sometimes by a significant amount! For this problem, the minimum MSE (best estimate) actually seems to occur near \\(\\lambda \\approx\\) 1.585\n\nModel Selection\nWhile theory and our experiment above tell us that there is some value of \\(\\lambda\\) that beats OLS, how can we actually find it? We can’t do a simulation like above as we had to know \\(\\bbeta_*\\) to compute the MSE and the theory is non-constructive. (In particular, we have to know \\(\\bbeta_*\\) to compute the bias.)\nWell, we can go back to our overarching goal: we want results that generalize to unseen data. Here, in particular, we want results that predict well on unseen data. So let’s just do that.\nWe can split our data into a training set and a validation set. The training set is like we have already seen, used to estimate the regression coefficients \\(\\bbeta_{\\text{Ridge}}\\); the validation set may be new. It is used to compute the MSE on ‘pseudo-new’ data and we then can select the best predicting value of \\(\\lambda\\). In this case, this looks something like the following:\n\n# Continuing with X, beta from above\ny &lt;- X %*% beta + rnorm(n, sd = 1)\n\nTRAIN_IX &lt;- sample(n, 0.8 * n)\nVALID_IX &lt;- setdiff(seq(n), TRAIN_IX) # Other elements\n\nTRAIN_X &lt;- X[TRAIN_IX, ]\nTRAIN_Y &lt;- y[TRAIN_IX]\n\nVALID_X &lt;- X[VALID_IX, ]\nVALID_Y &lt;- y[VALID_IX]\n\ncompute_validation_error &lt;- function(lambda){\n    beta_hat_rr &lt;- solve(crossprod(TRAIN_X) + lambda * eye(p), \n                         crossprod(TRAIN_X, TRAIN_Y))\n    \n    y_pred &lt;- VALID_X %*% beta_hat_rr\n    \n    data.frame(lambda = lambda, \n               validation_mse = mean((y_pred - VALID_Y)^2))\n}\n\nvalidation_error &lt;- map(lambda_grid, compute_validation_error) |&gt; list_rbind()\n\nggplot(validation_error, \n       aes(x = lambda, \n           y = validation_mse)) + \n    geom_point() + \n    xlab(expression(lambda)) + \n    ylab(\"MSE on Validation Set\") + \n    ggtitle(\"Hold-Out Tuning of Ridge Regression Regularization Parameter\") + \n    theme_bw() + \n    scale_x_log10()\n\n\n\n\n\n\n\n\nOur results here aren’t quite as definitive as when we knew the true \\(\\bbeta_*\\), but they suggest we want to take \\(\\lambda \\approx\\) 0.631 which isn’t too far from what we found above.\n\n\n\n\n\n\nPause to Reflect\n\n\n\n\n\nDoes this procedure give an unbiased estimate of the true test error? Why or why not?\n\n\n\nThere are some limitations to this basic procedure - randomness and (arguably) inefficient data usage - that we will address later, but this gets us started."
  },
  {
    "objectID": "notes/notes03.html#lasso-regression",
    "href": "notes/notes03.html#lasso-regression",
    "title": "STA 9890 - Regularized Regression",
    "section": "Lasso Regression",
    "text": "Lasso Regression\nRidge regression is a rather remarkable solution to the problem of ‘inflated’ regression coefficients. But let’s remind ourselves why we find inflated coefficients in the first place.\nA common cause of overly large regression coefficients is correlation among the columns of \\(\\bX\\). For example,\n\nmax_beta_sim &lt;- function(rho, n, p){\n    R &lt;- replicate(100, {\n        Z &lt;- matrix(rnorm(n * p), nrow=n)\n        L &lt;- chol(toeplitz(rho^(1:p)))\n        X &lt;- Z %*% L\n        beta &lt;- runif(p, 2, 3)\n        y &lt;- X %*% beta + rnorm(n)\n        \n        beta_hat &lt;- coef(lm(y ~ X))\n        max(abs(beta_hat))\n    })\n    data.frame(beta_max = max(R),\n               n = n, \n               p = p, \n               rho = rho)\n}\n\nRHO_GRID &lt;- seq(0.5, 0.999, length.out=101)\n\nBETAS &lt;- map(RHO_GRID, max_beta_sim, n = 40, p = 35) |&gt; list_rbind()\n\nggplot(BETAS, \n       aes(x = rho, \n           y = beta_max))+ \n    geom_point() + \n    scale_y_log10() + \n    theme_bw() + \n    xlab(\"Feature Correlation\") + \n    ylab(\"Maximum Observed Regression Coefficient\") + \n    geom_abline(intercept = 3, color=\"red4\", lwd=2, lty=2)\n\n\n\n\n\n\n\n\nThis simulation is maybe a bit unfair to OLS since we are taking maxima instead of averages or similar, but the message is certainly clear. As the columns of \\(\\bX\\) become highly correlated, we get larger values of \\(\\hat{\\beta}\\). Because OLS is unbiased here, this is fundamentally a story of variance: as we have more features,\n\\[\\V[\\hat{\\beta}] \\propto (\\bX^{\\top}\\bX)^{-1}\\]\nincreases, making it likely that we see increasingly wild values of \\(\\hat{\\beta}\\). (Recall that we generated our data so that \\(\\beta_*\\) was never larger than 3, as noted in red above.)\nWhy does this happen? It’s maybe a bit tricky to explain, but with highly correlated features the model can’t really distinguish them well, so it ‘overreacts’ to noise and produces very large swings (variance) in response to small changes in the training data. Like above, we can hope to tame some of this variance if we take on a bit of bias.\nAnother way we might want to ‘calm things down’ and improve on OLS is by having fewer features in our model. This reduces the chance of ‘accidental correlation’ (more features leads to more ‘just because’ correlations) and gives us a more interpretable model overall. (More about interpretability below)\n\nBest Subsets\nYou have likely already seen some sort of ‘variable selection’ procedures in prior courses: things like forward stepwise, backwards stepwise, or even hybrid stepwise. All of these are trying to get at the idea of only including variables that are ‘worth it’; in other settings, you may also see them used as finding the ‘most valuable’ variables. Let’s use our newfound knowledge of optimization to formalize this.\nThe best-subsets problem is defined by:\n\\[\\argmin_{\\beta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 \\text{ such that } \\|\\bbeta\\|_0 \\leq k\\]\nThis looks similar to our ridge regression problem, but now we’ve replaced the \\(\\ell_2\\)-norm constraint with the \\(\\ell_0\\)-‘norm’. Recall that the \\(\\ell_0\\)-‘norm’ is the number of non-zero elements in \\(\\bbeta\\) and that it’s not a real norm. So this says, find the minimum MSE \\(\\beta\\) with at most \\(k\\) non-zero elements. Since more features always reduces (or at least never increases) training MSE, we can assume that this problem will essentially always pick the best combination of \\(k\\) variables.\nAlternatively, in penalized form, we might write:\n\\[\\argmin_{\\beta} \\|\\by - \\bX\\bbeta\\|_2^2 + \\lambda \\|\\bbeta\\|_0\\]\nHere, we only introduce a new variable if including it lowers our training MSE by \\(\\lambda\\): we’ve implicitly set \\(\\lambda\\) as our ‘worth it’ threshold for variables.\nNote that these problems require finding the best combination of variables. The most individually useful variables might not be useful in combination: e.g., someone’s right shoe size might be very useful for predicting their height, and their left shoe size might be very useful for predicting their height, and their gender might be a little bit useful for predicting their height, but you’ll do better with right shoe and gender than with right and left shoe.\nBecause we’re checking all combinations of variables, this is a so-called ‘combinatorial’ optimization problem. These are generally quite hard to solve and require specialized algorithms (unlike our general purpose convex optimization algorithms). A naive approach of ‘try all models’ becomes impractical quickly. If we have \\(p=30\\) potential features, that’s \\(2^30\\), or just over a billion, possible models we would need to check. Even if we can check one model every minute, that takes about 2040 years to check them all; if we have a computer do the work for us and it’s faster, say 1000 models per minute, we still need over 2 years. And that’s just for 30 features! In realistic problems where \\(p\\) can be in the hundreds or thousands, we have no chance.\nThankfully, smart people have worked on this problem for us and found that for \\(p \\approx 100 - 1000\\), very fancy software can solve this problem.1 But that still leaves us very far we want to go…\nTODO:\n\nStepwise as approximate algorithm\nConvex relaxation"
  },
  {
    "objectID": "notes/notes03.html#model-selection",
    "href": "notes/notes03.html#model-selection",
    "title": "STA 9890 - Regularized Regression",
    "section": "Model Selection",
    "text": "Model Selection"
  },
  {
    "objectID": "notes/notes03.html#the-lasso",
    "href": "notes/notes03.html#the-lasso",
    "title": "STA 9890 - Regularized Regression",
    "section": "The Lasso",
    "text": "The Lasso\nTODO ### Model Selection TODO"
  },
  {
    "objectID": "notes/notes03.html#footnotes",
    "href": "notes/notes03.html#footnotes",
    "title": "STA 9890 - Regularized Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Best subset selection via a modern optimization lens” by Dimitris Bertsimas, Angela King, Rahul Mazumder Annals of Statististics 44(2): 813-852 (April 2016). DOI: 10.1214/15-AOS1388↩︎"
  }
]