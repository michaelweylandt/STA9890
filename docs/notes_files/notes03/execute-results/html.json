{
  "hash": "d43bd14a8b6cfd35294ebe9bbbf3261e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"3\"\nauthor: \"Michael Weylandt\"\ntopic: \"Regression II: Regularized Regression\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n\n$$\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}} \\newcommand{\\V}{\\mathbb{V}} \\newcommand{\\argmin}{\\text{arg min}}$$\n\nLast week we showed the following: \n\n- OLS can be derived purely as a 'loss minimization' problem without reference\n  to specific probabilistic models; this is an instance of the general strategy\n  of *empirical risk management*\n- OLS is a *convex* problem, making it particularly easy to solve even if we\n  don't use the closed-form solution\n- The BLUE property of OLS is perhaps less interesting than it sounds as it\n  requires very strong assumptions on the data generating process which are\n  unlikely to hold in practice\n- MSE, the loss we used to pose OLS, can be decomposed as \n  $\\text{Bias}^2 + \\text{Variance}$\n- Because BLUE restricts to _unbiased_ estimators, the MSE of OLS is all variance\n\nWe begin this week by asking if we can do better than OLS. To keep things simple,\nwe begin by assuming we are under a linear DGP (so no 'model error') but that's\nonly a mathematical niceity. It's not something you should always assume - in fact,\nit is really more important to think about how models do on non-linear DGPs. \nAs we will see, it may still be useful to use linear models... \n\nBecause OLS is BLUE under our assumptions, we know that we need to relax one\nor more of our assumptions to beat it. For now, we will focus on relaxing the\nU - unbiasedness; non-linear methods come later in this course. \n\nRecalling our decomposition: \n\n$$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n\nOur gambit is that we can find an alternative estimator with a bit more bias, \nbut far less variance. Before we attempt to do so for linear regression, let's \nconvince ourselves this is possible for a much simpler problem - estimating\nmeans. \n\n## Estimating Normal Means\n\nSuppose we have data from a distribution $$X_i \\buildrel \\text{iid} \\over \\sim \n\\mathcal{N}(\\mu, 1)$$ for some unknown $\\mu$ that we seek to estimate. Quite \nreasonably, we might use the sample mean \n$$\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$$\nto estimate $\\mu$. Clearly, this is an _unbiased_ estimator and it has variance\ngiven by $1/n$, which isn't bad. In general, it's pretty hard to top this. \n\nWe can verify all of this empirically: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(dplyr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_mse_sample_mean <- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R <- replicate(1000, {\n        X <- rnorm(n, mean=mu, sd=1)\n        mean(X)\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nMU_GRID <- seq(-5, 5, length.out=501)\nN <- 10\n\nSIMRES <- map(MU_GRID, compute_mse_sample_mean, n=N) |> list_rbind()\n```\n:::\n\n\n\nOur bias is essentially always zero: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES, aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(slope=0, \n                intercept=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Constant Zero Bias of Sample Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nSimilarly, our bias is small, and constant. Specifically, it is around \n$1/n$ as predicted by standard theory: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES, aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Variance of Sample Mean\") + \n    ggtitle(\"Constant Variance of Sample Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAs expected, the MSE is the sum of bias and variance, so it's basically just\nvariance here: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Sample Mean MSE \") + \n    ggtitle(\"Constant MSE of Sample Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nSo far, it looks like the sample mean is hard to beat. In particular,\nthis curve is But... what if we know, *e.g.*, that $\\mu$ is positive. We \nmight still use the sample mean, but with the additional step that we set\nit to zero if the sample mean looks negative. That is, our new estimator is\n$$\\hat{\\mu} = (\\overline{X}_n)_+ \\text{ where } z_+ = \\begin{cases} z & z > 0 \\\\ 0 & z \\leq 0 \\end{cases}$$\n\nThe $(\\cdot)_+$ operator is known as the *positive-part*. How does this \n$\\hat{\\mu}$ do? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npospart <- function(x) ifelse(x > 0, x, 0)\ncompute_mse_positive_mean <- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the positive part of the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R <- replicate(1000, {\n        X <- rnorm(n, mean=mu, sd=1)\n        pospart(mean(X))\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nSIMRES_POSPART <- map(MU_GRID, compute_mse_positive_mean, n=N) |> \n    list_rbind()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES_POSPART, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nNot surprisingly, we do _very poorly_ if we are estimating a negative $\\mu$\nbut we assume it is positive. Let's zoom in on the area near 0 however. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSIMRES_POSPART |>\n    filter(mu >= -0.5, \n           mu <= 1) |>\nggplot(aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nInteresting! For some of these values, we do better than the sample mean. \n\nIn particular, we do better in the following scenario: \n\n- True mean is positive\n- Sample mean is negative\n- Positive part of sample mean is zero, so closer than pure sample mean\n\nThe probability of step 2 (sample mean is negative) is near zero for large\n$\\mu$, but for $\\mu$ in the neighborhood of zero, it can happen.\n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nReview Question: As a function of $\\mu$, what is the probability that\n$\\overline{X}_n$  is negative? You can leave your answer in terms of the\nstandard normal CDF $\\Phi(\\cdot)$.\n\n:::\n\nThis is pretty cool. We have made an additional assumption and, when that\nassumption holds, it helps us or, worst case, doesn't really hurt us much. \nOf course, when the assumption is wrong ($\\mu < 0$), we do much worse, but \nwe can't really hold that against $(\\overline{X}_n)_+$. \n\nLooking more closely, we can look at the bias of $(\\overline{X}_n)_+$: \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSIMRES_POSPART |>\n    filter(mu >= -0.5, \n           mu <= 1) |>\nggplot(aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Bias of Positive Part Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nWe see here that our improvement came at the cost of some bias, particularly \nin the $\\mu \\in [0, 1]$ range. But for that bias, we see a good reduction in\nvariance: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSIMRES_POSPART |>\n    filter(mu >= -0.5, \n           mu <= 1) |>\nggplot(aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(Variance^2)) + \n    ggtitle(\"Non-Constant Variance of Positive Part Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nHere, we see that the variance is less than $1/n$ from $\\mu \\approx 0.5$ and \ndown.  Let's plot variance and bias against each other: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geomtextpath)\nSIMRES_POSPART |>\n    filter(mu >= 0, \n           mu <= 1) |>\n    ggplot(aes(x=bias, y=variance)) + \n       geom_point() + \n       geom_textline(aes(x=bias, y=1/n - bias),\n                     lty=2, color=\"red4\", \n                     label=\"Breakeven\") + \n       ylim(c(0, 0.1)) + \n       theme_bw() + \n       xlab(expression(\"Bias\"^2)) + \n       ylab(\"Variance\") + \n       ggtitle(\"Bias-Variance Tradeoff for Positive Part Sample Mean\")\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nHere, all of the values of $\\mu$ corresponding to points _below_ this line are\npoints where the positive part estimator does better than the standard sample\nmean. \n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nSee if you can compute the bias and variance of $(\\overline{X}_n)_+$ in closed\nform. The [moments of](https://math.stackexchange.com/a/3281287) the \n[Rectified Normal Distribution](https://en.wikipedia.org/wiki/Rectified_Gaussian_distribution)\nmay be of use. \n\n:::\n\n\nOk - now let's start to generalize. Clearly, the first step is to change the\n'positive' assumption. The easiest generalization is to restrict $\\mu$ to an\ninterval $[a, b]$. In this case, it makes sense to replace the positive part\noperator with a 'clamp' operator: \n\n$$(x)_{[a, b]} = \\begin{cases} a & x \\leq a \\\\ x & x\\in(a, b) \\\\ b & x \\geq b \\end{cases}$$\n\nThe positive part operator we applied before is $(x)_+ = (x)_{[0, \\infty)}$. \n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nExtend the simulation above to characterize the estimation performance (bias and\nvariance) of $(\\overline{X}_n)_[a, b]$. \n\n:::\n\nA particularly useful version of this bound is taking $(\\overline{X}_n)[-\\beta, +\\beta]$; \nthat is, we don't know the sign of $\\mu$, but we know it is less than $\\beta$ in\nmagnitude. This is not an improbable assumption - we often have a good sense of\nthe plausible magnitude of a parameter (Bayesian priors anyone?) - but it feels \na bit 'firm'. Can we relax this sort of assumption? We want $\\mu$ to be 'not too\nbig', but we're willing to go big if the data takes us there. \n\nWe can implement this as follows: \n\n$$\\hat{\\mu}_{\\alpha} = \\frac{\\overline{X}_n}{1+\\alpha}$$\n\nClearly, setting $\\alpha = 0$ gets us back to the standard sample mean. Can this\nbe better than the sample mean? Let's do the calculations by hand. First we note\nthat $\\E[\\hat{\\mu}_{\\alpha}] = \\frac{\\mu}{1+\\alpha}$ giving a bias of \n$$\\text{Bias} = \\E[\\hat{\\mu}_{\\alpha}] - \\mu = \\mu\\left(1 - \\frac{1}{1+\\alpha}\\right) \\implies \\text{Bias}^2 = \\mu^2\\left(1 - \\frac{1}{1+\\alpha}\\right)^2$$\nand \n\n$$\\text{Variance} = \\V[\\hat{\\mu}_{\\alpha}] = \\frac{1}{(1+\\alpha)^2}\\V[\\overline{X}_n] = \\frac{1}{n(1+\\alpha)^2}$$\n\nso the total MSE is given by \n\n\n$$\\begin{align*}\n\\text{MSE} &= \\E[(\\hat{\\mu}_{\\alpha} - \\mu)^2] \\\\\n           &= \\text{Bias}^2 + \\text{Variance} \\\\\n           &= \\mu^2\\left(1 - \\frac{1}{1+\\alpha}\\right)^2 + \\frac{1}{n(1+\\alpha)^2}\n\\end{align*}$$\n\nFor suitable $\\alpha, n$ this can be less than the standard MSE of $1/n$. For\ninstance, at $\\mu = 5$ and $n = 10$, \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshrunk_mean_mse <- function(mu, n, alpha){\n    mu^2 * (1 - 1/(1+alpha))^2 + 1/(n * (1+alpha)^2)\n}\n\nshrunk_mean_mse(5, 10, 1e-4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09998025\n```\n\n\n:::\n:::\n\n\n\nNot great - but an improvement! It's actually pretty hard to beat the sample mean\nwith an estimator of this form in the univariate case, but it can be incredibly\nuseful in more general settings. \n\n## James-Stein Estimation of Multivariate Normal Means\n\nTODO\n\n## Ridge Regression\n\nAbove, we saw that $(\\overline{X}_n)_{[-\\beta, \\beta]}$ could outperform \n$\\overline{X}_n$ if the true parameter is 'not too big'. Can we extend this idea\nto regression? \n\nRecall that we formulated OLS as: \n\n$$\\hat{\\bbeta}_{\\text{OLS}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|$$\n\nWe can apply a 'clamp' as: \n\n$$\\hat{\\bbeta}_{\\tau-\\text{Clamped}} = \\argmin_{\\bbeta \\in \\R: \\|\\bbeta\\| < \\tau} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\| \\leq {\\tau}$$\n\nHere, we have some choice in measuring the 'size' of $\\hat{\\bbeta}$: in fact, we\ncan *theoretically* use any of our $\\ell_p$-norms. As with the squared loss, \nit turns out to be mathematically easiest to start with the $\\ell_2$ (Euclidean)\nnorm. This gives us the *ridge regression* problem: \n\n$$\\hat{\\bbeta}_{\\text{Ridge}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2 \\leq {\\tau} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2^2 \\leq {\\tau}^2$$\n\nBefore we discuss solving this, let's confirm it is convex. Recall that for a\nproblem to be convex, it needs to have a convex *objective* and a convex *feasible\nset*. Clearly the objective here is convex - we still have the MSE loss from OLS\nunchanged. So now we need to convince ourselves that $\\|\\bbeta\\|_2^2 \\leq \\tau^2$\ndefines a convex set.  We can look at this in 2D first: \n\n$$\\|\\bbeta\\|_2^2 \\leq \\tau^2 \\implies \\beta_1^2 + \\beta_2^2 \\leq \\tau$$\n\nBut this is just the equation defining a circle and its interior (in math speak\na 2D 'ball') so it has to be convex!\n\nIn fact, constraints of the form \n\n$$\\|\\bbeta\\|_p \\leq \\tau \\Leftrightarrow \\|\\bbeta\\|_p^2 \\leq \\tau^p$$\n\nare convex for all $\\ell_p$ norms ($p \\geq 1$). We will use this fact many times\nin this course. The sets that this constraint defines are called $\\ell_p$ 'balls'\nby analogy with the $\\ell_2$ figure. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nSo our ridge problem \n\n$$\\hat{\\bbeta}_{\\text{Ridge}} = \\argmin_{\\bbeta \\in \\R} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\| \\text{ such that } \\|\\bbeta\\|_2^2 \\leq {\\tau}^2$$\nis indeed convex. While we can solve this without too much trouble, it turns\nout to be easier to use an optimization trick known as *Lagrange Multipliers* \nto change this to something easier to solve. \n\nThe [Method of Lagrange Multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier)\nlets us turn *constrained* optimization problems that look like: \n\n$$\\argmin_x f(x) \\text{ such that } g(x) \\leq \\tau$$\n\ninto *unconstrained* problems like: \n\n$$\\argmin_x f(x) + \\lambda g(x)$$\n\nHere, the constant $\\lambda$ is known as the Lagrange multiplier. Instead of \n_forcing_ $g(x)$ to be bounded, we simply _penalize_ any $x$ that makes $g(\\cdot)$\nlarge. For large enough penalties (large $\\lambda$), we will eventually force\n$g(x)$ to be small enough that we satisfy the original constraint. In fact, \nthere is a one-to-one relationship between $\\tau$ and $\\lambda$, but it's usually\nnot possible to work it out in any meaningful useful manner; all we can say is that\nlarger $\\lambda$ correspond to smaller $\\tau$. \n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nWhy is this true? Why does small $\\tau$ imply large $\\lambda$ and *vice versa*?\n\n:::\n\nIn this form, it's straightforward to pose ridge regression as: \n\n$$\\argmin_{\\beta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 + \\frac{\\lambda}{2}\\|\\bbeta\\|_2^2$$\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nWhy were we allowed to put an extra $\\frac{1}{2}$ in front of the penalty term?\n\n:::\n\nYou will show in [Report #01](../reports/repot01.html) that the solution to this\nproblem is given by: \n\n$$\\hat{\\bbeta}_{\\lambda} = (\\bX^{\\top}\\bX + \\lambda \\bI)^{-1}\\bX^{\\top}\\by$$\n\nThere are several important things to note about this expression: \n\n- There are actually *many* ridge regression solutions, one for each value of\n  $\\lambda$. It is a bit improper to refer to \"the\" ridge regression solution. \n  We will discuss selecting $\\lambda$ below. \n- If you are a bit sloppy, this looks something like: \n\n  $$\\hat{\\beta} \\approx \\frac{SS_{XY}}{SS_{XX} + \\lambda}$$\n  \n  so we 'shrink' the standard OLS solution towards zero, by an amount depending \n  on $\\lambda$. \n- The penalized form (as opposed to the constraint form) allows for $\\hat{\\beta}$\n  to be arbitrarily large, if the data supports it.\n- If we set $\\lambda = 0$, we recover standard OLS. \n- Unlike the OLS solution, the ridge regression exists even when $p > n$. \n  \nWhile this isn't as simple to analyze as some of the expressions we considered above, \nthis looks similar enough to our 'shrunk' mean estimator that it's plausible it\nwill improve on OLS. In fact, you will show in [Report #01](../reports/repot01.html)\nthat there is always _some_ value of $\\lambda$ that guarantees improvement over OLS. \n\nTo wit, \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100\np <- 80\nZ <- matrix(rnorm(n * p), nrow=n)\nL <- chol(toeplitz(0.6^(1:p))) # Add 'AR(1)' correlation\nX <- Z %*% L\nbeta <- runif(p, 2, 3)\n\neye <- function(n) diag(1, n, n)\ncalculate_ridge_mse <- function(lambda, nreps = 1000){\n    MSE <- mean(replicate(nreps, {\n        y <- X %*% beta + rnorm(n, sd=1)\n        beta_hat <- solve(crossprod(X) + lambda * eye(p), crossprod(X, y))\n        sum((beta - beta_hat)^2)\n    }))\n    \n    data.frame(lambda=lambda, MSE=MSE)\n}\n\nlambda_grid <- 10^(seq(-2, 2, length.out=41))\n\nRIDGE_MSE <- map(lambda_grid, calculate_ridge_mse) |> list_rbind()\nOLS_MSE <- calculate_ridge_mse(0)$MSE # Ridge at lambda = 0 => OLS\n\nggplot(RIDGE_MSE, aes(x=lambda, y=MSE)) + \n    geom_point() + \n    geom_line() + \n    geom_abline(intercept=OLS_MSE, slope=0, lwd=2, lty=2) + \n    xlab(expression(lambda)) + \n    ylab(\"Estimation Error (MSE)\") + \n    ggtitle(\"Ridge Regularization Improves on OLS\") + \n    theme_bw() + \n    scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nClearly, for smallish $\\lambda$, we are actually outperforming OLS, sometimes\nby a significant amount! For this problem, the minimum MSE (best estimate) actually\nseems to occur near $\\lambda \\approx$ 1.259\n\n### Model Selection\n\nWhile theory and our experiment above tell us that there is _some_ value of\n$\\lambda$ that beats OLS, how can we actually find it? We can't do a simulation\nlike above as we had to know $\\bbeta_*$ to compute the MSE and the theory is\nnon-constructive. (In particular, we have to know $\\bbeta_*$ to compute the \nbias.) \n\nWell, we can go back to our overarching goal: we want results that _generalize_\nto unseen data. Here, in particular, we want results that _predict well_ on\nunseen data. So let's just do that. \n\nWe can split our data into a _training_ set and a _validation_ set. The _training_\nset is like we have already seen, used to estimate the regression coefficients\n$\\bbeta_{\\text{Ridge}}$; the _validation_ set may be new. It is used to compute\nthe MSE on 'pseudo-new' data and we then can select the best predicting value of\n$\\lambda$. In this case, this looks something like the following: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Continuing with X, beta from above\ny <- X %*% beta + rnorm(n, sd = 1)\n\nTRAIN_IX <- sample(n, 0.8 * n)\nVALID_IX <- setdiff(seq(n), TRAIN_IX) # Other elements\n\nTRAIN_X <- X[TRAIN_IX, ]\nTRAIN_Y <- y[TRAIN_IX]\n\nVALID_X <- X[VALID_IX, ]\nVALID_Y <- y[VALID_IX]\n\ncompute_validation_error <- function(lambda){\n    beta_hat_rr <- solve(crossprod(TRAIN_X) + lambda * eye(p), \n                         crossprod(TRAIN_X, TRAIN_Y))\n    \n    y_pred <- VALID_X %*% beta_hat_rr\n    \n    data.frame(lambda = lambda, \n               validation_mse = mean((y_pred - VALID_Y)^2))\n}\n\nvalidation_error <- map(lambda_grid, compute_validation_error) |> list_rbind()\n\nggplot(validation_error, \n       aes(x = lambda, \n           y = validation_mse)) + \n    geom_point() + \n    xlab(expression(lambda)) + \n    ylab(\"MSE on Validation Set\") + \n    ggtitle(\"Hold-Out Tuning of Ridge Regression Regularization Parameter\") + \n    theme_bw() + \n    scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nOur results here aren't quite as definitive as when we knew the true $\\bbeta_*$, \nbut they suggest we want to take $\\lambda \\approx$ \n0.794\nwhich isn't too far from what we found above. \n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nDoes this procedure give an unbiased estimate of the true test error? Why or \nwhy not? \n\n:::\n\nThere are some limitations to this basic procedure - randomness and (arguably)\ninefficient data usage - that we will address later, but this gets us started. \n\n## Lasso Regression\n\nRidge regression is a rather remarkable solution to the problem of 'inflated' \nregression coefficients. But let's remind ourselves why we find inflated\ncoefficients in the first place. \n\nA common cause of overly large regression coefficients is correlation among the\ncolumns of $\\bX$. For example, \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_beta_sim <- function(rho, n, p){\n    R <- replicate(100, {\n        Z <- matrix(rnorm(n * p), nrow=n)\n        L <- chol(toeplitz(rho^(1:p)))\n        X <- Z %*% L\n        beta <- runif(p, 2, 3)\n        y <- X %*% beta + rnorm(n)\n        \n        beta_hat <- coef(lm(y ~ X))\n        max(abs(beta_hat))\n    })\n    data.frame(beta_max = max(R),\n               n = n, \n               p = p, \n               rho = rho)\n}\n\nRHO_GRID <- seq(0.5, 0.999, length.out=101)\n\nBETAS <- map(RHO_GRID, max_beta_sim, n = 40, p = 35) |> list_rbind()\n\nggplot(BETAS, \n       aes(x = rho, \n           y = beta_max))+ \n    geom_point() + \n    scale_y_log10() + \n    theme_bw() + \n    xlab(\"Feature Correlation\") + \n    ylab(\"Maximum Observed Regression Coefficient\") + \n    geom_abline(intercept = 3, color=\"red4\", lwd=2, lty=2)\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nThis simulation is maybe a bit unfair to OLS since we are taking maxima instead\nof averages or similar, but the message is certainly clear. As the columns of\n$\\bX$ become highly correlated, we get larger values of $\\hat{\\beta}$. Because\nOLS is unbiased here, this is fundamentally a story of variance: as we have more\nfeatures, \n\n$$\\V[\\hat{\\beta}] \\propto (\\bX^{\\top}\\bX)^{-1}$$\n\nincreases, making it likely that we see increasingly wild values of $\\hat{\\beta}$. \n(Recall that we generated our data so that $\\beta_*$ was never larger than 3, as \nnoted in red above.)\n\nWhy does this happen? It's maybe a bit tricky to explain, but with highly\ncorrelated features the model can't really distinguish them well, so it \n'overreacts' to noise and produces very large swings (variance) in response\nto small changes in the training data. Like above, we can hope to tame some of\nthis variance if we take on a bit of bias. \n\nAnother way we might want to 'calm things down' and improve on OLS is by \nhaving fewer features in our model. This reduces the chance of 'accidental \ncorrelation' (more features leads to more 'just because' correlations) and gives\nus a more interpretable model overall. (More about interpretability below)\n\n### Best Subsets\n\nYou have likely already seen some sort of 'variable selection' procedures in \nprior courses: things like _forward stepwise_, _backwards stepwise_, or even \n_hybrid stepwise_. All of these are trying to get at the idea of only including\nvariables that are 'worth it'; in other settings, you may also see them used as\nfinding the 'most valuable' variables. Let's use our newfound knowledge of \noptimization to formalize this. \n\nThe _best-subsets_ problem is defined by: \n\n$$\\argmin_{\\beta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2 \\text{ such that } \\|\\bbeta\\|_0 \\leq k$$\n\nThis looks similar to our ridge regression problem, but now we've replaced the\n$\\ell_2$-norm constraint with the $\\ell_0$-'norm'. Recall that the \n$\\ell_0$-'norm' is the number of non-zero elements in $\\bbeta$ and that it's not\na real norm. So this says, find the minimum MSE $\\beta$ with at most $k$ non-zero\nelements. Since more features always reduces (or at least never increases) training\nMSE, we can assume that this problem will essentially always pick the best \n_combination_ of $k$ variables. \n\nAlternatively, in penalized form, we might write: \n\n\n$$\\argmin_{\\beta} \\|\\by - \\bX\\bbeta\\|_2^2 + \\lambda \\|\\bbeta\\|_0$$\n\nHere, we only introduce a new variable if including it lowers our training MSE\nby $\\lambda$: we've implicitly set $\\lambda$ as our 'worth it' threshold for\nvariables. \n\nNote that these problems require finding the best _combination_ of variables. \nThe most individually useful variables might not be useful in combination: \n*e.g.*, someone's right shoe size might be very useful for predicting their height, \nand their left shoe size might be very useful for predicting their height, and their\ngender might be a little bit useful for predicting their height, but you'll do better\nwith right shoe and gender than with right and left shoe. \n\nBecause we're checking all combinations of variables, this is a so-called \n'combinatorial' optimization problem. These are generally quite **hard** to solve\nand require specialized algorithms (unlike our general purpose convex optimization\nalgorithms). A naive approach of 'try all models' becomes impractical quickly. \nIf we have $p=30$ potential features, that's $2^30$, or just over a billion,\npossible models we would need to check. Even if we can check one model every\nminute, that takes about 2040 **years** to check them all; if we have a computer\ndo the work for us and it's faster, say 1000 models per minute, we still need\nover 2 years. And that's just for 30 features! In realistic problems where $p$ \ncan be in the hundreds or thousands, we have no chance. \n\nThankfully, smart people have worked on this problem for us and found that for\n$p \\approx 100 - 1000$, very fancy software can solve this problem.[^bertsimas]\nBut that still leaves us very far we want to go...\n\n[^bertsimas]: \"Best subset selection via a modern optimization lens\" by \nDimitris Bertsimas, Angela King, Rahul Mazumder\n*Annals of Statististics* **44(2)**: 813-852 (April 2016). DOI: \n[10.1214/15-AOS1388](https://dx.doi.org/10.1214/15-AOS1388)\n\nTODO: \n\n- Stepwise as approximate algorithm\n- Convex relaxation\n\n\n## The Lasso\n\nTODO \n### Model Selection\nTODO\n",
    "supporting": [
      "notes03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}