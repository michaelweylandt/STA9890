[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "STA 9890 - Course Prediction Competition",
    "section": "",
    "text": "In lieu of a final exam, STA 9890 has a prediction competition worth approximately one-third of your final grade. Details about this prediction competition will be posted here."
  },
  {
    "objectID": "reports/report02.html",
    "href": "reports/report02.html",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "Released to Students: 2025-03-11\nSubmission: 2025-04-18 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report02.html#research-report",
    "href": "reports/report02.html#research-report",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "text": "Research Report #02: Ensemble Learning Techniques for Fair Classification\nIn this Research Report, you will explore concepts of Machine Learning Fairness and see how ensemble learning techniques can be used to create a “fair” ensemble from “unfair” base learners.1 Along the way, you will\n\nUse a black-box optimizer to implement a form of regularized logistic regression\nDesign and implement a custom ensemble learning strategy\nImplement and Assess Various ML Fairness Metrics\n\nProject Skeleton\nYour report should be divided into three sections, covering the following:\n\nBackground on ML Fairness\nComputation - Implementing Regularized Logistic Regression with CVXR or cvxpy\n\nImplementation and Assessment of the FairStacks Ensembling Method\n\nAt a minimum, these should include the following elements:\n\nBackground on ML Fairness\n\nSocial background, including two examples of bias in automated decision systems\nOverview of definitions used to measure ML Fairness\nIdentification and import of a real data set on which we can evaluate the proposed methodology. Be sure to describe both the “raw” data and the fairness metric you are interested in. If there is relevant social context, be sure to describe that as well.\n\n\nComputation\n\nIntroduction to the CVX optimization framework\nImplementation of the following classification methods as base learners to be used in construction of the FairStacks ensemble:\n\nNaive Bayes (no CVX)\nLinear Discriminant Analysis (no CVX)\nSupport Vector Machines (use CVX)\nLogistic Regression (use CVX): plain, ridge, and lasso variants\nDecision trees (you may use an existing implementation - use individual tree(s) here, not a full random forest)\n\n\n\n\nImplementation and Assessment of FairStacks\n\nDefine the FairStacks Ensembling Problem, taking care to describe both the choice of loss function (logistic loss)2, regularization and any constraints used\nImplement the FairStacks Problem using CVX\n\nImplement a full model building (train-validation/ensembling-test split) pipeline to implement FairStacks robustly\nApply the FairStacks pipeline to your real data set and chosen fairness metric\nCompute the fairness and accuracy obtained by the FairStacks ensemble and compare it with the fairness and accuracy of the individual base learners. Does FairStacks succeed in its goal of improving both accuracy and fairness?\n\n\nAdditional Background\nML Fairness\nAs ML systems continue to permate our society, increasing interest has been paid to their effects on society. ML systems are highly effective at replicating the characteristics of their training data - both good (accuracy) and bad (bias).3 The book BHN reviews this emerging area in some detail.\nFor our purposes, we can restrict our attention to ‘fair classification’, i.e., making sure that ML systems treat different groups ‘equally’ (BHN §3). There are many definitions of ‘equally’ that can be appropriate, depending on the problem. In the simplest, demographic parity, the ML system should give the same fraction of positive (+1) labels to all demographic groups; this is the type of fairness embodied by phrases like “looks like America”. We quantify this with measures like deviation from demographic parity. If we divide our population into two groups \\(\\mathcal{G}_1, \\mathcal{G}_2\\), the deviation from demographic parity associated with a classifier \\(f: \\mathbb{R}^p \\to \\{0, 1\\}\\) is given by:\n\\[\\text{DDP}(f) = \\left|\\frac{1}{|\\mathcal{G}_1|}\\sum_{i \\in \\mathcal{G}_1} f(\\mathbf{x}_i) - \\frac{1}{|\\mathcal{G}_2|}\\sum_{i \\in \\mathcal{G}_2} f(\\mathbf{x}_i)\\right| = \\left|\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{G}_1} f(\\mathbf{x}) - \\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{G}_2} f(\\mathbf{x}) \\right|\\]\nThe 2018 FAccT Tutorial “21 Fairness Definitions and Their Politics” covers this and other fairnesss metrics. For purposes of this project, you should consider the fairness metric that is most appropriate to the social context surrounding your dataset of interest. Note that, if your interest is in a non-US data set, the relevant subgroups and legal doctrines may be significantly different than those (implicitly) assumed by most of the FairML literature.\nFairStacks\nFairStacks is a novel ensemble learning method which attempts to create a fair ensemble from a set of unfair base learners. At the highest level, FairStacks generalizes the following idea: if Approach A is biased against a group and Approach B is biased in favor of the same group, then Approach (A + B) will be approximately unbiased.\nTo put FairStacks into practice, we modify our basic ensembling procedure: in standard ensemble learning, we create a “model of models”, finding weights \\(\\hat{\\mathbf{w}}\\) such that\n\\[ \\hat{\\mathbf{w}} = \\text{arg min}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\text{Loss}\\left(\\mathbf{y}, \\sum_j w_j \\hat{f}_j(\\mathbf{x}_i)\\right) \\]\nwhere the inner sum is taken over the base learners \\(\\{\\hat{f}_j\\}\\). We add ‘fairness’ in the form of a regularizer (penalty). If \\(\\hat{b}_j\\) is the bias of base learner \\(\\hat{f}_j\\), FairStacks solves\n\\[ \\hat{\\mathbf{w}} = \\text{arg min}_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\text{Loss}\\left(\\mathbf{y}, \\sum_j w_j \\hat{f}_j(\\mathbf{x}_i)\\right) + \\underbrace{\\lambda \\left|\\sum_j w_j \\hat{b}_j\\right|}_{\\text{Fairness Penalty}}\\]\nBy setting \\(\\lambda\\) large, we force the ‘average bias’ down to zero, resulting in a fairer ensemble. See the paper for more details.\nCVX\nThe CVX family of libraries attempt to make convex optimization easy and accesible to a wide range of users. Implementations of CVX can be found in:\n\nMatlab: the original CVX\n\nPython: cvxpy\n\nR: CVXR\n\nJulia: Convex.jl\n\n\nand more. While the details vary according to the host language, the essential structure is unchanged. I demonstrate the use of CVXR here to implement Lasso regression.\n\nset.seed(100)  # For reproducibility\nlibrary(CVXR)  # Load CVXR\nn &lt;- 400       # Set up a moderately high-dimensional problem\np &lt;- 1000\ns &lt;- 5\nsigma &lt;- 2\n\n# Lasso works best for IID Gaussian data\nX &lt;- matrix(rnorm(n * p), \n            nrow=n, ncol=p)\n\n# 'True' coefficients are mainly sparse with 5 non-zero values\nbeta_star &lt;- matrix(rep(0, p), ncol=1)\nbeta_star[1:s] &lt;- 3\n\n# Generate observed response from OLS DGP\ny &lt;- X %*% beta_star + rnorm(n, sd=sigma)\n\n## We are now ready to apply CVXR\n####  Also see discussion at\n####  https://www.cvxpy.org/examples/machine_learning/lasso_regression.html\n\nbeta &lt;- Variable(p) # Create 'beta' as a CVX _variable_ to be optimized\n\n# Per theory, about the right level of regularization to be used here\nlambda  &lt;- sigma * sqrt(s * log(p) / n) \nloss    &lt;- 1/(2 * n) * sum((y - X %*% beta)^2) # MSE Loss\npenalty &lt;- lambda * sum(abs(beta))\n\nobjective &lt;- Problem(Minimize(loss + penalty))\nbeta_hat  &lt;- solve(objective)$getValue(beta)\n\nWe can see that this correctly finds the non-zero elements:\n\nplot(beta_hat, \n     xlab=\"Coefficient Number\", \n     ylab=\"Lasso Estimate\", \n     main=\"CVX Lasso Solution\", \n     col=\"red4\", \n     pch=16)\n\n\n\n\n\n\n\nIf we look more closely, we see two important properties of the solution:\n\n\nNon-specialized solvers like CVXR cannot achieve exact zeros, so it is useful to do a bit of post-processing (e.g., zapsmall()).\nIf we need exact zeros, other approaches should be used\n\nThe lasso solution exhibits shrinkage, as the estimate are systematically smaller than the true values\n\n\nhead(beta_hat, n=10)\n\n               [,1]\n [1,]  2.435849e+00\n [2,]  2.498692e+00\n [3,]  2.376080e+00\n [4,]  2.418007e+00\n [5,]  2.424720e+00\n [6,]  1.756240e-21\n [7,]  1.604964e-21\n [8,] -1.193091e-21\n [9,] -9.722483e-22\n[10,] -1.320523e-21\n\n\nCVX is never the optimal approach for a given problem, but it is an incredibly useful tool for prototyping and trying out interesting new approaches without putting in the effort to derive and implement a specialized algorithm. (Tools like tensorflow and pytorch take this idea even further, but require significantly more effort to learn. If you are interested in working in ML after this course ends, they are a great place to invest your time.)\nThe documentation for the CVX packages has several examples which will be of use for you.\nPossible Topic(s) for Additional Research\nAlgorithms for Regularized Logistic Regression\nFor this report, you are not required to fit the FairStacks problem “by hand” and can use CVX instead. While CVX is flexible and easy to use, it rarely performs as efficiently as a hand-coded tailored algorithm. You may want to explore the use of convex optimization algorithms for fitting logistic regression problems generally and the FairStacks problem specifically.\nWhile the gradient descent method you used in Research Report #01 can be applied here, it is likely to be unbearably slow. You may achieve better performance using a Newton or Quadratic Approximation approach. These approaches use a (second-order) Taylor expansion to approximate the objective function by a quadratic function; that quadratic function has a closed-form minimizer (essentially the OLS solution) which is used to update the iterate.4 A new approximation is created at the new iterate, solved again, etc. See BV §9.5 for details. This approach is widely used to fit generalized linear models such as logistic regression. In the statistics literature, you may see it described as Iteratively Reweighted Least Squares or Fisher Scoring; see, e.g., the output of summary on a glm fit in R.\nAlternative Fair ML Proposals\nThe FairStacks approach is far from the only Fair ML proposal in the literature. If this is a topic that interest you, you may want to compare FairStacks to other proposals and see which performs the best on your data set and which achieves an acceptable level of fairness. (Not all methods can achieve all levels of fairness). Even though FairStacks worked the best on the problems we considered, it may not be the best for your data set - there is no free lunch after all!\nAdditional Randomization\nAs discussed in class, ensemble methods work best when the base learners exhibit relatively low degrees of correlation. (If all base learners make the same prediction, it does not matter how they are weighted in the ensemble.) This is often particularly difficult to guarantee when applying powerful ML techniques to not-too-difficult prediction problems, as any technique worth knowing will perform reasonably well. In the FairStacks paper, we used a novel technique called mini-patching5 to increase the variance of the base learner ensemble: mini-patching works by taking small subsets of both rows (observations) and columns (features) and fitting base learners to these subsets. We then fit the mini-patched learners using the FairStacks ensembling approach.\nDoes this technique improve the performance of FairStacks on your data set? Are there other techniques you can use to create alternative / better base learners to use in your ensemble?\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.6 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report02.html#footnotes",
    "href": "reports/report02.html#footnotes",
    "title": "STA 9890 Research Report #02: Ensemble Learning Techniques for Fair Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis Research Report is adapted from my ongoing research collaboration with Camille Little (Rice University) and Genevera Allen (Columbia University). A now-somewhat-dated manuscript describing our work in this space can be found online at ArXiv 2206.00074.↩︎\nIn the FairStacks paper, we used OLS as a classification method for somewhat technical reasons. For this Research Report, you should use logistic loss as your primary loss function. It may be an interesting extension to compare against OLS.↩︎\nThe earliest entries in the modern era of chatbots were marked by several flaws reflective their training data. Early iterations of the chat engine built into the Bing search engine were often rather overly-sexual and not-so-subtly malicious, perhaps reflecting the darker corners of the internet on which it was trained.↩︎\nFor this approach, you should use the matrix-analytic closed-form OLS solution instead of using ‘internal’ gradient descent. The aim of Newton methods is to avoid gradient descent.↩︎\nMini-Patching is related to, but should not be confused with the more common technique of Mini-Batching.↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "",
    "text": "There are no required textbooks for this course. Supplemental readings and practice problems will be assigned from the following readily-available resources:\n\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nConvex Optimization by Boyd and Vandenberghe (BV)\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (ISL)\nElements of Statistical Learning by Hastie Tibshirani, and Freedman (ESL)\nStatistical Learning with Sparsity by Hastie, Tibshirani, and Wainwright (SLS)\nPattern Recognition and Machine Learning by Bishop (PRML)\nProbabilistic Machine Learning: An Introduction by Murphy (PML-1)\nProbabilistic Machine Learning: Advanced Topics by Murphy (PML-2)\nGaussian Processes for Machine Learning by Rasmussen and Williams (GPML)\nReinforcement Learning by Sutton and Hutto (RL)\nBoosting: Foundations and Algorithms by Schapire and Freund (SF)\nFoundations of Machine Learning by Mohri, Rostamizadeh, and Talwalkar (FML)\nUnderstanding Machine Learning: From Theory to Algorithms by Shalev-Shwartz and Ben-David (UML)\nPatterns, Predictions, and Actions by Hardt and Recht (HR)\nFairness and Machine Learning by Barocas, Hardt, and Narayanan (BHN)\n\n\n\nSTA 9890 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nThe instructor will provide a formula sheet for use during in-class exams. No alternative resources may be used during these activities.\nStudents may use online resources, such as blog posts, discussion fora, etc. for the Research Reports and Course Competition but Generative AI tools are specifically disallowed.\n\nException: Coding Assistance Tools, e.g., GitHub CoPilot, may be used for the Research Reports and Course Competition. Their usage is limited to coding your ideas. They may not be used as idea generation tools.\n\nThe era of Generative AI is challenging and fast-paced. If you are unsure whether a tool is allowed in this course, please reseach out to the instructor before using it. In matters of Academic Integrity, it is far better to ask permission than forgiveness."
  },
  {
    "objectID": "resources.html#course-resources",
    "href": "resources.html#course-resources",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "",
    "text": "There are no required textbooks for this course. Supplemental readings and practice problems will be assigned from the following readily-available resources:\n\nMathematics for Machine Learning by Diesenroth, Faisal, and Ong (DFO)\nConvex Optimization by Boyd and Vandenberghe (BV)\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (ISL)\nElements of Statistical Learning by Hastie Tibshirani, and Freedman (ESL)\nStatistical Learning with Sparsity by Hastie, Tibshirani, and Wainwright (SLS)\nPattern Recognition and Machine Learning by Bishop (PRML)\nProbabilistic Machine Learning: An Introduction by Murphy (PML-1)\nProbabilistic Machine Learning: Advanced Topics by Murphy (PML-2)\nGaussian Processes for Machine Learning by Rasmussen and Williams (GPML)\nReinforcement Learning by Sutton and Hutto (RL)\nBoosting: Foundations and Algorithms by Schapire and Freund (SF)\nFoundations of Machine Learning by Mohri, Rostamizadeh, and Talwalkar (FML)\nUnderstanding Machine Learning: From Theory to Algorithms by Shalev-Shwartz and Ben-David (UML)\nPatterns, Predictions, and Actions by Hardt and Recht (HR)\nFairness and Machine Learning by Barocas, Hardt, and Narayanan (BHN)\n\n\n\nSTA 9890 will use Piazza as the course discussion board. Students are encouraged to direct all questions about course topics or logistics to Piazza; use of a public anonymous discussion board allows students to benefit from the insights of their classmates and allows instructors to answer questions publicly to the benefit of all students.\nStudents are encouraged to use Piazza’s private question feature if they need to contact the instructor directly. Please only use private questions for personal inquiries: questions about the technical substance of the course can and should be asked (pseudonymously) in the public section of Piazza.\nPiazza login information will be distributed through CUNY Brightspace.\n\n\n\nStudents are encouraged to make use of any and all external resources, including generative AI tools, for at home study and for practice problems. Students are strongly encouraged to collaborate on weekly practice problems. You are ultimately responsible for the correctness of any submitted materials - “the AI told me so” is not a valid defense.\nThe instructor will provide a formula sheet for use during in-class exams. No alternative resources may be used during these activities.\nStudents may use online resources, such as blog posts, discussion fora, etc. for the Research Reports and Course Competition but Generative AI tools are specifically disallowed.\n\nException: Coding Assistance Tools, e.g., GitHub CoPilot, may be used for the Research Reports and Course Competition. Their usage is limited to coding your ideas. They may not be used as idea generation tools.\n\nThe era of Generative AI is challenging and fast-paced. If you are unsure whether a tool is allowed in this course, please reseach out to the instructor before using it. In matters of Academic Integrity, it is far better to ask permission than forgiveness."
  },
  {
    "objectID": "resources.html#aipolicy",
    "href": "resources.html#aipolicy",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Academic Integrity Policy",
    "text": "Academic Integrity Policy\nI fully support CUNY’s Policy on Academic Integrity, which states, in part:\n\nAcademic dishonesty is prohibited in The City University of New York. Penalties for academic dishonesty include academic sanctions, such as failing or otherwise reduced grades, and/or disciplinary sanctions, including suspension or expulsion.\n\n\nAcademic integrity is at the core of a college or university education. Faculty assign essays, exams, quizzes, projects, and so on both to extend the learning done in the classroom and as a means of assessing that learning. When students violate the academic integrity policy (i.e., “cheat”), they are committing an act of theft that can cause real harm to themselves and others including, but not limited to, their classmates, their faculty, and the caregivers who may be funding their education. Academic dishonesty confers an unfair advantage over others, which undermines educational equity and fairness. Students who cheat place their college’s accreditation and their own future prospects in jeopardy.\n\nIn this course, expectations for academic integrity for in-class graded activities are straightforward: no use of unauthorized materials on weekly quizzes or mid-semester tests. Unless explicitly stated otherwise by the instructor in writing, the only authorized materials allowed during in class assessment are the instructor-provided formula sheets.\nThe Research Reports are “open peers”, “open book”, “open internet” but you may not use paid services or generative AI services. You must cite all sources used and include an acknowledgement section listing all peers with whom you collaborated. You do not need to acknowledge the instructor or any Piazza discussions, but you must acknowledge other faculty members or other Baruch students who help you complete your reports, even if they are not enrolled in this course.\nAcademic sanctions in this class will range from an F on the Assignment to an F in this Course. A report of suspected academic dishonesty will be sent to the Office of the Dean of Students.\nStudents are encouraged to contact the instructor with any questions or concerns related to matters of academic integrity."
  },
  {
    "objectID": "resources.html#course-accomodations",
    "href": "resources.html#course-accomodations",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Course Accomodations",
    "text": "Course Accomodations\n\nDisability Services\nIt is CUNY policy to provide Accommodations and Academic Adjustments to students with disabilities.\nAny student who has a disability who may need accommodations in this class should register as early as possible with Student Disability Services (SDS). Your registration with Student Disability Services is confidential, and is not recorded on your Baruch Academic Record. SDS can be reached by email at disability.services@baruch.cuny.edu, by phone at 646-312-4590, or in person at NVC 2-272.\nPlease note that the instructor cannot provide accommodations unless requested by SDS.\n\n\nReligious Accomodations\nIt is CUNY policy to provide accommodations for students’ sincerely held religious beliefs. If a religious accommodation is requested, please contact the instructor at least two weeks in advance.\n\n\nUnexcused Abscence Policy\nAttendance is not required, but lecture recordings will not be provided. Students are responsible for the content of all sessions missed."
  },
  {
    "objectID": "resources.html#personal-resources2",
    "href": "resources.html#personal-resources2",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Personal Resources1",
    "text": "Personal Resources1\nTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress.\nAll of us benefit from support during times of struggle. You are not alone. Asking for support sooner rather than later is often helpful.\n\nMental Health Resources\nIf you or anyone you know experiences significant academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support.\nThe Baruch Counselling Center is here to help. You can visit them in person at 137 E 25th St, 9th floor or call them at 646-312-2155 during normal business hours; you can make an appointment online here. For more immediate support, please call NYC WELL (1-888-NYC-WELL or 1-888-692-9355).2\nAsking for help is often difficult: consider reaching out to a friend, family, or a member of the faculty you trust for help getting connected to support that can help.\nIf you are worried about a friend or classmate, consider reaching out to the Baruch Campus Intervention Team.\n\n\nPhysical Health\nHealthy CUNY promotes well-being and a culture of health in order to foster the academic and life success of all CUNY students. They can connect you with a variety of campus- and community-based healthcare providers.\nBaruch Health Services provides students with a full range of clinical health services. Call 646-312-2040 or email StudentHealthCareCenter@baruch.cuny.edu to make an appointment.\n\n\nFood Security\nAll CUNY students have access to CUNY Food Pantries located throughout the five boroughs, thanks to the CUNY CARES program. CUNY CARES is also able to help qualifying students with SNAP (“Food Stamps”) enrollment.\nOn campus, you can also access the Bearcat Food Pantry.\n\n\nFinancial Security\nBaruch students experiencing heightened financial stress have access to Student Emergency Grants administered through the Office of the Dean of Students.\nNote that funds are also available for students experiencing immigration-related financial stress.\n\n\nImmigration Status\nCUNY Citizenship Now! provides confidential, high-quality immigration law services to all CUNY students.\nNote that Citizenship Now!’s primary Manhattan office is located in the Heights, not on the Baruch campus and that an appointment is strongly recommended. Call 646-664-9350 during business hours for more information."
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "STA 9890 - Additional Resources and Course Policies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLanguage adapted from Professor Ryan Tibshirani (UC Berkeley).↩︎\nDescriptions of Baruch and CUNY resources adapted from program websites.↩︎"
  },
  {
    "objectID": "objectives.html",
    "href": "objectives.html",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "",
    "text": "This course applies multiple regression techniques to the increasingly important study of very large data sets. Those techniques include linear and logistic model fitting, inference, and diagnostics. Methods with special applicability for Big Data will be emphasized, such as lasso and ridge regression. Issues of model complexity, the bias-variance tradeoff, and model validation will be studied in the context of large data sets. Methods that rely less on distributional assumptions are also introduced, including cross-validation, bootstrap resampling, and nonparametric methods. Students will learn dimension reduction methods, correlation analysis, and random forests."
  },
  {
    "objectID": "objectives.html#official-course-description--",
    "href": "objectives.html#official-course-description--",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "",
    "text": "This course applies multiple regression techniques to the increasingly important study of very large data sets. Those techniques include linear and logistic model fitting, inference, and diagnostics. Methods with special applicability for Big Data will be emphasized, such as lasso and ridge regression. Issues of model complexity, the bias-variance tradeoff, and model validation will be studied in the context of large data sets. Methods that rely less on distributional assumptions are also introduced, including cross-validation, bootstrap resampling, and nonparametric methods. Students will learn dimension reduction methods, correlation analysis, and random forests."
  },
  {
    "objectID": "objectives.html#course-learning-objectives",
    "href": "objectives.html#course-learning-objectives",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nStudents successfully completing STA 9890 will be able to:\n\nIdentify Key ML Tasks and Trade-Offs\nUse Regression and Classification Tools to Develop Interpretable and Accurate Predictive Models\nDevelop and Apply Ensemble Learning Strategies\nAccurately Assess Model Performance Using CV, Hold-Out, Stability, and Bootstrap Techniques\nUse Unsupervised Methods to Find Meaningful Structure in Data\nApply Statistical Machine Learning Methods to Novel Data Structures and Types\nGenerate and Communicate Insights via Statistical Machine Learning Methods\n\nThe following course elements contribute to these goals:\n\nContribution of Course Elements to Learning Goals\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Goal 1\nLearning Goal 2\nLearning Goal 3\nLearning Goal 4\nLearning Goal 5\nLearning Goal 6\nLearning Goal 7\n\n\n\n\nTest #01\n✓\n✓\n\n✓\n\n\n\n\n\nResearch Report #01\n✓\n✓\n\n✓\n\n\n\n\n\nTest #02\n✓\n✓\n✓\n✓\n\n\n\n\n\nResearch Report #02\n✓\n✓\n✓\n✓\n\n\n\n\n\nTest #03\n✓\n\n✓\n✓\n✓\n\n\n\n\nResearch Report #03\n✓\n\n✓\n✓\n✓\n\n✓\n\n\nCourse Competition\n✓\n✓\n✓\n✓\n✓\n✓\n\n\n\nIn-Class Presentation\n\n\n\n\n✓\n✓\n✓"
  },
  {
    "objectID": "objectives.html#program-learning-goals",
    "href": "objectives.html#program-learning-goals",
    "title": "STA 9890 - Course Learning Objectives",
    "section": "Program Learning Goals",
    "text": "Program Learning Goals\nThis course contributes to the program learning goals of several MS programs offered by the Zicklin School of Business.\n\nMS in Business Analytics\nThis course is intended to contribute to the following Program Learning Goals for the MS in Business Analytics:\n\n\n\n Learning Goal\nMS BA Learning Goal\nDescription\n\n\n\n\n\nData Management\nStudents will be able to apply methods, tools, and software for acquiring, managing/storing, and accessing structured and unstructured data. Students will also demonstrate knowledge of the strategic uses of data.\n\n\n✓\nFoundational Statistical / Quantitative Skills\nStudents will be able to prepare data for statistical analysis, perform basic exploratory and descriptive analysis as well as employ foundational statistical techniques needed to analyze data.\n\n\n✓\nAdvanced Statistical/Quantitative Skills\nStudents will be able to build and interpret advanced predictive models. Students will be able to combine business rules and mathematical models to optimize business decisions from data.\n\n\n✓\nEthical Awareness\nStudents will be able to articulate an understanding of ethical issues in all phases of business analytics with particular emphasis on the new possibilities afforded by the emergence of big data.\n\n\n✓\nProfessional Communication\nStudents will be able to explain complex analytical models and their results orally and in writing to technical and non-technical/lay audiences.\n\n\n✓\nKnowledge Integration\nStudents will be able to apply the three key types of analytics (descriptive, predictive, and prescriptive) in a business domain to add value to business decision-making.\n\n\n\n\n\nMS in Quantitative Methods & Modeling\nThis course is intended to contribute to the following Program Learning Goals for the MS in Quantitative Methods & Modeling:\n\n\n\n Learning Goal\nMS QMM Learning Goal\nDescription\n\n\n\n\n✓\nOperations Research & Mathematical Modeling\nStudents will be able to effectively model, evaluate, and solve quantitative (business) problems using quantitative modeling methods (e.g., deterministic and probabilistic operations research techniques).\n\n\n✓\nStatistics\nStudents will be able to correctly apply appropriate statistical methods when defining, solving, and analyzing problems.\n\n\n✓\nTechnology Competency\nStudents will be able to use current technological tools, including spreadsheets and specialized software, when solving problems.\n\n\n✓\nProfessional Communication\nStudents will be able to effectively communicate their problem solving methods and solutions to technical and non-technical audiences.\n\n\n\n\n\nMS in Statistics\nThis course is intended to contribute to the following Program Learning Goals for the MS in Statistics:\n\n\n\n Learning Goal\nMS Statistics Learning Goal\nDescription\n\n\n\n\n✓\nGeneral Statistical Competence\nStudents will be able to apply appropriate probability models and statistical techniques when analyzing problems form business and the other fields.\n\n\n✓\nStatistical Practice\nStudents will become familiar with the standard tools of statistical practice for multiple regression, along with the tools of a subset of specialized statistical areas such as multivariate analysis, applied sampling, time series analysis, experimental design, data mining, categorical analysis, and/or stochastic processes.\n\n\n✓\nTechnology Competency\nStudents will learn to use one or more of the benchmark statistical software platforms, such as SAS or R."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "STA 9890 - Handouts and Additional Notes",
    "section": "",
    "text": "Supplemental course notes and links to useful external resources will be posted here.\n\nSupplemental Lecture Notes\n\nWeek 1 - Course Overview & Introduction to ML (2025-01-28)\n\nInstructor Notes\n\n\n\nWeek 2 - Regression I (2025-02-04)\n\nInstructor Notes\n\n\n\nWeek 3 - Regression II (2025-02-11)\n\nInstructor Notes\n\n\n\nWeek 4 - Regression III (2025-02-25)\n\nInstructor Notes\n\n\n\nWeek 5 - Introduction to Classification (2025-03-04)\n\nInstructor Notes\n\n\n\nWeek 6 - Classification I (2025-03-11)\n\nInstructor Notes\n\n\n\nWeek 7 - Classification II (2025-03-18)\n\nInstructor Notes\n\n\n\nWeek 8 - Ensemble Learning & Resampling Methods (2025-03-25)\n\nInstructor Notes\n\n\n\nWeek 9 - Tree-Based Methods (2025-04-01)\n\nInstructor Notes\n\n\n\nWeek 10 - Introduction to Unsupervised Learning (2025-04-08)\n\nInstructor Notes\n\n\n\nWeek 11 - Unsupervised Learning I (2025-04-22)\n\nInstructor Notes\n\n\n\nWeek 12 - Unsupervised Learning II (2025-04-29)\n\nInstructor Notes\n\n\n\nWeek 13 - Introduction to Generative Models (2025-05-06)\n\nInstructor Notes"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "STA 9890 - Research Reports",
    "section": "",
    "text": "In lieu of traditional homework, STA 9890 has three “research reports” (one for each unit of the course). These research reports are intended to help you develop your skills in the computational and methodological aspects of Statistical Machine Learning.\nEach Research Report must be submitted as a fully-typed PDF using the course Brightspace. (No handwritten work will be graded.) Each report must include all code used and should have several figures. Reports should be 6-8 pages, double spaced.\n\nResearch Reports\n\nResearch Report #01: Bias and Variance in Linear Regression\nDue Dates:\n\nReleased to Students: 2025-02-04\nSubmission Deadline: 2025-03-07 11:45pm ET\n\nIn Research Report #01, you will dig into the oft-cited claim that Ordinary Least Squares is a Best Linear Unbiased Estimator (BLUE). In classical statistics, the BLUE property is often used as an argument of optimality, implying that we can’t beat OLS, so we shouldn’t even try. As you will see, this optimality of OLS is quite overstated: OLS can be beaten quite easily whenever its assumptions are violated, whenever non-linear estimators are allowed, or whenever bias is permitted (taking “Best” to mean “minimum MSE” instead of “minimum variance”).\nThese findings may seem a bit abstract, but they get at the heart of almost every method and principle we will cover in this course. In this project, in addition to getting a better understanding of what BLUE does and does not mean, you will learn to:\n\nimplement gradient descent methods\ndesign Monte Carlo simulations to assess bias and variance\nfind optimal values of tuning parameters using cross-validation\n\n\n\nResearch Report #02: Ensemble Learning Techniques for Fair Classification\nDue Dates:\n\nReleased to Students: 2025-03-11\nSubmission Deadline: 2025-04-18 11:45pm ET\n\nIn Research Report #02, you will apply some of the tools we have developed to the problem of fairness in machine learning (FairML). While not a core topic for this course, this exercise is useful to see how the core idea of this course–regularization, optimization, etc.–can be applied to interesting and novel questions. In this project, you will also engage critically with a newly proposed ML method and investigate i) whether it truly does what it claims to; ii) whether it can be efficiently and reliably implemented; and iii) the degree to which (if any!) it solves your problem of interest. As working Data Scientists and Business Analysts, you may not think of yourselves as researchers, but knowing how to read and critically evaluate cutting-edge work will let you maintain and enhance your skills throughout your career.\n\n\nResearch Report #03: Sparse Principal Components Analysis\nDue Dates:\n\nReleased to Students: 2025-04-22\nSubmission Deadline: 2025-05-09 11:45pm ET\n\nIn Research Report #03, you will explore sparse PCA and apply it to a data set of interest. As you do so, you will see how a modern machine learning principle (sparsity) can be used to improve a classical statistical technique like PCA to get ‘the best of both worlds.’ Because our focus here is on an unsupervised method, this report should be careful to consider interpretation and validation of the resulting PCs, as standard validation techniques for supervised methods cannot be applied."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 9890 - Statistical Learning for Data Mining",
    "section": "",
    "text": "Welcome to the course website for STA 9890 - Statistical Learning for Data Mining (Spring 2025)!\nSTA 9890 is a survey of Statistical Machine Learning targeted at students in the MS in Business Analytics, MS in Statistics, and MS in Quantitative Methods programs.\nThis site hosts the unofficial Course Syllabus, Course Policies, and Course Learning Objectives. Official copies of these documents can be found on CUNY Brightspace.\nThis site also hosts descriptions of the Course Competition and Research Reports (Homework), as well as selected Handouts and Additional Notes that will be useful.\nThere are quite a few moving parts to this course, so this key dates file or the list of upcoming course activities below may be useful:\n\n\n\n\n\n\n\nA CSV file suitable for import into Google Calendar with all assignment deadlines can be found here.\nInstructor: Michael Weylandt"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 9890 - Course Syllabus",
    "section": "",
    "text": "All syllabus and course schedule provisions subject to change with suitable advance notice."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA 9890 - Course Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, this may result in scores equivalent to an A in an un-curved course receiving a lower grade in this course. In practice, the instructor will design course assessments to induce a range of scores and does not anticipate “down-curving” happening.↩︎\nThough ubiquitous, email is a remarkably ‘flaky’ service, providing the sender no way to guarantee their message arrives untampered and providing the recipient no way to guarantee the providence of a message received. (This is not quite true: there are tools for more secure email but they are somewhat more difficult to use and are not supported at CUNY.) Brightspace is integrated with CUNY’s Identity Verification Services and allows students to guarantee correct submission. Note that Brightspace does not, by default, send students an email confirming submission, but I believe this is an option that can be enabled on the student’s end.↩︎\nHaoyu Chen and Jiongjiong Yang. “Multiple Exposures Enhance Both Item Memory and Contextual Memory over Time”. Frontiers in Psychology 11. November 2020. DOI:10.3389/fpsyg.2020.565169↩︎\nFor this course, an average student is a student who enters the course with:\n\nFluency with statistical and numerical software at the level of (at least) STA 9750;\nFluency with univariate and multivariate regression at the level of (at least) STA 9700; and\nFamiliarity with probability and linear algebra;\n\nand is earning a B-range grade. If you have less background or are aiming for a higher grade, you should expect to commit proportionally more time to this course.\nIf you lack the prerequisite background listed above or simply wish to review it before the semester begins in earnest, please reach out to the instructor and I will be more than happy to provide supplementary readings.↩︎\nThe CUNY Graduate Center has a useful summary of these expectations. Baruch courses follow the same standards. See also CUNY Central Policy.↩︎\nAs a student, you have free access to GitHub CoPilot once you create a student GitHub account and register for the Student Developer Pack.↩︎\nIn theory, you can work with these frameworks at this sort of “low-level”, but they are likely to be far more cumbersome than using base R or standard numpy for these tasks. If you are interested in working with these frameworks in this manner in order to better familiarize yourself with the core computational abstractions, you may request instructor permission to do so.↩︎"
  },
  {
    "objectID": "reports/report03.html",
    "href": "reports/report03.html",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "",
    "text": "Released to Students: 2025-04-22\nSubmission: 2025-05-09 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report03.html#research-report",
    "href": "reports/report03.html#research-report",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "Research Report #03: Sparse Principal Components Analysis",
    "text": "Research Report #03: Sparse Principal Components Analysis\nIn the final Research Report of this course, you will investigate the intersection of sparsity, a form of interpretability, with unsupervised learning, the branch of ML focused on finding meaningful patterns in data.\nClassically, principal components analysis (PCA) is the cornerstone of dimension reduction methods. By reducing a large number of features to a smaller number of linearly independent (uncorrelated) components, PCA is often said to increase interpretability. This assumes a degree of “reification” - is the PC something meaningful on its own?\nIf the PC captures a true underlying factor, then perhaps we have found something worth interpreting; but PCA always finds some sort of factorization, even when there is no “true” underlying factor. In some circumstances, this PC becomes a thing in itself, e.g., IQ, but that cannot always be assumed. If we do not assume or define a meaning to the PC, it can only be understood as a combination of all of the features used to create it, hardly a paragon of interpretability.\nSparse PCA takes a different approach: rather than finding an interpretation of the linear combination of all features that captures the most variance, it seeks a linear combination of only a few features that explain nearly-the-most variance. Because only a few features are used, the resulting PC is more interpretable a priori than the output of classical PCA.\nAs background, you should read the paper by Weylandt and Swiler1 and review the Supporting Materials to see how Sparse PCA can be applied in a scientific setting. For more methodological background, see the references cited therein, especially the paper by Witten et al., as well as ISL §6.3.1 and §12.2 and SLS §8.1-8.2.2.\nProject Skeleton\nYour report should be divided into three sections, covering the following:\n\nBackground on PCA and Sparse PCA\nComputation - Implementing the Power Method and the Sparse Power Method for PCA\nImplementation and Assessment of Sparse PCA\n\nAt a minimum, these should include the following elements:\n\nBackground on PCA and Sparse PCA\n\nDerivation of PCA from Variance Maximization to Singular Value Decomposition\nProof of Convergence of the Power Method for SVD Computations\nModification of Classical Power Method for Sparsity\n\n\nComputation\n\nImplementation of Classical and Sparsified Power Method\nDiscussion of Convergence for Power Methods2\n\nHow might one tune the sparsity level used?\n\n\nImplementation and Assessment\n\nIn-Simulation: Construct simulations to compare the accuracy of classical and sparse PCA. Which does better when the “true” PCs are dense? When they are sparse? Does this depend on the sample size? How can we measure accuracy of the estimated PCs?\nOn Real Data: Identify a real data set where PCA can be applied. Apply both classical and sparse PCA and compare the results. Which PCA is better (in whatever sense)? Which PCA is more interepretable? Is there a way to validate your interpretation?\n\n\nAdditional Background\nThe Power Method is a classical approach to computing eigenvectors and, by extension, singular vectors. For this research report, you will build upon the singular vector variant, so I review the eigenvector variant here.\nSuppose \\(\\mathbf{\\Sigma} \\in \\mathbb{R}^{p \\times p}_{\\succ 0}\\) is a strictly positive definite \\(p \\times p\\) matrix. By the spectral theorem, it has an eigendecomposition:\n\\[\\mathbf{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\]\nwhere the eigenvalues \\(\\{\\lambda_i\\}\\) are decreasing and strictly positive: (\\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p\\)). If the eigenvalues are distinct (as they are with probability 1 for continuous-valued data observed noisly), then this decomposition is unique up to sign.3\nThe power method for computing the leading (highest eigenvalue) eigenvector of \\(\\mathbf{\\Sigma}\\) proceeds as follows:\n\nInitialize: select \\(\\mathbf{v}^{(0)}\\) as a random unit vector\nRepeat Until Convergence:\n\nMultiply: \\(\\tilde{\\mathbf{v}}^{(k+1)} = \\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\)\n\nNormalize: \\(\\mathbf{v}^{(k+1)} = \\tilde{\\mathbf{v}}^{(k+1)} / \\|\\tilde{\\mathbf{v}}^{(k+1)}\\|_2\\)\n\nIterate: \\(k \\leftarrow k + 1\\)\n\n\n\nReturn: At convergence, return the eigenvector \\(\\mathbf{v}^{(k)}\\) and the eigenvalue \\(\\lambda = \\|\\mathbf{\\Sigma} \\mathbf{v}^{(k)}\\|_2\\)\n\n\nAdditional eigenvectors can be found by “deflating” \\(\\mathbf{\\Sigma}\\) and applying the power method to \\[\\mathbf{\\Sigma}' = \\mathbf{\\Sigma} - \\lambda \\hat{\\mathbf{v}}\\hat{\\mathbf{v}}^{\\top}.\\]\nSo long as the eigenvalues of \\(\\mathbf{\\Sigma}\\) are distinct, the power method converges to the leading eigenvector as long as the initial guess is not perfectly orthogonal to the true eigenvector. If we pick our initial guess randomly, this is a very reasonable assumption. To see why this is the case, note that:\n\\[\\begin{align*}\n\\mathbf{v}^{(k)} &\\propto \\mathbf{\\Sigma}^k \\mathbf{v}^{(0)} \\\\\n                 &= \\left(\\sum_{i=1}^p \\lambda_i^k \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\right)\\mathbf{v}^{(0)} \\\\\n                 &= \\sum_{i=1}^p \\lambda_i^k \\langle\\mathbf{v}_i, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_i\n\\end{align*}\\]\nViewed here, you can see why the power method has its name. As \\(k \\to \\infty\\), the first term in this series becomes much much larger than the others, so we get\n\\[\\mathbf{v}^{(k)} \\buildrel \\sim \\over \\propto \\lambda_1^k \\langle\\mathbf{v}_1, \\mathbf{v}^{(0)}\\rangle \\mathbf{v}_1\\] After normalizing and dropping the scalar terms, we have\n\\[\\mathbf{v}^{(k)} \\to \\mathbf{v}_1\\]\nas desired. This argument also clarifies the two possible failure modes of the power method:\n\nIf the initial guess is unlucky and orthogonal to the true eigenvector, we have \\(\\langle \\mathbf{v}_1, \\mathbf{v}^{(0)} \\rangle = 0\\) so the \\(\\mathbf{v}_1\\) term drops out of the answer and we converge to the next eigenvector (unless we are orthogonal to that one as well).\nIf the eigenvalues are not distinct, then multiple terms become large and we get a strange ‘superposition’ of multiple eigenvectors. (Alternatively, there isn’t a ‘true’ right answer, but we get one possible eigenvector.)\nPotential Topics for Additional Research\nWhile the main focus of this report is on Sparse PCA, a suitably sparsified SVD can be used for a wide variety of multivariate analysis problems. You may wish to extend your analysis to other sparse multivariate methods, such as Sparse CCA or Sparse PLS.\nAlternatively, a huge number of approaches to Sparse PCA have been proposed, not all of which are based on the power method. You may wish to compare the approach I prefer (projected power method) with other approaches: be sure to consider i) accuracy; ii) interpretability; and iii) computational ease / efficiency in your comparison.\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.4 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report03.html#footnotes",
    "href": "reports/report03.html#footnotes",
    "title": "STA 9890 Research Report #03: Sparse Principal Components Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\nM. Weylandt and L.P. Swiler. “Beyond PCA: Additional Dimension Reduction Techniques to Consider in the Development of Climate Fingerprints”. Journal of Climate 37(5), p.1723-1735. 2024. DOI: 10.1175/JCLI-D-23-0267.1. Direct Link↩︎\nRecall that PCs are only defined “up to sign”. If \\(\\hat{\\mathbf{u}}\\) is a valid PC, so is \\(-\\hat{\\mathbf{u}}\\). You will need to modify your usual convergence checks to account for this.↩︎\nNote that we can substitute \\(\\mathbf{v}_i \\to -\\mathbf{v}_i\\) without changing the result since the minus signs will cancel in \\((-\\mathbf{v}_i)(-\\mathbf{v}_i)^{\\top} = \\mathbf{v}_i\\mathbf{v}_i^{\\top}\\).↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  },
  {
    "objectID": "reports/report01.html",
    "href": "reports/report01.html",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "",
    "text": "Released to Students: 2025-02-04\nSubmission: 2025-03-07 11:45pm ET on Brightspace\n\nEstimated Time to Complete: 9 Hours"
  },
  {
    "objectID": "reports/report01.html#research-report",
    "href": "reports/report01.html#research-report",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "Research Report #01: Bias and Variance in Linear Regression",
    "text": "Research Report #01: Bias and Variance in Linear Regression\nIn this research report, you will dive deeply into the bias-variance trade-off and the BLUE-ness (or lack thereof) of ordinary least squares.\nProject Skeleton\nYour report should be divided into four sections, covering the following:\n\nTheoretical background\nComputation - Gradient Descent and Weight Decay\nBias and Variance Under Linear Data Generating Processes (DGP)\nBias and Variance Under Non-Linear DGP\n\nAt a minimum, these should include the following elements:\n\nTheoretical Background\n\n\nRigorous Statement and Proof of the Bias-Variance Decomposition\n\\[\\mathbb{E}[\\text{MSE}] = \\text{Bias}^2 + \\text{Variance} (+ \\text{Irreducible Noise})\\]\nDiscuss how this statement is to be interpreted in the context of parameter estimation (when the underlying DGP is linear) and in the context of (possibly misspecified) prediction. Be sure to clarify when the “Irreducible Noise” term is needed.\n\nProof of the “BLUE” property of OLS with clear statement of the relevant assumptions. Take care to differentiate which assumptions are required for “B” and which are required for “U”.\n\n\nComputation\n\nDerivation and implementation of the ‘closed-form’ matrix expression for OLS\nDerivation and implementation of the ‘closed-form’ matrix expression for Ridge Regression\nDerivation and implementation of a gradient descent method for OLS\nDerivation and implementation of a ‘weight decay’ modification for OLS gradient descent.\nEmpirical demonstration of the equivalance of OLS+Weight Decay with Ridge Regression\n\n\nBias and Variance Under Linear DGP\n\nPosit a Linear DGP\nUsing Monte Carlo simulations:\n\nShow that OLS is unbiased under this DGP\nCompute the variance of a given regression coefficient\nCompute the in- and out-of-sample prediction MSE\nShow how the variance, in-sample, and out-of-sample MSE change with the sample size, \\(n\\)\n\n\n\nCompare against ridge regression\n\nShow that RR is biased under this DGP\nCompare the variance of OLS and RR\nDemonstrate the MSE Existence Theorem for Ridge Regression\nCompare the MSE of RR and OLS as a function of the sample size \\(n\\)\n\n\n\n\n\nBias and Variance Under Non-Linear DGP\n\nPosit a Non-Linear DGP\nUsing simulation, determine the “best approximate” linear regression coefficients: that is, find the OLS coefficients minimizing MSE\nShow that the estimated OLS coefficients converge to the “best approximate” coefficients as the sample size \\(n\\) increases\nCompute the MSE of RR in simulation and then use 5-fold cross-validation to determine the optimal regularization level\nCompare the MSE of RR and OLS in the non-linear setting\n\n\n\nFinally, note that because we are interested in bias, variance, and MSE – each of which is defined with respect to a (known) DGP – we only use statistical simulation for this project. In Report #02 and Report #03, you will apply methods to real (non-simulated) data.\nAdditional Background\nGradient Descent methods fit models by taking small steps in the direction opposite the gradient of the loss function. (Recall the gradient points in the direction of steepest increase, so moving in the opposite direction leads to a decrease in error.)\nAbstractly, given a differentiable loss function, \\(\\mathcal{L}(\\beta)\\), gradient descent proceeds as follows:\n\nSelect initial guess \\(\\beta^{(0)}\\) and set \\(k=0\\)\n\nRepeat until convergence:\n\n\nCompute gradient \\(\\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}\\)\n\nUpdate guess using a gradient step \\[\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}\\]\n\nIncrement \\(k := k+1\\)\n\nCheck convergence:\n\nParameter convergence: if \\(\\beta^{(k+1)} \\approx \\beta^{(k)}\\), we have converged.\nObjective convergence: if \\(\\mathcal{L}(\\beta^{(k+1)}) \\approx \\mathcal{L}(\\beta^{(k)})\\), we have converged\n\n\n\n\nAt convergence, return \\(\\beta^{(k+1)}\\)\n\n\nThis scheme is pretty easy to implement, but there are a few points where it behooves you to be careful:\n\nHow do we check the approximate equality in the convergence check?\nWe will almost never get exact equality, so you need to pick a norm and a tolerance.\nHow do we choose the step-size \\(c\\)? Much has been written about this; for our purposes it suffices to take a very small value of \\(c\\) and keep it constant.\nWhat if we never reach convergence? Should we stop after some (large) number of iterations?\n\nThe method of weight-decay modifies the gradient update as follows:\n\\[\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}} - \\omega \\beta^{(k)}\\]\nHere \\(\\omega\\) is another small constant scalar value. By subtracting off a fraction of \\(\\beta^{(k)}\\) at each step, weight decay prevents the estimate from ever getting to be too big (hence the name). In the case of OLS, this can be shown to be equivalent to a form of ridge regression.1\nThe MSE Existence Theorem for Ridge Regression comes in two forms:\n\n\nEstimation Variant. Let \\[\\text{MSE}_i(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}}[(\\hat{\\beta_i} - \\beta_i^*)^2]\\] denote the mean squared estimation error in the \\(i^\\text{th}\\) component of \\(\\hat{\\beta}\\) as an estimator of the true regression coefficients \\(\\beta^*\\). Here the expectation is taken over random realizations of both the design matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{y}\\), as well as the estimator \\(\\hat{\\beta}\\) which is a function of \\(\\mathbf{X}, \\mathbf{y}\\).\nThen, there exists some positive value \\(\\lambda &gt; 0\\) such that\n\\[\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) &gt; \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})\\]\nwhere \\(\\hat{\\beta}_{\\text{RR}, \\lambda}\\) is the ridge regression estimate with regularization parameter \\(\\lambda\\).\n\n\nPrediction Variant. Let \\[\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}, \\mathbf{y}'}[\\|\\mathbf{X}\\hat{\\beta} - \\mathbf{y}'\\|_2^2\\] denote the mean squared prediction error associated with the estimator \\(\\hat{\\beta}\\). Here the expectation is taken over random realizations of both the design matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{y}\\) used for training, as well as the test data vector \\(\\mathbf{y}'\\) which must be drawn from the same DGP.\nThen, there exists some positive value \\(\\lambda &gt; 0\\) such that\n\\[\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) &gt; \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})\\]\nwhere \\(\\hat{\\beta}_{\\text{RR}, \\lambda}\\) is the ridge regression estimate with regularization parameter \\(\\lambda\\).\n\n\nIt is important to note that both of these are statements in expectation: OLS may perform better than RR on a single realization, but in average - over many realizations - suitably-tuned RR will perform better.\nAlso, we note that the MSE Existence Theorem does not give any practical advice on selecting \\(\\lambda\\); we have to fall back on our standard approaches, such as cross-validation, for doing so.\nFinally, note that the prediction version of the MSE Existence Theorem only holds when we look at test (out-of-sample) prediction accuracy. OLS is the optimal linear method for in-sample MSE always and forever.\nPossible Topic(s) for Additional Research\nRecent ML research has focused on the case of overparameterized models: that is, models with more parameters than data points used for training. We have already seen ridge regression as one way to deal with this problem. Another approach is to simply use an iterative method, such as gradient descent, and stop it when the training error reaches zero. (This is a non-unique global minimum for the convex OLS problem.) How does this procedure compare? Does it complicate our understanding of the bias-variance trade-off?\nAnother recent line of research generalizes the “BLUE” property, showing that OLS is in an appropriate sense “BUE” - it is still optimal even if we allow for (certain types of) non-linear estimators as well. Can you show this?\nSubmission Instructions\nSubmit your research report as a PDF of 6-8 pages on Brightspace.2 Your submission should include all essential code, e.g., code used to fit models or to perform simulations, but may omit incidental code, e.g. code used to create figures or import data.\nYou are required to implement ML methods by hand; use of ‘built-in’ ML functionality is disallowed unless authorized by instructor. You may use built-in linear algebra subroutines and are not required to, e.g., implement matrix multiplication from scratch. While you must use your own implementation of ML methods, it is smart to compare against existing ‘reference’ implementations.\nYour report should be in the style of a technical “blog post”, accessible to someone who is just getting into Machine Learning but has solid background in the prerequisite subjects (e.g., you on the day before this class started). Make sure to (rigorously) define and prove all results used. You should also cite relevant authoritative resources, e.g., the recommended textbooks, where appropriate.\nEach student must submit an individual and independently written report, but you are encouraged to work together with your peers to understand methods, design simulations, review relevant literature, etc. I strongly recommend having a classmate “red-team” your report before submission to find unclear writing, sub-standard figures and tables, unconvincing simulations, incorrect code, etc. Per the course’s Academic Integrity Policy, you must explicitly acknowledge all collaborators in your submission.\nGrading\nYour submission will be evaluated holistically and graded out of 100 points. The following rubric will guide this holistic evaluation, but the instructor may deviate as necessary to accurately grade the final submission.\n\n\nReport Element\nExcellent.  “A-” to “A” (90% to 100%)\nGreat.  “B-” to “B+” (80% to 89%)\nAverage.  “C” to “C+” (73% to 79%)\nPoor.  “D” to “C-” (60% to 72%)\nFailure.  “F” (0% to 59%)\n\n\n\nPresentation (15%)\nReport has excellent formatting, with particularly effective tables and figures. Tables and Figures are “publication-quality” and clearly and succinctly support claims made in the body text.\nReport has strong formatting; tables and figures make their intended points, but do not do so optimally.\nFormatting is average; tables and figures do not clearly support arguments made in the text and/or are not “publication quality”\nPoor formatting distracts from substance of report. Tables and Figures exhibit significant deficiencies in formatting.\n\nFormatting prohibits or significantly impairs reader understanding.\n\n\nProject Skeleton (15%)\n\nReport includes all required elements and goes significantly deeper than required by the project skeleton in a manner that provides additional insights.\nMathematical definitions and proofs are clearly stated.\n\n\nReport includes all required elements and dives deeper into the topic, but does not generate additional insights. (E.g., additional simulations showing the same phenomena)\nMathematical definitions and proofs are correctly stated, but clarity could be improved.\n\n\nReport includes all required elements.\nMathematical definitions and proofs are essentially correct, but difficult to understand and/or contain trivial errors.\n\n\nReport does not include all required elements, but is still able to capture key points.\nMathematical definitions and proofs contain significant, but non-fatal, errors.\n\n\nReport fails to adequately address the topic.\nMathematical definitions and proofs are fundamentally incorrect.\n\n\n\nAlgorithms and Computational Efficiency (20%)\nReport implements all methods efficiently with high-quality, well-formatted and performant code.\nReport implements all methods efficiently with acceptable code quality.\nReport implements all methods, but does not do so efficiently and/or with substandard code.\nReport uses built-in methods instead of implementing methods from scratch\nCode does not appear to run properly / insufficient code submitted.\n\n\nMethodological Aspects and Simulation Design(20%)\nMethods are accurately and correctly implemented in a robust / bullet-proof manner, designed to responsibly check for and address possible modes of failure. Simulations clearly and efficiently support all claims.\nMethods are accurately and correctly implemented, but are not robust to failure. Simulations clearly and efficiently support all claims.\nMethods are implemented with minor harmless errors and/or poor validation. Simulations do not fully support claims.\n\nMethods are implemented with significant errors leading to incorrect results.\nSimulations do not give sufficient support for key claims.\n\nMethods are not correctly implemented. Simulations do not support claims.\n\n\nCommunication and Writing (20%)\nReport exhibits excellent written communication, making all points exceptionally clearly and at the level of a quality academic publication. Code, results, and text are skillfully woven together.\nReport exhibits great written communication, all points are made clearly without notable grammatical errors. Code, results, and text are neatly tied together.\nReport exhibits solid written communication, key points are made understandably and any grammatical errors do not impair understanding. Code, results, and text could be better integrated, but it is clear which elements relate.\nWritten communication is below standard: points are not always understandable and/or grammatical errors actively distract from content. Code, results, and text are not actively integrated, but are generally located ‘near’ each other in a semi-systematic fashion.\nWritten communication is far below standard, possibly bordering on unintelligible. Large blocks of code are shown without any relevant results or text.\n\n\nContextualization (10%)\nReport draws on relevant, modern research literature to give context to its findings and to broaden its scope.\nReport draws on standard textbooks and/or classical research literature to give context to its findings and to broaden its scope.\nReport cites textbooks to give context, but does not take a ‘big picture’ view.\nReport gives appropriate context, but lacks citations.\nReport gives limited, insufficient context.\n\n\n\n\nThis work ©2025 by Michael Weylandt is licensed under a Creative Commons BY-NC-SA 4.0 license."
  },
  {
    "objectID": "reports/report01.html#footnotes",
    "href": "reports/report01.html#footnotes",
    "title": "STA 9890 Research Report #01: Bias and Variance in Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nA somewhat remarkable finding of much recent research is that many ‘new’ regularization methods pioneered for fitting neural networks have essentially the same effect as ridge (\\(\\ell_2^2\\) or Tikhonov) regularization: weight decay, drop out, early stopping, mini-batching and mini-patching. Sometimes it’s hard to beat the classics.↩︎\nIf you wish to use an alternate submission format to allow interactive and/or animated elements, please seek pre-approval from the instructor.↩︎"
  }
]