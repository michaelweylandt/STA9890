{
  "hash": "a4493fa6f3be19fcde5808d589c3a0b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"2\"\nauthor: \"Michael Weylandt\"\ntopic: \"Regression I\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\nfreeze: \"auto\"\n---\n\n\n\n$$\\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\by}{\\mathbf{y}} \\newcommand{\\bz}{\\mathbf{z}} \\newcommand{\\bA}{\\mathbf{A}} \\newcommand{\\bb}{\\mathbf{b}} \\newcommand{\\bc}{\\mathbf{c}} \\newcommand{\\bH}{\\mathbf{H}} \\newcommand{\\bI}{\\mathbf{I}}$$\n\n\n\n\n\n\n## Linear Algebra Review\n\nIn mathematics, a _vector_ - random or otherwise - is a fixed-length\n_ordered_ collection of numbers. When we want to be precise about the\nsize of a vector, we often call it a \"tuple\", *e.g.*, a length-three vector\nis a \"triple\", a length-four vector is a \"4-tuple\", a length-five vector\nis a \"5-tuple\" *etc.*. \n\nSo, these are all vectors: \n\n- $(3, 4)$\n- $(1, 1, 1)$\n- $(1, 5, 6, 10)$\n\nWhen we want to talk about the set of vectors of a given size, we use\nthe _Cartesian product_ of sets. For two sets, $A, B$, the product set\n$A \\times B$ is the set of all pairs, with the first element from $A$ and\nthe second from $B$. In mathematical notation, \n\n$$A \\times B = \\left\\{(a, b): a \\in A, b \\in B\\right\\} $$\n\nThis _set-builder_ notation is read as follows: \"The Cartesian Product of $A$\nand $B$ is the set of all pairs $(a, b)$ such that $a$ is in $A$ and $b$\nis in $B$.\" \n\nIf $A$ and $B$ are the same set, we define a Cartesian _power_ as follows:\n\n$$A^2 = A \\times A = \\left\\{(a_1, a_2): a_1 \\in A, a_2 \\in A\\right\\}$$\n\nNote that even though the sets $A$ and $A$ in this product are the same, \nthe elements in each pair may vary. For example, if $A = \\{1, 2, 3\\}$, we have\n\n$$A^2 = \\left\\{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3,2), (3, 3)\\right\\}$$\n\nNote that vectors are _ordered_ pairs so $(2, 1) \\neq (1, 2)$. From here, it\nshould be pretty easy to convince yourself that set sizes play nicely with\nCartesian products: \n\n- $|A \\times B| = |A| |B|$\n- $|A^k| = |A|^k$\n\nThe most common set of vectors we use are those where each element is an\narbitrary real number. The set of vectors of length $n$ ($n$-tuples) is thus\n$\\R^n$. We rarely mix vectors of different lengths, so we don't really have\na name or notation for the \"combo pack\" $\\R^2 \\cup \\R^3 \\cup \\R^4$. \n\nConventionally, vectors are written in bold (if on a computer) or with a\nlittle arrow on top (hand written): so a vector called \"x\" would be\ndenoted $\\mathbf{x}$ or $\\vec{x}$. The elements of $\\bx$ are denoted by \nsubscripts $\\bx = (x_1, x_2, \\dots, x_n)$. \n\n### Vector Arithmetic\n\nWe have three arithmetic operations we can perform on general vectors. \nThe simplest is _scalar multiplication_. A _scalar_ is a non-vector number, \n*i.e.*, a 'regular' number. Scalar multiplication consists of applying the\nscalar independently to each element of a vector. \n\n$$\\alpha \\bx = \\alpha(x_1, x_2, \\dots, x_n) = (\\alpha x_1, \\alpha x_2, \\dots, \\alpha x_n)$$\n\nFor example, if $\\bx = (3, 4)$ and $\\alpha = 2$, we have\n$$\\alpha \\bx = (6, 8)$$\n\nNote that the output of scalar multiplication is always a vector of the\nsame length as the input. \n\nWe also have the ability to add vectors. This again is performed element-wise.\n\n$$\\bx + \\by = (x_1, \\dots, x_n) + (y_1, \\dots, y_n) = (x_1 + y_1, \\dots, x_n + y_n) $$\n\nNote that we can't add vectors of different lengths (recall our \"no mixing\"\nrule) and the output length is always the same as the input lengths. \n\nFinally, we have the vector _inner product_, defined as: \n\n$$\\langle \\bx, \\by \\rangle = x_1y_1 + x_2y_2 + \\dots + x_ny_n $$\n\nYou might have seen this previously as the \"dot\" product. The inner product\ntakes two length-$n$ vectors and gives back a scalar. This structure might\nseem a bit funny, but as we'll see below, it's actually quite useful. \n\nYou might ask if there's a \"vector-out\" product: there is one, with the\nfancy name \"Hadamard product\", but it doesn't play nicely with other tools, \nso we don't use it very much. \n\nThese tools play nicely together: \n\n- $\\alpha(\\bx + \\by) = \\alpha \\bx + \\alpha \\by$ (Distributive)\n- $\\langle \\alpha \\bx, \\by \\rangle = \\alpha \\langle \\bx, \\by \\rangle$ (Associative)\n- $\\langle \\bx, \\by \\rangle = \\langle \\by, \\bx \\rangle$ (Commutative)\n\n### Vector Length and Angle\n\nWe sometimes want to think about the \"size\" of a vector, analogous to\nthe absolute value of a scalar. In scalar-world, we say \"drop the sign\"\nbut there's not an obvious analogue to a sign for a vector. For instance, \nif $\\bx = (3, -4)$ is $\\bx$ \"positive\", \"negative\" or somewhere in beetween? \n\nWe note a trick from scalar-land: $|x| = \\sqrt{x^2}$. We can use the same idea\nfor vectors: \n\n$$ \\|\\bx\\| = \\sqrt{\\langle \\bx, \\bx\\rangle} = \\sqrt{\\sum_{i=1}^n x_i^2}$$\n\nThis quantity, $\\|\\bx\\|$, is called the _norm_ or _length_ of a vector.\nWe use the double bars to distinguish it from the absolute value of\na scalar, but it's fundamentally the same idea. \n\nIn $\\R^2$, we recognize this formula for length as the Pythagorean theorem: \n\n$$ \\|(3, 4)\\| = \\sqrt{3^2 + 4^2} = \\sqrt{25} = 5 $$\n\nWe also sometimes want to define the _angle_ between two vectors. We can\ndefine this as: \n\n$$ \\cos \\angle(\\bx, \\by) = \\frac{\\langle \\bx, \\by\\rangle}{\\|\\bx\\|\\|\\by\\|} \\Leftrightarrow \\angle(\\bx, \\by) = \\cos^{-1}\\left(\\frac{\\langle \\bx, \\by\\rangle}{\\|\\bx\\|\\|\\by\\|}\\right)$$ \n\nWe won't use this formula too often for implementation, but it's good to have\nit for intuition. In particular, we note that angle is a proxy for sample\ncorrelation, justifying the common vernacular of \"orthogonal\", meaning \"at right\nangles\", for \"uncorrelated\" or \"unrelated.\"\n\n### Matrices\n\nAn $n$-by-$p$ array of numbers is called a _matrix_; here the first dimension\nis the number of rows while the second is the number of columns. So \n$$\\bA = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix}$$\nis a 3-by-2 matrix. We denote the set of matrices of a given size as \n$\\R^{n \\times p}$, extending slightly the notation we use for vectors. \n\nIn this course, we will use matrices for two closely-related reasons: \n\n1) To organize our data\n2) To specify and manipulate linear models\n\nSpecifically, if we have $n$ training samples, each of $p$ covariates\n(predictors), we will arrange them in a matrix traditionally called \n$\\bX \\in \\R^{n \\times p}$. Here, each _row_ of $\\bX$ corresponds to an \nobservation. Statisticians tend to call this matrix a _design_ matrix because\n(historically) it was something designed as part of an experiment; the name \ngot carried forward into the observational (un-designed) setting. You may also\nhear it called the 'data matrix'. \n\nSuppose we have a design matrix $\\bX \\in \\R^{n \\times p}$ and a vector of \nregression coefficients $\\mathbf{\\beta} \\in \\R^p$. We can use _matrix_\nmultiplication to make predictions about all observations simultaneously. \n\nSpecifically, recall that the standard (multivariate) linear model looks like: \n\n$$\\hat{y} = \\sum_{i=1}^p x_i\\beta_i$$\n\nFor a single observation, this can be written in vector notation as\n\n$$\\hat{y} = \\bx^{\\top}\\bbeta$$ \n\nIf we have $n$ observations, we can stack them in a vector as: \n\n$$\\begin{pmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_n \\end{pmatrix} = \n\\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\\\bx_n^{\\top}\\bbeta \\end{pmatrix}$$\n\nWe can connect this with our design matrix $\\bX$ by using the above as a \ndefinition of matrix-vector multiplication: \n\n$$\\hat{\\by} = \\bX\\bbeta =  \n\\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\\\bx_n^{\\top}\\bbeta \\end{pmatrix}$$\n\nHere, matrix multiplication proceeds by taking the inner product of each _row_\nwith the (column) vector $\\bbeta$. This may feel a bit unnatural at first, but\nwith a bit of practice, it will become second nature. Note that we don't need\na transpose on $\\bX$: the multiplication 'auto-transposes' in some sense. \n\nIt is always helpful to keep track of dimensions when doing matrix\nmultiplication: checking that matrices and vectors have the right size is a\nuseful way to make sure you haven't done anything _too wrong_. In general, \nwe can only multiply a $m$-by-$n$ matrix with a $n$-by-$p$ matrix and the result\nis a $m$-by-$p$ matrix (the $n$-dimension gets reduced to a scalar by the inner\nproduct). Formally, we have something like\n\n$$ \\R^{m \\times n} \\times \\R^{n \\times p} \\to \\R^{m \\times p}$$\n\nFor purposes of this, you can always think of an $n$-vector as a $n$-by-1 \n\"column\" matrix, giving us: \n\n$$\\underbrace{\\bX}_{\\R^{n \\times p}} \\underbrace{\\bbeta}_{\\R^p} = \\underbrace{\\hat{\\by}}_{\\R^{n \\times 1}}$$\n\n### Spectral Properties of Symmetric Matrices\n\nAn important class of matrices we will consider are _symmetric_ matrices, which\nare just what the name sounds like. These come up in several key places in\nstatistics, none more important than the _covariance_ matrix, typically denoted\n$\\Sigma$. Recall that the covariance operator $\\mathbb{C}$ is symmetric\n($\\mathbb{C}[X, Y] = \\mathbb{C}[Y, X]$) so the covariance matrix of a random\nvector turns out to be symmetric as well. \n\nAnother common source of symmetric matrices is when a matrix is multiplied by\nits transpose: you should convince yourself that \n$\\bX^{\\top}\\bX \\in \\R^{p \\times p}$ is a symmetric matrix. \n\n\n\n\n\n\n## Ordinary Least Squares\n\nSuppose we want to fit a linear model to a given data set (for any\nof the reasons we discuss in more detail below): how can we\nchoose which line to fit? (There are infinitely many!)\n\nSince our goal is minimizing test error, and we hope training error\nis at least a somewhat helpful proxy for training error, we can pick\nthe line that *minimizes training error*.  To do this, we need to \ncommit to a specific measure of error. As the name *Ordinary Least\nSquares* suggests, OLS uses (mean) squared error as its target. \n\nWhy is MSE the right choice here? It turns out that MSE is very nice\ncomputationally, but the reason is actually more fundamental: given a\nrandom variable $Z$, suppose we want to minimize the $(Z - \\mu)$ for \nsome $\\mu$: it can be shown that\n\n$$ \\E[Z] = \\text{argmin}_{\\mu \\in \\R} \\E[(Z - \\mu)^2]$$\n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nProve this for yourself. \n\n*Hint*: Differentiate with respect to $\\mu$. \n\n:::\n\nThat is, the quantity that minimizes the MSE is the mean. So when we\nfit a line to some data by OLS, we are implicitly trying to fit to\n$\\E[y]$ - a very reasonable thing to do!\n\nSpecifically, given training data $\\mathcal{D} = \\{(\\bx_i, y_i)\\}_{i=1}^n$ where each $\\bx_i \\in \\R^p$, OLS finds $\\bbeta \\in \\R^p$ such that\n$$ \\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^p \\beta_jx_j\\right)^2 = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\bx^{\\top}\\bbeta\\right)^2$$\n\nSome possibly new notation here: \n\n  - $\\bx^{\\top}\\bbeta$ is the (inner) product of two vectors: defined as the sum of their elementwise products. \n  - Optimization problems: \n    - $$\\hat{x} = \\text{argmin}_{x \\in \\mathcal{C}} f(x)$$\n    - $$f_* = \\text{min}_{x \\in \\mathcal{C}} f(x)$$\n     \n    These problems say: find the value of $x$ in the set \n    $\\mathcal{C}$ that minimizes the function $f$. $\\text{argmin}$ \n    says 'give me the minimizer' while $\\min$ says\n    give me the minimum value'. These are related by \n    $f_* = f(\\hat{x})$\n    \n    The function $f$ is called the *objective*; the set $\\mathcal{C}$ \n    is called the *constraint set*.\n    \n*Ordinary Least Squares* refers to the use of an MSE objective\nwithout any additional constraints.\n\nNote the general structure of our approach here: \n\n- Define the loss we care about\n- Set up an optimization problem to minimize loss on the training set\n- Solve optimization problem\n\nML folk call this *empirical risk minimization* (ERM) since we're\nminimizing the risk (average loss) on the data we can see (the\ntraining data). Statisticians call this $M$-estimation, since it\ndefines an estimator by **M**inimization of a measure of 'fit'.\nWhatever you call it, it's a very useful 'meta-method' for coming\nup with ML methods. \n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\n\n1. How does this compare with *Maximum Likelihood Estimation*? \n\n2. We set up this ERM method using *mean squared error* - what happens \n   with other errors? Specifically, formulate this ERM for \n\n   - *mean absolute error*\n   - *mean percent error*\n   \n   and compare to OLS. \n   \n:::\n\nSo far we've set up OLS as \n$$ \\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\bx^{\\top}\\bbeta\\right)^2$$\n\nWe can clean this up to make additional analysis easier: \n\n- Let $$\\by = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$$ \n  be the (vertically stacked) vector of responses. \n- Next look at our predictions: \n  $$\\hat{\\by} = \\begin{pmatrix} \\bx_1^{\\top}\\bbeta \\\\ \\bx_2^{\\top}\\bbeta \\\\ \\vdots \\\\ \\bx_n^{\\top}\\bbeta \\end{pmatrix} = \\begin{pmatrix} \\bx_1^{\\top} \\\\ \\bx_2^{\\top} \\\\ \\vdots \\\\ \\bx_n^{\\top} \\end{pmatrix}\\bbeta = \\bX\\bbeta$$\n  \nHence, OLS is just $$\\hat{\\bbeta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n} \\|\\by - \\bX\\bbeta\\|_2^2$$ Here $\\|\\cdot\\|_2^2$ is the (squared Euclidean or $L_2$) norm of a vector: defined by $\\|\\bz\\|_2^2 = \\sum z_i^2$\n\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\n\n1. Do we actually need the $1/n$ part? \n\n2. Why is this called linear?\n\n:::\n\nWe've formulated OLS as \n$$\\hat{\\beta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{n} \\|\\by - \\bX\\bbeta\\|_2^2.$$ \nIn order to solve this, it will be useful to modify it slightly to\n$$\\hat{\\beta} = \\text{argmin}_{\\bbeta \\in \\R^p} \\frac{1}{2} \\|\\by - \\bX\\bbeta\\|_2^2$$ \n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nWhy is this ok to do?\n\n:::\n\nYou will show in [Report #01](../reports/repot01.html) that the solution is\ngiven by \n\n$$\\hat{\\beta} = (\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\by$$\n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nProve this for yourself. What conditions are required on $\\bX$ for the\ninverse in the above expression to exist?\n\n:::\n\nWith this estimate, our in-sample predictions are given by: \n\n$$\\hat{\\by} = \\bX\\hat{\\bbeta} = \\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\by $$\n\nThis says that our predictions are, in some sense, just a linear function of\nthe original data $\\by$. The matrix\n\n$$\\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top} = \\bH$$\n\nis sometimes called the 'hat' matrix because it puts a hat on $\\by$\n($\\hat{\\by}=\\bH\\by$). \n\n$\\bH$ can be shown to be a special type of matrix called a _projector_, meaning\nthat it has eigenvalues all 0 or 1. For the hat matrix specifically, we can show \nthat eigenvalues of $\\bH$ are $p$ zeros and $n - p$ ones. \n\nThis in turn implies that it is _idempotent_, meaning $\\bH^2 = \\bH\\bH = \\bH$. \n(To show this, simply express $\\bH$ in terms of its eigendecomposition.)\nWe can use this property to finally justify the in-sample MSE of OLS we have\ncited several times. \n\nThe in-sample MSE is given by: \n\n$$\\begin{align*}\n\\text{MSE} &= \\frac{1}{n}\\|\\by - \\hat{\\by}\\|_2^2 \\\\\n           &= \\frac{1}{n}\\left\\|\\by - \\bH\\by\\right\\|_2^2 \\\\\n           &= \\frac{1}{n}(\\by - \\bH\\by)^{\\top}(\\by - \\bH\\by) \\\\\n           &= \\frac{1}{n}(\\by^{\\top} - \\by^{\\top}\\bH)(\\by - \\bH\\by) \\\\\n           &= \\frac{\\by^{\\top}\\by - \\by^{\\top}\\bH\\by - \\by^{\\top}\\bH\\by + \\by^{\\top}\\bH\\bH\\by}{n} \\\\\n           &= \\frac{\\by^{\\top}\\by - \\by^{\\top}\\bH\\by}{n} \\\\\n           &= \\frac{\\by^{\\top}(\\bI - \\bH)\\by}{n} \\\\\n\\implies \\E[\\text{MSE}] &= \\E\\left[\\frac{\\by^{\\top}(\\bI - \\bH)\\by}{n}\\right] \\\\\n&= \\frac{1}{n}\\E\\left[\\by^{\\top}(\\bI - \\bH)\\by\\right]\n\\end{align*}$$\n\nTo finish this, we need to know that the [expectation of a symmetric quadratic \nform](https://en.wikipedia.org/wiki/Quadratic_form_(statistics)) satisfies \n\n$$\\bx \\sim (\\mu, \\Sigma) \\implies \\E[\\bx^{\\top}\\bA\\bx] = \\text{Tr}(\\bA\\Sigma) + \\mu^{\\top}\\bA\\mu$$\n\nfor any random vector $\\bx$ with mean $\\mu$ and covariance matrix $\\Sigma$. \n\nTo apply this above, we note $\\by \\sim (\\bX\\beta_*, \\sigma^2 \\bI)$, so we get\n\n$$\\begin{align*}\n\\E\\left[\\by^{\\top}(\\bI - \\bH)\\by\\right] &= \\text{Tr}((\\bI - \\bH) \\sigma^2 \\bI) + (\\bX\\bbeta_*)^{\\top}(\\bI - \\bH)(\\bX\\bbeta_*) \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}\\bX^{\\top}(\\bI - \\bH)\\bX\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bH\\bX)\\bbeta_* \\\\ \n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bX(\\bX^{\\top}\\bX)^{-1}\\bX^{\\top}\\bX)\\bbeta_* \\\\ \n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top}(\\bX^{\\top}\\bX - \\bX^{\\top}\\bX)\\bbeta_* \\\\ \n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) + \\bbeta_*^{\\top} \\mathbf{0}\\bbeta_* \\\\\n&= \\sigma^2 \\text{Tr}(\\bI - \\bH) \\\\\n&= \\sigma^2 (\\text{Tr}(\\bI) - \\text{Tr}(\\bH))\n\\end{align*}$$\n\nRecall that the trace is simply the sum of the eigenvalues, so this last term\nbecomes $\\sigma^2(n - p)$, finally giving us: \n\n$$\\E[\\text{MSE}] = \\frac{\\sigma^2(n-p)}{n}  = \\sigma^2\\left(1 - \\frac{p}{n}\\right)$$\n\nWhew! That was a lot of work! But can you imagine how much more work this would\nhave been without all of these matrix tools? \n\n::: {.callout-note title=\"Pause to Reflect\" collapse=\"true\"}\n\nMake sure you can justify every step in the derivation above. This is a\nparticularly long computation, but we will use the individual steps many more\ntimes in this course.\n\n:::\n\n## Bias-Variance Decomposition\n\nIn [Report #01](../reports/repot01.html), you will show that, under MSE\nloss, our expected test error can be decomposed as\n\n$$\\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n\nLet's show how we can analyze these quantities for a KNN *regression* problem. Here, we're using the 'regression' version of KNN since it\nplays nicely with MSE.[^tradeoff]\n\n[^tradeoff]: The Bias-Variance decomposition (and tradeoff) holds\napproximately for other loss functions, though the math is only this\nnice for MSE. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Don't attach the package to avoid weird aliasing issues\nknn.reg <- FNN::knn.reg \nargs(knn.reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (train, test = NULL, y, k = 3, algorithm = c(\"kd_tree\", \n    \"cover_tree\", \"brute\")) \nNULL\n```\n\n\n:::\n:::\n\n\n\nWe also need a 'true' function which we're trying to estimate. \nLet's use the following model:\n\n$$\\begin{align*}\nX &\\sim \\mathcal{U}([0, 1]) \\\\\nY &\\sim \\mathcal{N}(4\\sqrt{X} + 0.5 * \\sin(4\\pi * X), 0.25)\n\\end{align*}$$\n\nThat is, $X$ is uniform on the unit interval and $Y$ is a non-linear\nfunction of $X$ plus some Gaussian noise. \n\nFirst let's plot $X$ vs $\\E[X]$ - under MSE loss this is our 'best' (Bayes-optimal) possible guess. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyfun <- function(x) 4 * sqrt(x) + 0.5 * sinpi(4 * x)\n\nx <- seq(0, 1, length.out=101)\ny_mean <- yfun(x)\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     main=\"True Regression Function\", cex.lab=1.5)\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nTo generate training data from this model, we simply implement the PRNG components:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_train <- runif(25, 0, 1) # 25 training points\ny_train <- rnorm(25, mean=yfun(x_train), sd=0.5)\n\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\npoints(x_train, y_train)\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nWe have some variance of course, but we can still \"squint\" to get the\nright shape of our function. Let's see how KNN looks on this data. \n\nWe start with $K=1$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_train <- matrix(x_train, ncol=1)\nplot_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\nY_hat <- knn.reg(train=X_train, y=y_train, k=1, test=plot_grid)$pred\n\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\npoints(x_train, y_train)\n\nlines(plot_grid, Y_hat, col=\"red4\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nThis is not a great fit - what happens if we repeat this \nprocess may times? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\nfor(i in seq(1, 20)){\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    test_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    Y_hat <- knn.reg(train=X_train, y=y_train, k=1, test=test_grid)$pred\n    lines(test_grid, Y_hat, col=\"#FFAA0099\", lwd=0.5)\n}\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nClearly we have some variance!\n\nIf we repeat with a higher value of $K$, we see far less variance:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\nfor(i in seq(1, 20)){\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    Y_hat <- knn.reg(train=X_train, y=y_train, k=10, test=test_grid)$pred\n    lines(test_grid, Y_hat, col=\"#FFAA0099\", lwd=0.5)\n}\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nHow well does KNN do _on average_? \n\nThat is, if we could repeat this process (infinitely) many times, how\nwell would it recover the true regression function? Let's try $K=1$\nand $K=10$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y_mean, type=\"l\", \n     xlab=\"X\", ylab=\"E[X]\", \n     cex.lab=1.5)\n\n\n\nKNN_AVERAGE_PRED_K1 <- rowMeans(replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=1, test=test_grid)$pred\n}))\n\nlines(test_grid, KNN_AVERAGE_PRED_K1, col=\"red4\")\n\n\nKNN_AVERAGE_PRED_K10 <- rowMeans(replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=10, test=test_grid)$pred\n}))\n\nlines(test_grid, KNN_AVERAGE_PRED_K10, col=\"blue4\")\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nWe see here that, _on average_, KNN with $K=1$ (red) basically gets the\nfunction just right - no bias!\n\nOn the other hand, because KNN with $K=10$ smooths out the function, we\nsee systematic errors (here oversmoothing). That's some bias. \n\nSo which is better?\n- $K=1$ - High variance, but low bias\n- $K=10$ - Low variance, but high bias\n\nWe'll have to look at some test error to see. For now, we'll generate\nour test data exactly the same way as we generate our training data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKNN_K1_ERROR <- replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    \n    # Generate from same model as before\n    X_test <- matrix(runif(25, 0, 1), ncol=1) \n    y_test <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    y_hat <- knn.reg(train=X_train, y=y_train, k=1, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K1_MSE <- mean(rowMeans(KNN_K1_ERROR^2))\n\nKNN_K10_ERROR <- replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    plot_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    \n    # Generate from same model as before\n    X_test <- matrix(runif(25, 0, 1), ncol=1) \n    y_test <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    y_hat <- knn.reg(train=X_train, y=y_train, k=10, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K10_MSE <- mean(rowMeans(KNN_K10_ERROR^2))\n\ncbind(K1_MSE = KNN_K1_MSE, \n      K10_MSE = KNN_K10_MSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       K1_MSE K10_MSE\n[1,] 1.954541 1.46504\n```\n\n\n:::\n:::\n\n\n\n$K=10$ does better overall!\n\nBut does it do better _everywhere_ or are some parts of the problem\nbetter for $K=1$?\n\nNow we'll be systematic in our test data - spacing it equally on the\ngrid and computing 'pointwise' MSE: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKNN_K1_ERROR <- replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    # Generate from same model as before\n    test_grid <- seq(0, 1, length.out=101)\n    X_test <- matrix(runif(test_grid, 0, 1), ncol=1) \n    y_test <- matrix(rnorm(test_grid, mean=yfun(X_test), sd=0.5), ncol=1)\n    \n    y_hat <- knn.reg(train=X_train, y=y_train, k=1, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K1_MSE <- rowMeans(KNN_K1_ERROR^2)\n\nKNN_K10_ERROR <- replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    \n    test_grid <- seq(0, 1, length.out=101)\n    X_test <- matrix(runif(test_grid, 0, 1), ncol=1) \n    y_test <- matrix(rnorm(101, mean=yfun(X_test), sd=0.5), ncol=1)\n    \n    y_hat <- knn.reg(train=X_train, y=y_train, k=10, test=X_test)$pred\n    \n    err = y_test - y_hat\n})\n\nKNN_K10_MSE <- rowMeans(KNN_K10_ERROR^2)\n\nplot(KNN_K1_MSE, col=\"blue4\", pch=16, ylim=c(0, 1))\npoints(KNN_K10_MSE, col=\"red4\", pch=16)\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nIt looks like - for this set up at least - $K=10$ is better\n_everywhere_ but that's not always the case.\n\nPlay around with the size of the training data, the noise in the \nsamples, and the data generating function (`yfun`) to see if you \ncan get different behavior. \n\nAlso - is $K=10$ really the optimal choice here? What would happen\nif we changed $n$?\n\nSo, now that we have a good sense of (average) test error, can we verify our MSE decomposition? \n\nRecall\n$$\\begin{align*}\n\\E[\\text{MSE}] &= \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} \\\\\n\\text{ where } \\text{Bias}^2 &= \\E\\left[\\left(\\E[\\hat{y}] - \\E[y]\\right)^2\\right] \\\\\n&= \\left(\\E[\\hat{y}] - \\E[y]\\right)^2 \\text{ (Why can I drop the outer expectation?)} \\\\\n\\text{Variance} &= \\E\\left[\\left(\\hat{y} - \\E[\\hat{y}]\\right)^2\\right] \\\\\n\\text{Irreducible Error} &= \\E\\left[\\left(y - \\E[y]\\right)^2\\right]\n\\end{align*}$$\n\n(Make sure you understand these definitions and how they work together!)\n\nLet's work these out using all the tools we built before. \n\nFirst, for the bias: \n- we already have $\\E[y]$ - this is just the `yfun` we selected\n- we can compute $\\E[\\hat{y}]$ by running KNN many times and averaging the result\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\nKNN_AVERAGE_PRED_K1 <- rowMeans(replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    knn.reg(train=X_train, y=y_train, k=1, test=sample_grid)$pred\n}))\n\nKNN_BIAS_K1 <- KNN_AVERAGE_PRED_K1 - yfun(sample_grid)\nplot(sample_grid, KNN_BIAS_K1^2, col=\"red4\", \n     type=\"l\", main=\"Squared Bias of KNN with K=1\")\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nNot too much bias, but things do go a bit off the rails near the \nend points.\n\nNext, we can compute variance pointwise:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\nKNN_VARIANCE_K1 <- rowMeans(replicate(500, {\n    X_train <- matrix(runif(25, 0, 1), ncol=1)\n    y_train <- matrix(rnorm(25, mean=yfun(X_train), sd=0.5), ncol=1)\n    (knn.reg(train=X_train, y=y_train, k=1, test=sample_grid)$pred - KNN_AVERAGE_PRED_K1)^2\n}))\n\nplot(sample_grid, KNN_VARIANCE_K1, col=\"red4\", \n     type=\"l\", main=\"Variance of KNN with K=1\")\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nFor this data at least, the variance term is generally larger than the\nbias term: this is what we expect with a very flexible (high variance\n+ low bias) model like $1$-NN.\n\nFinally, irreducible error is just 0.25 everywhere (recall \n$y \\sim \\mathcal{N}(\\E[y], 0.25)$). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKNN_IE <- rowMeans(replicate(500, {\n    sample_grid <- matrix(seq(0, 1, length.out=101), ncol=1)\n    X_test <- matrix(sample_grid, ncol=1)\n    y_test <- matrix(rnorm(sample_grid, mean=yfun(X_test), sd=0.5), ncol=1)\n    y_best_pred <- matrix(yfun(X_test), ncol=1)\n    (as.vector(y_best_pred - y_test))^2\n}))\n\nplot(KNN_IE)\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nPut these together and we see the decomposition in action:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(KNN_BIAS_K1)^2 + mean(KNN_VARIANCE_K1) + mean(KNN_IE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5290861\n```\n\n\n:::\n:::\n\n\n\nas compared to\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(KNN_K1_MSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5242858\n```\n\n\n:::\n:::\n\n\n\nVisually, \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyverse)\nDECOMP_DATA <- data.frame(\n   sample_grid = sample_grid, \n   KNN_K1_BIAS2 = KNN_BIAS_K1^2, \n   KNN_K1_VARIANCE=KNN_VARIANCE_K1, \n   KNN_K1_IE = KNN_IE, \n   KNN_K1_MSE = KNN_K1_MSE) |> \n   pivot_longer(-sample_grid) |>\n   mutate(Error=value,\n          Type=case_when(\n            name==\"KNN_K1_BIAS2\" ~ \"Bias^2\", \n            name==\"KNN_K1_IE\" ~ \"Irreducible Error\", \n            name==\"KNN_K1_VARIANCE\" ~ \"Variance\", \n            name==\"KNN_K1_MSE\" ~ \"Total Error\")) \n\n\nggplot() + \n  geom_bar(data=DECOMP_DATA |> filter(Type != \"Total Error\"), \n           mapping=aes(x=sample_grid, y=Error, color=Type), \n           stat=\"identity\") + \n  geom_line(data=DECOMP_DATA |> filter(Type == \"Total Error\"), \n            mapping=aes(x=sample_grid, y=Error), \n            color=\"red4\", linewidth=2) + \n  xlab(\"X\") + ylab(\"Test Error\")\n```\n\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nSo a bit of weirdness at the left end point - but holds up as well \nas we might expect for $N=25$ samples. \n\nHow do the relative magnitudes of these terms change as you \nadjust the parameters of the simulation?\n\n## Introduction to Convex Optimization\n\nIn this course, we will frequently want to find the minimizer of\ncertain functions. Typically, these will arise as ERMs and the function\nwill take a $p$-vector of inputs, *e.g.* regression coefficients, and \nproduce a scalar loss output. Let us generally call this function\n$f: \\R^p \\to \\R$.  \n\nIn some circumstances, we can take the derivative of $f$, typically\ncalled the _gradient_ in this context, and set it equal to zero. For\nexample, suppose we want to minimize an expression of the form: \n\n$$f(\\bx) = \\frac{1}{2}\\bx^{\\top}\\bA\\bx + \\bb^{\\top}\\bx + c$$\n\nwhere $\\bA$ is a _symmetric_ strictly positive-definite matrix, \n$\\bb$ is an arbitrary $p$-vector, and $c$ is a constant. Taking the\ngradient, we find\n\n$$ \\frac{\\partial f}{\\partial \\bx} = \\bA\\bx + \\bb$$\n\nWe set this to zero and find a crticial point at: \n\n$$\\begin{align*}\n\\mathbf{0} &= \\bA\\bx + \\bb \\\\\n- \\bb &= \\bA\\bx \\\\\n\\implies \\bx &= -\\bA^{-1}\\bb\n\\end{align*}$$\n\nassuming that $\\bA$ is invertible. (Here, invertibility is implied\nby assuming $\\bA$ is **strictly** positive-definite.) As usual, we are\nnot quite done here, as we must also check that this is a\n_minimizer_ and not a maximizer or a saddle point. To do so, we\ntake the *second* derivative to find \n\n$$ \\frac{\\partial}{\\partial \\bx}\\frac{\\partial f}{\\partial \\bx} = \\bA$$\n\nIn this multivariate context, we need the second derivative, \n*a.k.a* the Hessian, to be _strictly positive-definite_ to guarantee\nthat we have found a minimizer and this is indeed exactly what we\nassumed above. As discussed above, 'definiteness' plays the role of\nsign for many applications of matrix-ness. Here, by assuming strictly\npositive definite, we are essentially treating the matrix as\nstrictly positive ($>0$), which is exactly the condition we need to \nguarantee a minimizer in the scalar case as well. \n\nHence, we have that the one minimizer of $f$ is found at \n$\\bx_* = \\bA^{-1}\\bb$. Compare this to the scalar case of minimizing\na quadratic $\\frac{1}{2}ax^2 + bx + c$ with minimizer at $x = -b/a$.[^spd]\n\n[^spd]: Note that we can only minimize this quadratic in the case where $a$ is\nstrictly positive so the parabola is upward facing. This is the scalar\nequivalent of the strict positive-definiteness condition we put on $\\bA$. \n\nThe above analysis works well, but it is essentially the only \nminimization we will be able to do in closed form in this course.[^ols]\n\nFor other functions, we will need to apply _optimization_: the\nmathematical toolkit for finding minimizers (or maximizers) of functions.\nFortunately for us, many of the methods we begin this course with\nfall in the realm of _convex optimization_, a particularly nice branch\nof optimization. \n\n*Convex* Optimization refers to the problem of *minimizing* *convex* functions\nover *convex* sets. Let's define both of these: \n\n- A *convex* function is one which satisfies this inequality at all points: \n  $$f(\\lambda \\bx + (1-\\lambda)\\by) \\leq \\lambda f(\\bx) + (1-\\lambda) f(\\by)$$\n  \n  for any $\\bx, \\by$ and any $\\lambda \\in [0, 1]$. \n  \n  This definition is a bit non-intuitive, but it basically implies that we have\n  a \"bowl-like\" function. This definition captures the idea that the actual\n  function value is always less than we might get from linear interpolation. \n  A picture is helpful here: \n \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](notes02_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n  Here, we see that the actual value of the function (blue dot) is less \n  than what we would get if we interpolated the two red points (red line). \n  \n  An alternative definition is that if $f$ is twice-differentiable, its Hessian\n  (2nd derivative matrix) is positive semi-definite. \n  \n- A *convex* set is one that allows us to look \"between\" points. \n  Specifically, a set $\\mathcal{C} \\subseteq \\R^p$ is convex if \n  $$\\bx, \\by \\in \\mathcal{C} \\implies \\lambda \\bx + (1-\\lambda)\\by \\in \\mathcal{C}$$\n  \n  for all $\\bx, \\by \\in \\mathcal{C}$ and any $\\lambda \\in [0, 1]$. \n  \nClearly these are related by the idea of looking \"between\" two points: \n\ni) Convex functions guarantee that this will produce something lower than\n   naive interpolation; and \nii) Convex sets guarantee that the midpoint will be \"allowable\". \n\nTo tie these together, note another alternative characterization of a *convex\nfunction*: one whose epigraph (the area above the curve in the plot) is a \nconvex set. \n\nFor optimization purposes, these two properties imply a rather remarkable fact: \n\nIf $\\bx_0$ is a *local* minimum of $$\\min_{\\bx \\in \\mathcal{C}} f(\\bx)$$, then it\nis a *global* minimum. \n\nThis is quite shocking: if we find a point where we can't improve by going in \nany direction, we are guaranteed to have found a global minimum and no point\ncould be better. (It is possible to have _multiple_ minimizers however: consider\n$f(\\bx) = 0$. Any choice of $\\bx$ is _a_ global minimizer.) This lets us turn\nthe \"global\" search problem into a \"local\" one. \n\nOf course, this only helps us _if_ we can find a local minimizer of $f$. How\nmight we do so? Let's recall a basic calculus idea: the gradient of a function\nis a vector that points in the direction of greatest increase (the \"steepest\"\nuphill). So if we go in the opposite direction of the gradient, we actually\nwill go \"downhill\". In fact, this is basically all we need to start applying\n*gradient descent*. \n\n**Gradient Descent**: Given a convex function $f$: \n\n- Initialize at (arbitrary) starting point $\\bx_0$. \n- Initialize step counter $k=0$. \n- Repeat until convergence: \n  - Compute the gradient of $f$ at $\\bx_k$: $\\left.\\nabla f\\right.|_{\\bx=\\bx_k}$\n  - Set $\\bx_{k+1} = \\bx_k - c \\left.\\nabla f\\right.|_{\\bx=\\bx_k}$\n  \nRepeated infinitely many times, this will converge to a local, and hence global,\nminimizer of $f$. There are **many** variants of this basic idea[^pt], mostly\nrelated to selecting the optimal step size $c$, but this is the most important\nalgorithm in convex optimization and it is important to understand it deeply. \n\n[^pt]: For instance, of the [14 optimization algorithms](https://pytorch.org/docs/stable/optim.html#algorithms) included in\nbase `pytorch`, all but one are advanced versions of gradient descent. \nThe exception is `LBFGS` which attempts to (approximately) use both the gradient\nand the Hessian (second derivative); computing the Hessian is normally quite \nexpensive, so `LBFGS` uses some clever tricks to _approximate_ the Hessian. \n\nWe can apply it here to the 3D function \n$$f(\\bx) = (x_1-2)^2 + (x_2-3)^2 + (x_3 - 4)^2.$$\nClearly, we can see that the minimizer has to be at $(2, 3, 4)$, but our\nalgorithm won't use that fact. \n\nBefore we can implement this algorithm, we need the gradient, which is given by\n\n$$f(\\bx) = \\left\\|\\bx - (2,3,4)^{\\top}\\right\\|_2^2 \\implies \\nabla f = 2[\\bx - (2, 3, 4)^{\\top}]$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- matrix(c(0, 0, 0), ncol=1) # We want to work exclusively with column vecs\nconverged <- FALSE\nc <- 0.001 # Small step size\nf <- function(x) sum((x - c(2, 3, 4))^2)\ngrad <- function(x) 2 * (x - matrix(c(2, 3, 4), ncol=1))\n\nwhile(!converged){\n    g <- grad(x)\n    x_new <- x - c * g\n    \n    if(sum(abs(x - x_new)) < 1e-5){\n        converged <- TRUE\n    }\n    \n    x <- x_new\n}\n\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 1.998893\n[2,] 2.998340\n[3,] 3.997787\n```\n\n\n:::\n:::\n\n\n\nWe don't get the _exact_ minimizer, but we certainly get something 'close enough'\nfor our purposes. In [Report #01](../reports/repot01.html), you will use\ngradient descent and some variants to study the least squares problem. \n\nOften, we will want to see how the value of $f(\\bx_k)$ changes over the course\nof the optimization. We expect that it will go down monotonically, but it may\nnot be worth continuing the optimization if we have reached a point of \n'diminishing returns.' You can do this by hand (evaluating $f$ after each update\nand storing the results), but many optimizers will often track this automatically\nfor you: *e.g.* [TensorBoard](https://www.tensorflow.org/tensorboard).[^tools]\n\n[^ols]: OLS and some basic variants (ridge regression) can be written\nin this 'quadratic' style. Can you see why? \n\n[^tools]: Modern ML toolkits like `pytorch` or `TensorFlow` are (at heart) fancy\nsystems to do two things automatically that we did 'by hand' in this example: \n\n    - Compute gradients via a process known as 'back-propogation' or 'autodiff'\n    - Implement (fancy) gradient descent\n\n",
    "supporting": [
      "notes02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}