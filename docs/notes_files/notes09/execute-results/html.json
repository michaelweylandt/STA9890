{
  "hash": "a65edb4f104f96aba4ff591c843291c4",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"9\"\nauthor: \"Michael Weylandt\"\ntopic: \"Tree-Based Methods\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n$$\\newcommand{\\bx}{\\mathbf{x}} \\newcommand{\\bb}{\\mathbf{b}}\\newcommand{\\by}{\\mathbf{y}}$$\n\n\n\n\n\n\n\n## Review of Bagging\n\nLast week we discussed the ensemble learning technique known as \n*bagging*. Bagging, a contraction of 'boostrap aggregation' can be used\nto create an ensemble by training $B$ copies of a given base learner on $B$\ndistinct bootstrap samples, *i.e.*, 'faux' data sets of $n$ points obtained\nby sampling the original training data $n$ times _with replacement_. \nBecause these $B$ trained base learners are essentially interchangeable, \nit is conventional to create the bagged prediction by a straight average\n(or majority vote if classification) of the individual base learners, though\nyou will occasionally see the bagging base leraners combined in a 'stacked'\nfashion. \n\nWe showed that the MSE associated with a bagged ensemble of size $B$ is: \n\n$$\\text{MSE}_{\\text{Bagging}} = \\text{Bias}^2_{\\text{Base Learner}} +\\underbrace{\\rho\\sigma^2 + \\frac{1}{B}(1-\\rho)\\sigma^2}_{\\text{Variance}} + \\text{Irreducible Error}$$\n\nand noted that, by sending $B \\to 0$, we reduce the 'bootstrapping variance'\nto 0 but that some 'inherent variance' remains from the training\ndata.[^training_variance] As we noted last week, this implies that our\nsearch for the optimal base learner should do three things: \n\n- Have low bias\n- Have low correlation among the bagged learners\n- Best fast enough that we can take $B \\to \\infty$ and cancel that\n  term\n  \nIf we can find something that does all of these, we will have a (near)\noptimal predictor ($\\text{MSE} \\approx \\text{Irreducible Error}$). This\nis our primary goal for the day. \n\n[^training_variance]: There is randomness in our selection / acquisition of\ntraining data and nothing we do can completely remove it. Three cheers for\nlarge and well-constructed training sets, the unsung heros of machine\nlearning!\n\n### 'Out of Bag' Error\n\nBefore we move to our long-promised final base learner, let's consider\nanother advantage of bagging. We know that it is important to have 'new'\ndata when assessing model performance. To date, we have obtained this 'new'\ndata through variants of the hold-out principle, such as the test-train\nsplit or cross-validation: if it is important to have new data, it is\nimportant enough to set aside some of our putative training data for model\nassessment. \n\nBut bagging gives us another source of 'unseen' data: if we sample $n$\npoints from a population of size $n$ with replacement, approximately\n$n/e \\approx 2n/3$ points will be sampled, leaving $n(1-e^{-1}) \\approx n/3$\npoints unseen by each base learner. If these points were the same across\neach base learner, we would have a readily available test set. But then\nour whole bagging strategy would be essentially useless... \n\nWe can estimate the so-called 'out-of-bag' error (OOB) associated with\neach training point by computing the predictive performance \n_on the ensemble of learners that haven't seen that point_. That is, \ninstead of testing the performance of the whole ensemble, we take a \n'subensemble' of the base learners that never saw that point. Repeating\nthis process over all $n$ training points (and creating $n$ different\nsub-ensembles) we obtain an estimate of the OOB error. Clearly, OOB is not\ntest error of the entire ensemble, but it is a useful quantity nevertheless.\n\nFormally, we obtain the bagged ensemble and associated OOB error as follows: \n\n- **Inputs**:\n  - Base learner family $\\mathcal{F}$\n  - Training set of size $n$: $\\mathcal{D}=\\{(\\bx_1, y_1), (\\bx_2, y_2), \\dots, (\\bx_n, y_n)\\}$\n  - Number of bootstrap samples $B$\n- For $b=1, \\dots, B$:\n  - Create bootstrap training set $\\mathcal{D}_b = \\{(\\bx_1^{(b)}, y_1^{(b)}), (\\bx_2^{(b)}, y_2^{(b)}), \\dots, (\\bx_n^{(b)}, y_n^{(b)})\\}$\n  - Fit base learner $\\hat{f}_b$ to $\\mathcal{D}_b$\n- Build ensemble $\\hat{f}(\\bx) = B^{-1}\\sum_{b=1}^B \\hat{f}_b(\\bx)$\n- For $i=1, \\dots, n$: \n  - Let $b_i^c = 0$ be the number of bootstrap samples not containing $i$\n  - Let $y_i^c = 0$ be the average prediction of bootstrapped learners not trained on $i$\n  - For $b = 1, \\dots, B$: \n    - If $i \\notin \\mathcal{D}_b$: \n      - $b_i^c := b_i^c+1$\n      - $y_i^c := y_i^c + \\hat{f}_i(\\bx_i)$\n  - Let $\\hat{y}_i^c = y_i^c / b_i^c$ (or majority vote if classifying)\n  - Let $\\text{OOB}_i = \\text{Loss}(\\hat{y}_i^c, y_i)$\n- Estimate OOB error as $\\text{OOB} = n^{-1} \\sum_{i=1}^n \\text{OOB}_i$\n- **Return** $\\hat{f}, \\text{OOB}$\n\nFor each training point, we expect it to occur in approximately $B/e$ \nbootstrap samples and hence we have approximately $1 - B/e \\approx B/3$\nbase learners used for computing the OOB error. This isn't quite as good\nas the full $B$ samples we would use for a true test point, but since\nvariance decreases quadratically in $B$, this isn't really a major \nloss.[^oob_var]\n\n[^oob_var]: Taking a typical choice of $B = 500$, the OOB ensemble of \n(expected) $316$ base learners will have a variance that is approximately\n$500/316 \\approx 1.6$ times higher than the 'true' ensemble. If $B$ is\nlarge enough $(1-\\rho)\\sigma^2/B \\approx 0$,\n$(1-\\rho)\\sigma^2/(B(1-e^{-1})) \\approx 1.6(1-\\rho)\\sigma^2/B$ will \nalso be negligible. Put another way, the bulk of the error in bagged\nensembles comes from i) bias; ii) _training data_ variance; and iii) \nirreducible error. The 'not enough bootstraps' contribution is typically\nquite small.\n\n## Decision Trees\n\nOk - now it's time to consider the final base learner of this course: \n_decision trees_. You are almost certainly already familiar with \ndecision trees through diagrams like this: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code}\ndtree <- rpart(medv ~ . , data=Boston)\n\nrpart.plot(dtree)\n```\n\n::: {.cell-output-display}\n![](notes09_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nHere, the tree is evaluated by applying a series of _univariate binary_\ndecision rules 'downwards': that is, at each step, we ask one step about\none variable and take the left or right branch as appropriate. The final\nterminus (or 'leaf') of the tree is then our prediction.\n\nDecision trees are incredibly popular because they are easy to apply\n'mentally' and suggest natural interpretations. Variables which are\nused to split early or split often are more important. Interestingly, \ndecision trees also can be used to give an 'intermediate prediction'\nwhich is what we would get by stopping the prediction at that point. \nSplits which materially change the intermediate prediction are \nalso more important. \n\nBut how should we actually fit a decision tree to data? This is, like\nbagging and computing OOB error, the sort of thing which is easy to \nunderstand intuitively, but comes with a significant amount of bookkeeping\nin practice. \n\nEssentially, trees are fit by repeatedly asking the question: \"What split\nwould most reduce the error if my predictions stop here?\" So, for the \nfirst split, the tree looks at each variable to determine its optimal\nsplit point, and then finds the optimal variable to split on. To find\nthe optimal split point, we need to sort the data on that variable\nand then compute the 'before' and 'after' variance for each value. \nAfter finding the first split, the two 'second level' splits repeat\nthis process _on only the data in their path_. That is, the two 'second\nlevel' splits are found on _different_ data. The four 'third level' splits\nare found on four different data sets, *etc.* \n\nBecause the later splits are computed on different data sets, they \nalmost always select different variables at each level (as in our\nexample above, where the two 'level two' splits use the `lstat` and `rm`\nvariables). Predictions are made by taking the average (or majority)\nof data left in that split. \n\nNote that the splits need not be _and typically are not_ 'even' splits\nalong a variable. *E.g.*, if we were doing a split on a variable that\nlooks like\n\n| Variable | Value | \n|----------|-------|\n| 1.0      | 1.0   | \n| 1.1      | 0.9   | \n| 1.2      | 1.1   | \n| 1.3      | 1.4   | \n| 100      | 500   | \n| 105      | 550   | \n\nThe optimal split is clearly something like $\\texttt{Variable} < 1.3$ with\nconditional predictions of $1.1$ and $525$ in each split and split sizes\nof $4$ and $2$. \n\nClearly, if the tree is allowed to run 'to completion', there will be\none data point left in each leaf and the tree can obtain 0 training error. \nThat is, maximal decision trees are a very high complexity (low bias, \nhigh variance) model. On the other extreme, a 'single split' tree (often\ncalled a 'stump') will be low complexity (high bias, low variance). So\nwe can take tree depth (number of levels) as a good proxy for model\ncomplexity. Like $K$-Nearest Neighbors, we can pick depth (complexity)\nby a suitable cross-validation or train-test scheme, but will soon see\na better way. \n\nSome other notes on trees: \n\n- You will sometimes see decision trees refered to as `CART`\n  (Classification and Regression Trees) or `rpart` (Recursive Partitioning)\n  but most folks will know the term \"decision trees\" so I just use that. \n- Trees extend _very naturally_ to categorical variables and do not require\n  numeric encoding: we simply have to adapt the decision rule from \n  $x \\leq y$ to $x \\in \\mathcal{Y}$ where $x$ is now a categorical feature\n  and $\\mathcal{Y}$ is a set of lables. \n- Trees also handle missing values nicely: we can simply treat `NA` as\n  _very high_ or _very low_ or use an explicit `is.na` decision rule\n- Trees are rather robust to outliers: if a data point has an extreme\n  value, it will be 'split away' rather early and the 'main branch' of\n  the tree will be fit on the rest of the data. Non-outlier test points\n  will funnel into this 'main branch' and it will be like the outlier\n  essentially never existed\n  \nThere are tons of tiny little decisions in building trees, so I don't\nrecommend writing your own implementation. See, *e.g.*, `?rpart.control`\nfor the fitting parameters used by the `rpart` package: other software\nwill have similar options. \n\n## Random Forests\n\nWe are now ready to introduce the concept of _random forests_, which are\n_almost_ bagged decision trees. \n\n## Boosting\n\n### `AdaBoost`\n\n### `LogitBoost`\n\n### Gradient Boosting\n\n### `xgboost`\n\n",
    "supporting": [
      "notes09_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}