{
  "hash": "e6469f5889df14f3ee3b030f54e39b53",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"{{< var course.short >}} Research Report #{{< meta rr_num >}}: {{< meta rr_title >}}\"\nformat:\n  html:\n    code-link: true\nrr_num: \"01\"\nrr_title: \"Bias and Variance in Linear Regression\"\nfilters:\n  - list-table\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n### Key Dates\n\n- Released to Students: 2025\\-02\\-04\n- **Submission: 2025\\-03\\-07 11:45pm ET on Brightspace**\n\n*Estimated Time to Complete: 9 Hours*\n\n## Research Report #{{< meta rr_num >}}: {{< meta rr_title >}}\n\nIn this research report, you will dive deeply into the\nbias-variance trade-off and the BLUE-ness (or lack thereof) of \nordinary least squares.\n\n### Project Skeleton\n\nYour report should be divided into four sections, covering the\nfollowing: \n\ni)   Theoretical background\nii)  Computation - Gradient Descent and Weight Decay\niii) Bias and Variance Under Linear Data Generating Processes (DGP)\niv)  Bias and Variance Under Non-Linear DGP\n\nAt a minimum, these should include the following elements: \n\n- Theoretical Background\n  - Rigorous Statement and Proof of the Bias-Variance Decomposition\n  \n    $$\\mathbb{E}[\\text{MSE}] = \\text{Bias}^2 + \\text{Variance} (+ \\text{Irreducible Noise})$$\n    \n    Discuss how this statement is to be interpreted in the context\n    of parameter estimation (when the underlying DGP is linear) and\n    in the context of (possibly misspecified) prediction. Be sure\n    to clarify when the \"Irreducible Noise\" term is needed. \n    \n  - Proof of the \"BLUE\" property of OLS with clear statement of the\n    relevant assumptions. Take care to differentiate which assumptions\n    are required for \"B\" and which are required for \"U\".\n- Computation\n  - Derivation and implementation of the 'closed-form' matrix \n    expression for OLS\n  - Derivation and implementation of the 'closed-form' matrix\n    expression for Ridge Regression\n  - Derivation and implementation of a gradient descent method\n    for OLS\n  - Derivation and implementation of a 'weight decay' modification\n    for OLS gradient descent.\n  - Empirical demonstration of the equivalance of OLS+Weight Decay\n    with Ridge Regression\n- Bias and Variance Under Linear DGP\n  - Posit a Linear DGP\n  - Using *Monte Carlo* simulations:\n     - Show that OLS is unbiased under this DGP\n     - Compute the variance of a given regression coefficient\n     - Compute the in- and out-of-sample prediction MSE\n     - Show how the variance, in-sample, and out-of-sample MSE change\n       with the sample size, $n$\n  - Compare against ridge regression\n    - Show that RR is *biased* under this DGP\n    - Compare the variance of OLS and RR\n    - Demonstrate the *MSE Existence Theorem* for Ridge Regression\n    - Compare the MSE of RR and OLS as a function of the sample size $n$\n- Bias and Variance Under Non-Linear DGP\n  - Posit a Non-Linear DGP\n  - Using simulation, determine the \"best approximate\" linear regression\n    coefficients: that is, find the OLS coefficients minimizing MSE\n  - Show that the estimated OLS coefficients converge to the \"best\n    approximate\" coefficients as the sample size $n$ increases\n  - Compute the MSE of RR in simulation and then use 5-fold\n    cross-validation to determine the optimal regularization level\n  - Compare the MSE of RR and OLS in the non-linear setting\n\nFinally, note that because we are interested in bias, variance, and MSE -- \neach of which is defined with respect to a (known) DGP -- we only use\nstatistical simulation for this project. In [Report #02](./report02.html)\nand [Report #03](./report03.html), you will apply methods to real\n(non-simulated) data. \n\n### Additional Background\n\n*Gradient Descent* methods fit models by taking small steps in the\ndirection opposite the gradient of the loss function. (Recall the\ngradient points in the direction of steepest increase, so moving\nin the opposite direction leads to a decrease in error.) \n\nAbstractly, given a differentiable loss function, $\\mathcal{L}(\\beta)$,\ngradient descent proceeds as follows: \n\ni) Select initial guess $\\beta^{(0)}$ and set $k=0$\nii) Repeat until convergence: \n  - Compute gradient $\\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}$\n  - Update guess using a gradient step $$\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}}$$\n  - Increment $k := k+1$\n  - Check convergence: \n    - Parameter convergence: if $\\beta^{(k+1)} \\approx \\beta^{(k)}$, we have converged.\n    - Objective convergence: if $\\mathcal{L}(\\beta^{(k+1)}) \\approx \\mathcal{L}(\\beta^{(k)})$, we have converged\niii) At convergence, return $\\beta^{(k+1)}$\n\nThis scheme is pretty easy to implement, but there are a few points where it behooves you to be careful: \n\ni) How do we check the approximate equality in the convergence check?   \n   We will almost never get _exact_ equality, so you need to pick\n   a norm and a _tolerance_. \nii) How do we choose the step-size $c$? **Much** has been written about\n    this; for our purposes it suffices to take a very small value\n    of $c$ and keep it constant. \niii) What if we never reach convergence? Should we stop after some\n     (large) number of iterations?\n     \nThe method of *weight-decay* modifies the gradient update as follows:\n\n$$\\beta^{(k+1)} = \\beta^{(k)} - c \\nabla\\mathcal{L}|_{\\beta = \\beta^{(k)}} - \\omega \\beta^{(k)}$$\n\nHere $\\omega$ is another small constant scalar value. By subtracting\noff a fraction of $\\beta^{(k)}$ at each step, weight decay prevents\nthe estimate from ever getting to be too big (hence the name). In\nthe case of OLS, this can be shown to be equivalent to a form of \nridge regression.[^rr]\n\nThe *MSE Existence Theorem* for Ridge Regression comes in two forms: \n\n- Estimation Variant. Let \n  $$\\text{MSE}_i(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}}[(\\hat{\\beta_i} - \\beta_i^*)^2]$$ \n  denote the mean squared estimation error in the $i^\\text{th}$ component of\n  $\\hat{\\beta}$ as an estimator of the true regression coefficients $\\beta^*$. \n  Here the expectation is taken over random realizations of both the design\n  matrix $\\mathbf{X}$ and the response vector $\\mathbf{y}$, as well as the \n  estimator $\\hat{\\beta}$ which is a function of $\\mathbf{X}, \\mathbf{y}$. \n  \n  Then, there exists some positive value $\\lambda > 0$ such that \n  \n  $$\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) > \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})$$\n  \n  where $\\hat{\\beta}_{\\text{RR}, \\lambda}$ is the ridge regression estimate\n  with regularization parameter $\\lambda$. \n\n- Prediction Variant. Let \n  $$\\text{MSE}(\\hat{\\beta}) = \\mathbb{E}_{\\mathbf{X}, \\mathbf{y}, \\mathbf{y}'}[\\|\\mathbf{X}\\hat{\\beta} - \\mathbf{y}'\\|_2^2$$ \n  denote the mean squared prediction error associated with the estimator\n  $\\hat{\\beta}$. \n  Here the expectation is taken over random realizations of both the design\n  matrix $\\mathbf{X}$ and the response vector $\\mathbf{y}$ used for training,\n  as well as the test data vector $\\mathbf{y}'$ which must be drawn from the\n  same DGP. \n  \n  Then, there exists some positive value $\\lambda > 0$ such that \n  \n  $$\\text{MSE}_i(\\hat{\\beta}_{\\text{OLS}}) > \\text{MSE}_i(\\hat{\\beta}_{\\text{RR}, \\lambda})$$\n  \n  where $\\hat{\\beta}_{\\text{RR}, \\lambda}$ is the ridge regression estimate\n  with regularization parameter $\\lambda$. \n\nIt is important to note that both of these are statements *in expectation*: \nOLS may perform better than RR on a single realization, but in average - over \nmany realizations - suitably-tuned RR will perform better. \n\nAlso, we note that the MSE Existence Theorem does not give any practical\nadvice on selecting $\\lambda$; we have to fall back on our standard\napproaches, such as cross-validation, for doing so. \n\nFinally, note that the prediction version of the MSE Existence Theorem only\nholds when we look at test (out-of-sample) prediction accuracy. OLS is the optimal linear method for in-sample MSE **always and forever.**\n\n[^rr]: A somewhat remarkable finding of much recent research is that\n_many_ 'new' regularization methods pioneered for fitting neural\nnetworks have essentially the same effect as ridge ($\\ell_2^2$ or\nTikhonov) regularization: weight decay, drop out, early stopping, \nmini-batching and mini-patching. Sometimes it's hard to beat the\nclassics.\n\n\n### Possible Topic(s) for Additional Research\n\nRecent ML research has focused on the case of *overparameterized*\nmodels: that is, models with more parameters than data points used for\ntraining. We have already seen ridge regression as one way to deal with\nthis problem. Another approach is to simply use an iterative method,\nsuch as gradient descent, and stop it when the training error reaches\nzero. (This is a non-unique global minimum for the convex OLS problem.)\nHow does this procedure compare? Does it complicate our understanding\nof the bias-variance trade-off? \n\nAnother recent line of research generalizes the \"BLUE\" property, showing\nthat OLS is in an appropriate sense \"BUE\" - it is still optimal even\nif we allow for (certain types of) non-linear estimators as well. Can\nyou show this? \n\n### Submission Instructions\n\nSubmit your research report as a PDF of 6-8 pages on Brightspace.[^pdf]\nYour submission should include all *essential* code, *e.g.*, code\nused to fit models or to perform simulations, but may omit\n*incidental* code, *e.g.* code used to create figures or import\ndata. \n\nYou are **required** to implement ML methods by hand; use of 'built-in'\nML functionality is disallowed unless authorized by instructor. You may use\nbuilt-in linear algebra subroutines and are not required to, *e.g.*, implement\nmatrix multiplication from scratch. While you must use your own implementation\nof ML methods, it is smart to compare against existing 'reference' implementations.\n\nYour report should be in the style of a technical \"blog post\", accessible\nto someone who is just getting into Machine Learning but has solid\nbackground in the prerequisite subjects (*e.g.*, you on the day before\nthis class started). Make sure to (rigorously) define and prove all\nresults used. You should also cite relevant authoritative resources, \n*e.g.*, the recommended textbooks, where appropriate. \n\nEach student must submit an individual and independently written report, but you\nare encouraged to work together with your peers to understand methods, design\nsimulations, review relevant literature, *etc.* I strongly recommend having a\nclassmate [\"red-team\"](https://en.wikipedia.org/wiki/Red_team) your report\nbefore submission to find unclear writing, sub-standard figures and tables, \nunconvincing simulations, incorrect  code, *etc.* \n\n[^pdf]: If you wish to use an alternate submission format to allow\ninteractive and/or animated elements, please seek pre-approval from\nthe instructor. \n\n### Grading \n\nYour submission will be evaluated *holistically* and graded out of 100\npoints. The following rubric will guide this holistic evaluation, but\nthe instructor may deviate as necessary to accurately grade the final\nsubmission. \n\n::: {.list-table width=\"1,1,1,1,1,1\" aligns=\"c,c,c,c,c,c\" header-rows=1 .hover}\n\n * - Report Element\n   - Excellent. <br> \"A-\" to \"A\" (`90% to 100%`)\n   - Great. <br> \"B-\" to \"B+\" (`80% to 89%`)\n   - Average. <br> \"C\" to \"C+\" (`73% to 79%`)\n   - Poor. <br> \"D\" to \"C-\" (`60% to 72%`)\n   - Failure. <br> \"F\" (`0% to 59%`)\n   \n * - Presentation (15%)\n   - Report has excellent formatting, with particularly effective \n     tables and figures. Tables and Figures are \"publication-quality\"\n     and clearly and succinctly support claims made in the body text.\n   - Report has strong formatting; tables and figures make their\n     intended points, but do not do so optimally. \n   - Formatting is average; tables and figures do not clearly support\n     arguments made in the text and/or are not \"publication quality\"\n   - Poor formatting distracts from substance of report. Tables and\n     Figures exhibit significant deficiencies in formatting.  \n   - Formatting prohibits or significantly impairs reader understanding.\n \n * - Project Skeleton (15%)\n   - Report includes all required elements and goes significantly deeper\n     than required by the project skeleton in a manner that provides \n     additional insights. \n     \n     Mathematical definitions and proofs are clearly stated. \n     \n   - Report includes all required elements and dives deeper into the\n     topic, but does not generate additional insights. (*E.g.*,\n     additional simulations showing the same phenomena)\n     \n     Mathematical definitions and proofs are correctly stated, but \n     clarity could be improved. \n     \n   - Report includes all required elements. \n   \n     Mathematical definitions and proofs are essentially correct,\n     but difficult to understand and/or contain trivial errors.\n   \n   - Report does not include all required elements, but is still\n     able to capture key points. \n     \n     Mathematical definitions and proofs contain significant, but\n     non-fatal, errors. \n   - Report fails to adequately address the topic. \n   \n     Mathematical definitions and proofs are fundamentally incorrect. \n \n * - Algorithms and Computational Efficiency (20%)\n   - Report implements all methods efficiently with high-quality,\n     well-formatted and performant code. \n   - Report implements all methods efficiently with acceptable\n     code quality. \n   - Report implements all methods, but does not do so efficiently\n     and/or with substandard code. \n   - Report uses built-in methods instead of implementing methods\n     from scratch\n   - Code does not appear to run properly / insufficient code submitted. \n \n * - Methodological Aspects and Simulation Design(20%)\n   - Methods are accurately and correctly implemented in a robust / bullet-proof\n     manner, designed to responsibly check for and address possible modes of\n     failure. Simulations clearly and efficiently support all claims. \n   - Methods are accurately and correctly implemented, but are not robust to\n     failure. Simulations clearly and efficiently support all claims. \n   - Methods are implemented with minor harmless errors and/or poor validation.\n     Simulations do not fully support claims. \n   - Methods are implemented with significant errors leading to incorrect results. \n   \n     Simulations do not give sufficient support for key claims.\n   - Methods are not correctly implemented. Simulations do not support claims.\n \n * - Communication and Writing (20%)\n   - Report exhibits excellent written communication, making all points\n     exceptionally clearly and at the level of a quality academic\n     publication. Code, results, and text are skillfully woven together.\n   - Report exhibits great written communication, all points are made\n     clearly without notable grammatical errors. Code, results, and\n     text are neatly tied together. \n   - Report exhibits solid written communication, key points are made \n     understandably and any grammatical errors do not impair\n     understanding. Code, results, and text could be better integrated, \n     but it is clear which elements relate. \n   - Written communication is below standard: points are not always\n     understandable and/or grammatical errors actively distract from\n     content. Code, results, and text are not actively integrated, but\n     are generally located 'near' each other in a semi-systematic fashion.\n   - Written communication is far below standard, possibly bordering\n     on unintelligible. Large blocks of code are shown without any\n     relevant results or text. \n \n * - Contextualization (10%)\n   - Report draws on relevant, modern research literature to give context\n     to its findings and to broaden its scope. \n   - Report draws on standard textbooks and/or classical research literature\n     to give context to its findings and to broaden its scope. \n   - Report cites textbooks to give context, but does not take a 'big picture' \n     view. \n   - Report gives appropriate context, but lacks citations. \n   - Report gives limited, insufficient context.\n\n:::\n\n------------------------------------------------------------------------\n\nThis work Â©2025 by [Michael Weylandt](https://michael-weylandt.com) is licensed\nunder a [Creative Commons BY-NC-SA \n4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en) license.\n![](http://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png){width=\"10%\"}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}