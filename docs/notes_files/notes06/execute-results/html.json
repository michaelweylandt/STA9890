{
  "hash": "eddec6c3bea8c2d88fcd3de8e9901c9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"6\"\nauthor: \"Michael Weylandt\"\ntopic: \"Classification I: Generative Classifiers\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n$$\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\P}{\\mathbb{P}}$$\n\nThis week, we begin our study of classifiers with an analysis\nof *generative* classifiers. While many generative\nclassifiers have appeared in the literature, they can \nessentially all be derived from a standard formula. We will discuss\nthis standard formula first and then turn our attention to specific\nnamed instances. \n\n## Mixture Models\n\nRecall from [STA 9715](https://michael-weylandt.com/STA9715) that\na (discrete) mixture model is a distribution of the form: \n\n$$X \\sim \\begin{cases} X_1 & Z = 1 \\\\ X_2 & Z = 2 \\\\ \\dots \\\\ X_K & Z=K \\end{cases} \\text{ where } \\textsf{supp}(Z) = \\{1,\\dots, K\\} \\text{ and } X_i \\sim \\mathcal{P}_i \\text{ for all $i$ and } Z \\perp X_i, X_i \\perp X_j\\, (i\\neq j)$$\n\nThat's quite a bit to unpack, so let's take it piece by piece: the\nrandom variable $X$ has a $K$-component *mixture* distribution if\nwe can sample $X$ by generating $K$ independent random variables\n$\\{X_1, \\dots, X_K\\}$ (with *different* distributions) and then\nselecting one of them at random. \n\nLet's look at a concrete version of this: suppose we have\na population of male and female basset hounds that is 60\\% female.\nThe weight of male basset hounds follows a $\\mathcal{N}(65, 10)$\ndistribution while females have weight distributed as \n$\\mathcal{N}(55, 10)$. The weight of a randomly selected basset\ncan be sampled as\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbasset <- function(n){\n    Z <- rbinom(n, size=1, prob=0.6) # Z=1 <- Female\n    ifelse(Z, rnorm(n, mean=55, sd=sqrt(10)), \n              rnorm(n, mean=65, sd=sqrt(10)))\n}\nrbasset(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53.19108\n```\n\n\n:::\n:::\n\n\n\nIf we collect the weights of many basset hounds, we see that\nthe resulting distribution is not normally distributed: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(rbasset(10000), breaks=50, main=\"Distribution of Basset Hound Weights\")\n```\n\n::: {.cell-output-display}\n![](notes06_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nIn fact, this is a *bimodal* distribution with each 'lump' representing\nmales or females.[^bimodal] Distributions of this form can often be\nwritten  more compactly using a 'convex combination' notation: \n\n[^bimodal]: Note that a mixture of normals is not *always* multimodal\n(*e.g.*, if the two means are very close together), but a mixture\nof normals is a common model for multimodal data.\n\n$$\\text{Basset} \\sim \\frac{2}{5}\\mathcal{N}(65, 10) + \\frac{3}{5}\\mathcal{N}(55, 10)$$\n\nThe above notation can be interpreted as specifying a draw from \nthe first distribution with probability $2/5 = 40\\%$ and from\nthe second with probability $3/5 = 60\\%$; that is, it's a particular\n2-component mixture of normals. \n\nIn specifying this mixture, the probabilities of each component must\nbe non-negative and must sum to 1; these are exactly the same\nrestrictions we put on a convex combination of points earlier in\nthe course. In fact, the set of mixture distributions can be \nconsider the 'convex set' containing all of its individual \ncomponents. \n\n### Inference from a Mixture Distribution\n\nNow suppose you are on a walk and you meet a very friendly 62 \npound basset hound. You of course want to pet this marvelous dog, but\nyou need to ask the owner's permission first: in doing so, should\nyou ask \"may I pet him?\" or \"may I pet her?\"\n\nWhile the basset hound won't care if you misgender, you recall\nthat you can use [*Bayes'\nRule*](https://en.wikipedia.org/wiki/Bayes%27_theorem) to 'invert'\nthe mixture distribution. That is, while the mixture distribution\nnormally goes from sex to weight, Bayes' rule will let us go from\nweight back to sex. \n\nSpecifically, in this case, Bayes' rule tells us: \n\n$$\\P(\\text{Female}|\\text{Weight=62}) = \\frac{\\P(\\text{Weight=62}|\\text{Female})\\P(\\text{Female})}{\\P(\\text{Weight=62}|\\text{Female})\\P(\\text{Female}+\\P(\\text{Weight=62}|\\text{Male})\\P(\\text{Male}))}$$\n\nThe math here is a bit nasty numerically, but we can do it in `R` quite\nsimply: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_female <- 0.6\np_male <- 1-p_female\n\np_62_female <- dnorm(62, mean=55, sd=sqrt(10))\np_62_male   <- dnorm(62, mean=65, sd=sqrt(10))\n\np_female_62 <- (p_62_female * p_female) / (p_62_female * p_female +\n                                           p_62_male * p_male)\n\npaste0(\"There is a \", round(p_female_62 * 100, 2), \"% chance that basset is a female.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"There is a 16.87% chance that basset is a female.\"\n```\n\n\n:::\n:::\n\n\nGiven that calculation, you take a chance and ask whether you \ncan pet *him*. \n\nWhat have we done here? We have *built* a basset-sex classifier from\na combination of the population distribution and Bayes' rule. This is\nthe essence of generative classifiers - *if* we know the distribution,\naccurate probabilistic classification is a straightforward application\nof Bayes' rule. In practice, we don't know the distribution *a priori*\nand it must be estimated from data. Different generative classifiers\nessentially are just different ways of estimating the population\n(mixture) distribution. \n\n## Building Generative Classifiers\n\nLet's formalize our approach for a two-class classifier though\nthe extension to multi-class is simple enough. Given a training set\n\n$$\\mathcal{D}_{\\text{Train}} = \\{(\\bx_1, y_1), (\\bx_2, y_2), \\dots, (\\bx_n, y_n)\\}$$ \n\nwe assume $\\bX$ follows a two-component mixture where $Y$ is the \nmixing variable:\n\n$$\\bX | Y=0 \\sim \\mathcal{P}_0 \\text{ and } \\bX | Y=1 \\sim \\mathcal{P}_1$$\n\nClearly to fully specify this distribution, we need three things: \n\n- To know the distribution of $Y$\n- To know the distribution $\\mathcal{P}_0$\n- To know the distribution $\\mathcal{P}_1$\n\nThe distribution of $Y$ is relatively straightforward: since it only\ntakes values $\\{0, 1\\}$ it _must_ have a Bernoulli distribution and hence\nthe only thing we need to estimate is the probability, $\\pi_1$, that $Y=1$. \nThe simplest approach to estimating the Bernoulli probability is to simply\ntake the empirical probability from the training data set, *i.e.*, the fraction\nof observations where $Y=1$, but this can also be estimated using knowledge of\na broader data set or a different population than the training data. \nFor instance, if a data set is known to overindex on $Y=1$, *e.g.*, because it\ncontains patients who self-reported some disease, it may be more appropriate to\nestimate $p$ based on the population-wide prevalance of the disease. These concerns\nare typically rather problem specific. \n\nEstimation of $\\mathcal{P}_0, \\mathcal{P}_1$ may be more difficult. If the \ntraining data is _extremely_ large, we may use a *non-parametric* estimator, \n*e.g.*, a multivariate histogram, but in general we will need to use some sort\nof restrictive (parametric) model. The most commonly used generative classifiers\nassume some sort of multivariate normal structure, but you can create 'custom'\nclassifiers appropriate to specific domains. \n\nFor now, let us assume $\\mathcal{P}_{0}, \\mathcal{P}_1$ have been estimated\nand that they have PDFs $p_0(\\cdot), p_1(\\cdot)$. Applying Bayes' rule as before, \nwith prior (baseline) probabilities $\\pi_0, \\pi_1 = 1-\\pi_0$, we get the following\nprobabilities\n\n$$\\begin{align*}\n\\P(Y = 1 | \\bx) &= \\frac{\\P(\\bx | Y = 1)\\P(Y=1)}{\\P(\\bx | Y = 0)\\P(Y=0)+\\P(\\bx | Y = 1)\\P(Y=1)} \\\\\n&= \\frac{p_1(\\bx)\\pi_1}{p_0(\\bx)\\pi_0 + p_1(\\bx)\\pi_1}\n\\end{align*}$$\n\nIf we are happy with a *probabilistic* classifier, we are essentially done. If\nwe want a true binary classifier, *i.e.*, a $\\{0, 1\\}$ valued 'point predictor', \nwe also need to set a classification threshold, $\\overline{p}$, to obtain: \n\n$$\\delta(\\bx) = \\begin{cases} 0 & \\frac{p_1(\\bx)\\pi_1}{p_0(\\bx)\\pi_0 + p_1(\\bx)\\pi_1} < \\overline{p} \\\\ 1 & \\text{ otherwise} \\end{cases}$$\n\n$\\overline{p}$ can be chosen to optimize a relevant performance metric (if you \npractice good sample splitting hygiene) or via a full decision theoretic analysis,\nas discussed last week. \n\nThat's basically it! \n\n\n## Named Generative Classifiers\n\nAs mentioned above, the 'popular' generative classifiers are essentially \ndefined by specific multivariate normality assumptions on \n$\\mathcal{P}_0, \\mathcal{P}_1$. Let us review these in ascending order of\ncomplexity. \n\nRecall that a multivariate normal distribution is uniquely identified by its\n*mean* and *(co)variance (matrix)*, just like a standard univariate normal \ndistribution. When estimating means, we will simply use the class means for \neach classifier, though James-Stein time estimators could be used for a \n(likely imperceptible) improvement. Estimation of variances is a harder task: \nto estimate the full covariance matrix of $p$ random variables (or a random\n$p$-vector), we must estimate: \n\ni) $p$ (marginal) variances and $\\frac{p^2-3p}{2}$ correlations; or \nii) $\\binom{p}{2} = \\frac{p(p-1)}{2}$ covariances\n\nMaking this even harder, we have to deal with the fact that the \"variance of the\nvariance\", *i.e.*, the kurtosis, is typically quite large. While we do not focus\non high-dimensional covariance estimation in these notes, it is quite a rich\nsubject: if you want dig further into it, the\n[Ledoit-Wolf](http://www.ledoit.net/honey.pdf) estimator is a good place to start.\n\n### Naive Bayes\n\nIn Naive Bayes, we assume $\\mathcal{P}_0, \\mathcal{P}_1$ are multivariate normal\ndistributions with no correlation among the features and a common covariance\nacross classes.[^isotropic] This *significantly* simplifies the covariance \nestimation problem, since we now have all $n$ samples available to estimate only\n$p$ variances. Our estimates are given as follows: \n\n$$\\hat{\\mu}_0 = \\frac{1}{|\\mathcal{C}_0|} \\sum_{i \\in \\mathcal{C}_0} \\bx_i$$\n\nwhere $\\mathcal{C}_0$ is the set of observations from class 0 and \n\n$$\\hat{\\mu}_1 = \\frac{1}{|\\mathcal{C}_1|} \\sum_{i \\in \\mathcal{C}_1} \\bx_i.$$\nThe variance estimates are a bit trickier: \n\n$$\\hat{\\Sigma}_{ii} = \\frac{1}{n}\\left(\\sum_{j\\in\\mathcal{C}_0} (x_{ji} - \\hat{\\mu}_{0i})^2 + \\sum_{j\\in\\mathcal{C}_1} (x_{ji} - \\hat{\\mu}_{1i})^2\\right)$$\nThis looks a bit nasty, but it's not too hard: we take the standard \n'average squared difference' estimator of the variance, but instead of using\nthe overall mean, we subtract the estimated mean from the relevant class. Note\nthat here we are only estimating $\\Sigma_{ii}$, the diagonal elements, and not\na full matrix $\\Sigma$. \n\nNaive Bayes classifiers are particularly important to the history of ML as\nthey were used for one of the first and most influential spam filters. \n\n\n\n[^isotropic]: Sometimes, we go further and assume the standard deviations of \neach feature are the same: this assumption is typically not made explicitly as \nsuch but is done 'off-camera' by pre-standardizing features.)\n\n### Linear Discriminant Analysis, \n\nGeneralizing a bit, we can relax the assumption that the covariance matrix \n$\\Sigma$ is diagonal and only assume that it is constant across the two classes.\nHere, \n\n## A Non-Standard Example\n\nSuppose we have \n",
    "supporting": [
      "notes06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}