\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{multicol}
\usepackage{lettrine}
\usepackage{calligra}
\usepackage{hyperref}
\usepackage[parfill]{parskip}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{xcolor}

\usepackage{amsmath}

\newcommand{\R}{\mathbb{R}}

\usepackage{bm}
\newcommand{\bzero}{\bm{0}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bI}{\bm{I}}
\newcommand{\bX}{\bm{X}}
\newcommand{\by}{\bm{y}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bD}{\bm{D}}

\newcommand{\E}{\mathbb{E}}

\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\usepackage{soul}
\newcommand{\cmark}{\ding{52}}

\usepackage{rotating}

\usepackage[margin=0.75in]{geometry}

\begin{document}
\begin{center}
    {\Large \bf Midterm Exam \\ STA9890 \\ Statistical Learning for Data Mining}
\end{center}

{\bf Assignment Parameters:} \\
\phantom{abc}Date Assigned: 2024-03-24\\
\phantom{abc}Date Due: 2024-04-02 @ 5:45pm \\

\begin{center}
    \bf \large This is a closed-note, closed-book exam.\\You may not use any external resources.
\end{center}

\section*{Instructions}
This exam will be graded out of \textbf{100 points}. 

You will have 1.5 hours (90 minutes) to complete this exam. 

You will have 100 minutes from the time you download this exam on Blackboard to upload your solutions: that is, you have 90 minutes to complete the exam and 10 minutes to scan and upload any written answers. \emph{You are responsible for uploading your exam in time, so plan accordingly.}

\begin{center} \large \textbf{Late exams will not be accepted.} \end{center}



This exam is divided into four equally weighted parts: 
\begin{itemize}
    \item Multiple Choice
    \item Short Answer
    \item Issue Spotter
    \item Mathematics of Machine Learning
\end{itemize}

\emph{Though all sections are worth the same total number of points, they are not all equally difficult. Individual questions within a section also vary in difficulty. You need to use your time wisely. Skip questions that are not easy to answer quickly and return to them later.}

For the multiple choice and short answer questions, please answer in the space provided on the exam sheet.  If you need additional space for the issue spotter or mathematical questions, use the additional pages at the end of the PDF.

\emph{Mark your answers clearly: if I cannot easily identify your intended answer, you may not get credit for it.}

I \emph{will not} answer questions during the exam period. If a question is ambiguous, do your best to answer it. If you need to make additional assumptions to answer a question, please state them in full.

\begin{center}
    \bf This is a closed-note, closed-book exam. You may not use any external resources.
\end{center}

\clearpage

\section*{Multiple Choice (25 points; 2.5 points each)}

For each question, \textbf{CIRCLE} your answer(s).

\begin{enumerate}[label={Q\arabic*.}]
\item True/False: Supervised learning problems have a continuous response variable ($y$).

\qquad TRUE \qquad \qquad FALSE
\item True/False: OLS finds the linear model with the lowest training MSE.

\qquad TRUE \qquad \qquad FALSE
\item True/False: Low-bias models should always be preferred to maximize out-of-sample accuracy.

\qquad TRUE \qquad \qquad FALSE
\item True/False: Models with higher training error always have higher test error

\qquad TRUE \qquad \qquad FALSE
\item True/False: Selecting the model with the lowest cross-validation error will always minimize in-sample (training) error

\qquad TRUE \qquad \qquad FALSE
\item True/False: The lasso does a better job selecting `true' variables in low correlation settings

\qquad TRUE \qquad \qquad FALSE
\item Select all that apply: Which of these describes kernel methods?
\begin{itemize}
    \item They allow us to fit spline methods efficiently
    \item They allow us to do `infinite-dimensional feature expansion'
    \item Because they are more flexible, they always provide out of sample (test accuracy) improvements
\end{itemize}
\item Select all that apply: The ``$\ell_0$ norm'' is
\begin{itemize}
    \item The number of non-zero elements in a vector
    \item An example of an $\ell_p$-norm
    \item The sum of the absolute values of a vector
\end{itemize}
\item Select all that apply: The following are convex penalties
\begin{itemize}
    \item Lasso
    \item SCAD
    \item Best Subsets
\end{itemize}
\item Select all that apply: The following are sparsity-inducing penalties
\begin{itemize}
    \item Local Polynomial Penalty
    \item Ridge 
    \item MC-PLUS
\end{itemize}
\end{enumerate}
\clearpage
\section*{Short Answer (25 points; 5 points each)}

\begin{enumerate}[label={Q\arabic*.}]
\item Sketch the bias-variance trade-off curve for $K$-Nearest Neighbors. Place $K$ on the horizontal ($x$) axis and draw two lines (one for bias and one for variance). Identify the point with the best test error. Label the lines clearly. \vfill 
\item Give three reasons we may want to use a \emph{sparse} linear model: \vfill 
\item When might we prefer to use an $\ell_1$-loss instead of an $\ell_2$-loss for regression? \vfill 
\item Under what conditions is OLS unbiased? \vfill 
\item Rank the following models in terms of complexity:
OLS; $1$-Nearest Neighbor Regression; Cubic Spline Regression; Ridge Regression; Cubic Polynomial Regression. 

\vspace{0.5in}
\end{enumerate}

\clearpage
\section*{Issue Spotter (25 points)}


This is an \emph{issue spotter} question. Below, I describe (in words) a hypothetical application of the ML techniques we have discussed in this class. Your task is to find \textbf{5 mistakes} in the ML pipeline and to:
\begin{enumerate}[label={\roman*)}]
\item describe the problem (2 points each); and 
\item say how that step could have been performed more effectively / accurately. (3 points)
\end{enumerate}

Scenario:
\begin{quote}
    Baruch College is looking to better understand the post-graduation outcomes of its students. The Office of Institutional Research (OIR) sent a survey to 1,000
    randomly selected students to have graduated during the past 20 years and received 750 responses. Students were contacted at their last reported work email. OIR collected the following information:
    \begin{itemize}
        \item Years of Study at Baruch
        \item Year of Graduation
        \item Initial post-graduation salary
        \item Baruch GPA
    \end{itemize}
    OIR noted that more recent graduating classes had a higher response rate, but because the sample was selected uniformly from all graduates, it is representative.
    
    The CUNY Board has set a goal of ensuring over 80\% of Baruch graduates have salaries putting them in the middle class, which they define as a household income of \$50,000 or more annually (roughly 2/3 of the NYC median household income of \$75,000). Using this information, OIR creates a response variable 
    \[y_i = \begin{cases} 1 & \text{ Initial post-graduation salary } > 50,000 \\ 0 & \text{ Initial post-graduation salary } < 50,000 \end{cases} \] and builds the following OLS model to predict $y$:
    \[\hat{y} = -0.1 * \text{\# Years of Study} + 0.3* \text{Baruch GPA} + 0.25 * \text{Year of Graduation} \]
    
    The CUNY Board examines this model and recommends the following policy changes:
    \begin{itemize}
        \item Terminating all Bachelor's (4 year) programs and focusing only on Associate's (2 year) programs to decrease the average number of years of study at Baruch.
        \item Instructing the faculty to relax grading standards to raise average GPAs by 0.5.
    \end{itemize}
    OIR notes that, if these proposals had been enacted, their model predicts 35\% more middle class outcomes; the CUNY Board responds enthusiastically and implements these changes without a pilot program. OIR also notes that the fraction of middle-class salaries has increased rapidly over the past 4 years. In recognition of this achievement, the CUNY Board authorizes substantial performance-recognition compensation (bonuses) for Baruch leadership.
    
\end{quote}

\clearpage 

\textbf{Mistake \#1: }

\vfill 

\textbf{Fix \#1: }

\vfill 
\hrule 


\textbf{Mistake \#2: }

\vfill 

\textbf{Fix \#2: }

\vfill 

\hrule 

\textbf{Mistake \#3: }

\vfill 

\textbf{Fix \#3: }

\vfill 

\hrule


\textbf{Mistake \#4: }

\vfill 

\textbf{Fix \#4: }

\vfill 

\hrule 

\textbf{Mistake \#5: }

\vfill 

\textbf{Fix \#5: }

\vfill 

\clearpage
\section*{Mathematics of Machine Learning (25 points)}

Consider the \emph{generalized ridge regression} estimator given
\[\hat{\bbeta}_{\text{GRR}} = \text{argmin}_{\bbeta} \frac{1}{2}\|\by - \bX\bbeta\|_2^2 + \lambda \|\bD\bbeta\|_2^2\]
where $\bD$ is a known matrix. 
\begin{enumerate}[label={Q\arabic*.)}]
\item Derive a closed form expression for $\hat{\bbeta}_{\text{GRR}}$ (5 points).

\vspace{3in}
\item Write out a gradient descent algorithm that can be used to solve for $\hat{\bbeta}_{\text{GRR}}$ (10 points).


\vspace{6in}
\item Now, suppose $\bD$ is chosen so that 
\[\|\bD\bbeta\|^2_2 = \sum_{j=2}^p (\beta_j - \beta_{j-1})^2\]

What happens as $\lambda$ gets larger? When might you want to use this type of penalty? (5 points)

\vspace{3in}
\item How does this method differ from the \emph{fused lasso}, given by: 
\[\hat{\bbeta}_{\text{FL}} = \text{argmin}_{\bbeta} \frac{1}{2}\|\by - \bX\bbeta\|_2^2 + \lambda \sum_{j=2}^p |\beta_j - \beta_{j-1}|\]
? (5 points)

{\small \it There are many valid answers to this problem, but the simplest might be to take $\bX$ to be the identity matrix and to draw solutions for $\hat{\bbeta}_{\text{GRR}}$ and $\hat{\bbeta}_{\text{FL}}$.}

\end{enumerate}


\clearpage
\begin{center} (Extra page for longer answers) \end{center}

\clearpage
\begin{center} (Extra page for longer answers) \end{center}

\clearpage
\begin{center} (Blank page for scratch work - not graded) \end{center}


\clearpage
\begin{center} (Blank page for scratch work - not graded) \end{center}


\clearpage
\begin{center} (Blank page for scratch work - not graded) \end{center}

\end{document}
\section*{Question 0: Formatting and Presentation (15 points)}
Upload your submission to this assignment as a single \texttt{PDF} file on Blackboard. Ten points will be assigned based on formatting and presentation of your submission. For the best presentation, I recommend the use of \LaTeX\ or similar software (\emph{e.g.} Markdown + MathJax), but other software is allowed. 

All code used to produce figures in your submission should be included at the end of the PDF document.

\section*{Question 1: Properties of OLS (12 points)}
Prove the following properties of OLS: 
\begin{enumerate}[label={(\alph*)}]
\item Suppose data is generated as $y = \bbeta_*^{\top}\bx + \epsilon$ for some mean-zero $\epsilon$ noise. Show $\hat{\bbeta}$ is unbiased, \emph{i.e.}, $\E[\hat{\bbeta}] = \bbeta_*$. (3 points)
\item Suppose data is generated as $y = \alpha +  \bbeta_*^{\top}\bx + \epsilon$ for some mean-zero $\epsilon$ noise and $\bx$ that is mean zero. Show that $\alpha$ is equal to the average value of $y$. (3 points)
\item Given training data of the form $\{(\bx_i, y_i)\}_{i=1}^n$, show that the OLS in-sample prediction error (residuals) are given by $\hat{\by} - \by = \bX(\bX^{\top}\bX)^{-1}\bX^{\top}\by - \by = (\bI - \bX(\bX^{\top}\bX)^{-1}\bX^{\top})\by$. (3 points)
\item Suppose we fit OLS with an intercept term. Show that the mean (unsquared) error (residual) must be zero. (3 points)
\item Prove that $\bX^{\top}\bX$ is strictly positive-definite if the data matrix $\bX \in \R^{n \times p}$ has rank $n$ ($n \geq p$). (3 points)
\item Given $n$ observations and $p$ features, when  does OLS achieve 0 training error? (You may assume the data matrix $\bX \in \R^{n \times p}$ is full-rank) (3 points)
\end{enumerate}

\section*{Question 2:  $K$-Nearest Neighbors (33 points)}
\begin{enumerate}[label={(\alph*)}]
\item Write a $K$-nearest neighbor classifier (15 points) from scratch. 

Your function should perform the following
\begin{enumerate}
    \item Given a test point $\tilde{\bx}$, compute the distances to each row of the training data matrix $\bX$ using Euclidean ($L_2$ distance)
    \item Identify the $K$-nearest neighbors of $\tilde{\bx}$
    \item Take the majority vote of the nearest neighbors
\end{enumerate}
\item Download the \texttt{zip} code data from the data page at \url{https://hastie.su.domains/ElemStatLearn/}. Extract the training and test data for the \texttt{3}s and \texttt{8}s. 
\item Apply your $K$-NN classifier with $K=5$ to the training data and use it compute test error on the test data (misclassfication rate). (3 points)
\item Repeat this process for multiple values of $K$ and find which one minimizes i) training error; and ii) test error. Interpret your results (8 points)
\item Standardize your data so that each column of $\bX$ has the same variance (\emph{e.g.}, using the \texttt{scale} function in \texttt{R}) and repeat the above process. Do your results change?  (7 points)
\end{enumerate}


\section*{Question 3: The Bias-Variance Trade-Off in OLS (20 points)}

In this problem, you will examine the bias-viarance trade-off of OLS (least squares) regression. 

\begin{enumerate}[label={(\alph*)}]
    \item In class, we briefly derived the Bias-Variance-Irreducible decomposition of mean squared error. Give a formal proof of the following result
    \[\E[\text{MSE}] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Noise}\]
    including definitions of all four terms. (4 points)
    \item Generate $n=25$ training samples from the following linear model: \[y = 3x_1 - 2x_2 + \epsilon\]
    where $x_1, x_2$ are independently $\mathcal{N}(0, 5^2)$ and $\epsilon$ is drawn from a standard normal distribution. Calculate and report the following error measures: (2 points)
    \begin{itemize}
        \item In-sample (training) MSE
        \item In-sample Bayes MSE (or the irreducible error): \emph{i.e.}, the prediction error you would get from the \emph{true} solution
        \item Out-of-sample (test) MSE on 25 new test points
        \item Out-of-sample Bayes MSE on the same 25 test points.
    \end{itemize}
    \item Repeat the above process a large number of times to get the expected values of the four errors. What are they for this problem and how do they change as you vary $n$? (3 points)
    \item Repeat this process a large number of times and show that OLS is indeed unbiased for this linear model (2 points)
    \item Using the MSE decomposition above and the fact that OLS is unbiased for linear models, what is the variance and what is the irreducible noise for this problem? (2 points)
    \item Repeat the above analysis for the non-linear function: 
    \[y = \text{sign}(x_1) * x_1^2 + \cos(x_2) + \epsilon \]
    What is the bias, variance, and irreducible noise for this problem? How do they change with $n$? (4 points)
    \item Repeat the analysis again but now with $x_1, x_2 \buildrel{\textsc{iid}}\over\sim \mathcal{N}(25, 5^2)$: how do the bias, variance, and irreducible noise change? Why? (3 points)
\end{enumerate}

\section*{Question 4: Fitting Least Squares Models (20 points)}

In class, we derived the closed form expression for OLS coefficients: $\hat{\bbeta} = \bX^{\dagger}\by = (\bX^{\top}\bX)^{-1}\bX^{\top}\by$. In this question, we will explore alternate approaches to fitting linear models. 

\begin{enumerate}[label={(\alph*)}]
    \item Generate 100 samples from the following data generating process (2 points): 
    \begin{align*}
        \bx &\sim \mathcal{N}(\bzero_5, \bI_{5 \times 5}) \\
        y &\sim \mathcal{N}\left(x_1 + \sqrt{x_2^2 + 5} + 0.1x^3_3 + \cos|x_4| + \frac{1}{|x_5| + 3}, 0.25\right)
    \end{align*}
    \item Fit a linear model to this data using the closed-form solution (not the built-in \texttt{lm} function!) and plot $\hat{y}$ against $y$. (3 points)
    \item \emph{Gradient Descent} methods fit models by taking small steps in the direction of the gradient of the loss function. For classic OLS, this gradient is given by: 
    \begin{align*}
        \mathcal{L}(\bbeta) &= \frac{1}{2}\|\by - \bX\bbeta\|_2^2 \\
        \frac{\partial \mathcal{L}}{\partial \bbeta} &= -\bX^{\top}(\by - \bX\bbeta)
    \end{align*}
    and the gradient update is given by: 
    \[\bbeta^{(k+1)} \leftarrow \bbeta^{(k)} - c * \left.\frac{\partial \mathcal{L}}{\partial \bbeta}\right|_{\bbeta = \bbeta^{(k)}}\]
    That is, at step-$k$, use your current estimate of $\bbeta$, $\bbeta^{(k)}$ to compute the gradient, multiply it by a small constant $c$,\footnote{For this problem, $c < 1/200$ should suffice.} and subtract it to get your new estimate $\bbeta^{(k+1)}$. Repeat this process until $\bbeta^{(k+1)}$ stops changing ($\|\bbeta^{(k)} - \bbeta^{(k+1)}\| < 1\times 10^{-6}$). 

    Implement gradient descent for OLS and show that you get nearly the same value as the closed form solution. (5 points)
    \item Generate 100 new test data points. Repeat gradient descent, but now using the original (training) and new (test) data points to compute the training and test loss at each step. Plot the evolution of these quantities over GD iterates. Does it make sense to run GD all the way to (numerical) convergence? (5 points)
    \item Modern machine learning methods are often fit with \emph{weight decay}. Weight decay updates the gradient descent update to: 
        \[\bbeta^{(k+1)} \leftarrow \bbeta^{(k)} - c * \left.\frac{\partial \mathcal{L}}{\partial \bbeta}\right|_{\bbeta = \bbeta^{(k)}} - \omega \bbeta^{(k)}\]
    \emph{i.e.}, we subtract off a little bit extra in each update step.

    Implement OLS with weight decay and show that it corresponds to the equivalent ridge regression solution, $(\bX^{\top}\bX + \omega \bI)^{-1}\bX^{\top}\by$. (2 points)
    \item Use 5-fold cross validation to identify the optimal value of the weight decay parameter. (3 points)
\end{enumerate}

\end{document}
