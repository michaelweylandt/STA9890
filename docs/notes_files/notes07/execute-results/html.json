{
  "hash": "31811ed89a9870f81345597c7de70eae",
  "result": {
    "engine": "knitr",
    "markdown": "---\nsession: \"7\"\nauthor: \"Michael Weylandt\"\ntopic: \"Classification II: Discriminative Classifiers\"\ntitle: \"{{< var course.short >}} - {{< meta topic >}}\"\n---\n\n\n\n$$\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\bx}{\\mathbf{x}}\\newcommand{\\bbeta}{\\mathbf{\\beta}} \\newcommand{\\P}{\\mathbb{P}}\\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\by}{\\mathbf{y}}\\newcommand{\\argmax}{\\text{arg\\,max}}\\newcommand{\\argmin}{\\text{arg\\,min}}$$\n\nThis week we begin to examin _discriminative_ classifiers. Unlike\ngenerative classifiers, which attempt to model $\\bx$ as a function\nof $y$ and then use Bayes' rule to _invert_ that model, \ndiscriminative classifiers take the more standard approach of \ndirectly modeling $y$ as a function of $\\bx$. This is the\napproach you have already seen in our various variants of OLS. \n\n\n## Why Not Accuracy Maximization? \n\nAs we saw in our regression unit, we can pose many interesting\nestimators as _loss minimization_ problems. For least squares, we\nprimarily focused on mean squared error (MSE) as a loss function,\nthough we also briefly touched on MAE, MAPE, and 'check' losses.\nThere is not a single 'canonical' loss for the classification\nsetting and different choices of loss will yield different\nclassifiers. \n\nBefore we build our actual first loss, it is worth asking\nwhy we can't use something like classification accuracy as our loss\nfunction. Specifically, define\n$$\\text{Acc}(y, \\hat{y}) = \\begin{cases} 1 & y = \\text{sign}(\\hat{y}) \\\\ 0 & y \\neq \\text{sign}(\\hat{y}) \\end{cases}$$\nHere we consider the case where $\\hat{y}$ may be real-valued \nas a (slight) generalization of probabilistic classification and\nwe use the $\\pm 1$ convention for the two classes. \n\nFixing $y = 1$, $\\text{Acc}$ has the following shape: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat <- seq(-3, 3, length.out=201)\nacc <- function(y_hat, y=1) sign(y_hat) == sign(y)\n\nplot(y_hat, acc(y_hat), type=\"l\",\n     xlab=expression(hat(y)),\n     ylab=expression(Acc(hat(y), y==1)), \n     main=\"Accuracy as a Loss Function\")\n```\n\n::: {.cell-output-display}\n![](notes07_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThis is a terrible loss function! It is both non-convex (can you see\nwhy?) and just generally unhelpful. Because it is flat almost\neverywhere, we have no access to gradient information (useful for\nfitting) or to any sense of 'sensitivity' to our loss. In this\nscenario, if $y=1$, both $\\hat{y}=0.00001$ and $\\hat{y}=0.9999$\nhave the same accuracy even though the latter is a more 'confident'\nprediction. \n\n## OLS for Classification?\n\nSo if not accuracy, where else might we get a good loss function?\nYou might first ask if we an use OLS? It works perfectly well for \nregression and we know it is well-behaved, so what happens if we\ntry it for classification? The answer is ... complicated. \n\nOLS as a _loss_ function for classification is a bit strange, but\nit turns out to be more-or-less fine. In particular, OLS can be\nused to evaluate _probabilistic_ classifiers, where it is known\nas the _Brier score_. OLS as a _predictor_ can be a bit more \nproblematic: in particular, if we just predict with \n$\\P(y=1) = \\bx^{\\top}\\bbeta$, we have to deal with the fact that\n$\\hat{y}$ can be _far_ outside a $[0, 1]$ range we might want\nfrom a probabilistic classifier. In certain limited circumstances, \nwe can assume away this problem (perhaps by putting specific\nbounds on $\\bx$), but these fixes are fragile. Alternatively, \nwe can try to 'patch' this approach and use a classifier\nlike \n\n$$\\P(y=1) = \\begin{cases} 1 & \\bx^{\\top}\\bbeta > 1 \\\\ 0 & \\bx^{\\top}\\bbeta < 0 \\\\ \\bx^{\\top}{\\bbeta} & \\text{ otherwise}\\end{cases}$$\n\nEven if this feels a bit unsophisticated, Ttis is not awful, \nbut it still struggles from the 'long flat region' problems that\naccuracy encounters. \n\nAt this point, it's hopefully clear that it will be a bit hard to\n'hack together' a suitable loss and that we might benefit from\napproaching the problem with more theoretical grounding. \n\n## Deriving Logistic Regression\n\nWhile we argued that OLS is well-grounded in MSE alone, we can\nrecall it has additional connections to the Gaussian. These\n\"Gaussian vibes\" were not essential to making OLS work, but \nthey were useful in _deriving_ OLS as a sensible procedure. \nCan we do something similar for classification? That is, can we \nassume a 'working model' to come up with a good loss function\nand then use it, even if we doubt that the model is 'correct'? \n\nAs with all leading questions, the answer is of course a resounding\nyes. We start from the rather banal observation that $y$ must have\na Bernoulli distribution conditional on $\\bx$. After all, the\nBernoulli is (essentially) the only $\\{0, 1\\}$ distribution we\nhave to work with. So we really just need a way to model\nthe $p$ parameter of a Bernoulli as a function of $\\bx$. Because\nwe love linear models, we may choose to set $p = \\bx^{\\top}\\bbeta$,\nbut this gets us back to the range problem we had above. \n\nSpecifically, we require the Bernoulli parameter to take values in\n$[0, 1]$ but our linear predictor $\\bx^{\\top}\\bbeta$ takes values\nin all of $\\R$. We can fix this with a simple 'hack': we need a \nfunction that 'connects' $\\R$ to $[0, 1]$. If we call this function\n$\\mu$, we then can specify our whole model as $$y | \\bx \\sim \\text{Bernoulli}(\\mu(\\bx^{\\top}\\bbeta))$$ and reduce our problem\nto estimating $\\bbeta$. Where can we get such a function? \n\nThe most common choice is $$\\mu(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1+e^{-z}}$$, which is known as the _logistic_ or\n_sigmoid_ function due to its 's-like' shape: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz <- seq(-5, 5, length.out=201)\nplot(z, 1/(1+exp(-z)), type=\"l\", main=\"Sigmoid Mapping\")\n```\n\n::: {.cell-output-display}\n![](notes07_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nMore generally, we can use essentially the CDF of any random\nvariable supported on the real line as a choice of $\\mu$: since\nCDFs map the support onto the $[0, 1]$ range they are perfect \nfor this choice. Unfortunately, most CDFs are a bit unwieldy\nso this approach, while theoretically satisfying, is not too widely\nused in practice. If you see it, the most common choice is \n$\\mu(z) = \\Phi(z)$, the normal CDF, resulting in a method known as\n\"probit\" regression (after 'probability integral transform,' an\nold-fashioned name for the CDF) or \n$$\\mu(z) = \\frac{\\tan^{-1}(z)}{\\pi} + \\frac{1}{2}$$\nwhich gives a method known as \"cauchit\" regression, since\nthis is the CDF of the standard Cauchy distribution. These\nchoices are far less common than the default sigmoid we used\nabove. Generally, they are only _slightly_ different in practice\nand far more computationally burdensome and just aren't really worth\nit. \n\nReturning to our default sigmoid, we now have the model \n$$ y | \\bx \\sim \\text{Bernoulli}\\left(\\frac{1}{1+e^{-\\bx^{\\top}\\bbeta}}\\right)$$\nThis model is well-posed (in the sense that the distribution 'fits'\nthe data and we are guaranteed never to put invalid parameters in)\nbut we don't yet have a way to use it as a loss function. \n\n## From Model to Loss Function\n\nWe can build a loss-function by relying on the _maximum likelihood_\nprinciple. The maximum likelihood principle is a core idea of\nstatistics - and one you will explore in much greater detail in other\ncourses - but, in essence, it posits that we should use the\n(negative) PMF or PDF as our loss function. Specifically, the ML\nprinciple says that, if many models could fit our data, we should\npick the one that makes our data _most probable_ (as determined\nby the PMF/PDF). \n\nBefore we work out the ML estimator (MLE) for our classification \nmodel,  let's take a brief detour and look at the MLE for \n(Gaussian) regression. Specifically, if we assume a model \n$$y \\sim \\mathcal{N}(\\bx^{\\top}\\bbeta, \\sigma^2)$$ for known \n$$\\sigma^2$$, the PDF of the training point $(\\bx_1, y_1)$ is\n\n$$p(y_1 | \\bx_1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{\\|y_1 - \\bx_1^{\\top}\\bbeta\\|_2^2}{2\\sigma^2}\\right\\}$$\n\nIf we have a set of $n$ IID training data points, the joint PDF can\nbe obtained by multiplying together PDFs (IID is great!) to get\n\n$$\\begin{align*}\np(\\mathcal{D}_{\\text{train}}) &= \\prod_{i=1}^n p(y_i | \\bx_i) \\\\\n&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{\\|y_i - \\bx_i^{\\top}\\bbeta\\|_2^2}{2\\sigma^2}\\right\\} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\|y_i - \\bx_i^{\\top}\\bbeta\\|_2^2\\right\\} \\\\\n&= (2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2} \\|\\by - \\bX\\bbeta\\|_2^2\\right\\} \\\\\n\\end{align*}$$\n\nWe could maximize this, but a few minor tweaks make the problem\nmuch easier: \n\ni) Since all of the action is in an exponential, we might as well\n   maximize the _log_ of the PDF instead of the raw PDF. Since \n   logarithms are a monotonic transformation, this won't change\n   our minimizer. Similarly, we will also strategically drop\n   some constant terms as they also do not change the minimizer. \nii) By convention, we like to _minimize_ more than _maximize_, \n    specifically when dealing with convex approaches, so we \n    will introduce a sign flip.\n    \n$$\\begin{align*}\n\\hat{\\bbeta} &= \\argmax_{\\bbeta} p(\\mathcal{D}_{\\text{train}}) \\\\\n             &= \\argmax_{\\bbeta} \\log p(\\mathcal{D}_{\\text{train}}) \\\\\n             &= \\argmax_{\\bbeta} \\log\\left( (2\\pi\\sigma^2)^{-n/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n \\|\\by - \\bX\\bbeta\\|_2^2\\right\\}\\right) \\\\\n             &= \\argmax_{\\bbeta} -\\frac{n}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\|\\by - \\bX\\bbeta\\|_2^2 \\\\\n             &= \\argmin_{\\bbeta} \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\|\\by - \\bX\\bbeta\\|_2^2 \\\\\n             &= \\argmin_{\\bbeta} \\frac{1}{2}\\|\\by - \\bX\\bbeta\\|_2^2\n\\end{align*}$$\n\nWhy were we able to do the simplifications in the last line? (You\nmight also recognize the _penultimate_ line as a term used in the AIC\nof a linear model.)\n\nThis is quite cool! If we assume $\\by$ follows a particular \nGaussian, we get OLS by applying the general MLE approach. Deeper\nstatistical theory tells us that the MLE is (essentially) optimal,\nso if our data is not too 'un-Gaussian' it makes sense that the \nGaussian MLE (OLS) will perform reasonably well. Since many things\nin this world are Gaussian-ish, OLS is optimal-ish for a large\nclass of interesting problems. \n\nReturning to our classification problem, we see that if our \ntraining data is IID, the MLE can be obtained by minimizing\nthe negative sum of the log PDFs. For space, we often start from \nthis point instead of doing the full set of manipulations from scratch.\n\nSo what is the Bernoulli log PMF? Recall that if $B \\sim \\text{Bernoulli}(p)$, its PMF is given by: \n\n$$\\P(B=k) = \\begin{cases} p & k=1 \\\\ 1-p & k = 0 \\end{cases}$$\n\nor more compactly, \n\n$$\\P(B = k) = p^k(1-p)^(1-k)$$\n\nTaking logarithms, we have\n\n$$\\log \\P(B = k) = k \\log p + (1-k) \\log(1-p)$$\n\nand hence the negative log-likelihood: \n\n$$-\\log \\P(B = k) = -k \\log p -(1-k) \\log(1-p)$$\n\n\nFrom our working model, we take $p = 1/(1+e^{-\\bx^{\\top}\\bbeta})$ so our joint\nMLE is given by: \n\n$$\\hat{\\bbeta} = \\argmin_{\\bbeta} \\sum_{i=1}^n -y_i \\log\\left(1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right) - (1-y_i)\\log\\left(1 - 1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right)$$\n\nThis is still quite hairy, so let's simplify it. In the first term, we note\nthat $-\\log(1/x) = \\log x$ to get: \n\n$$\\hat{\\bbeta} = \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) - (1-y_i)\\log\\left(1 - 1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right)$$\n\nFor the second term, note that\n\n$$1-\\frac{1}{1+e^{-z}} = \\frac{1+e^{-z} - 1}{1+e^{-z}} = \\frac{e^{-z}}{1+e^{-z}} \\implies \\log\\left(1-\\frac{1}{1+e^{-z}}\\right) = -z - \\log(1 + e^{-z})$$\n\nso we get: \n\n$$\\begin{align*}\n\\hat{\\bbeta} &=  \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) - (1-y_i)\\log\\left(1 - 1/(1+e^{-\\bx_i^{\\top}\\bbeta})\\right) \\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) - (1-y_i)\\left[-\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right] \\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) + (1-y_i)\\left[\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right] \\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n y_i \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right) + \\left[\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right] -y_i\\left[\\bx_i^{\\top}\\bbeta + \\log\\left(1+e^{-\\bx_i^{\\top}\\bbeta}\\right)\\right]\\\\\n&= \\argmin_{\\bbeta} \\sum_{i=1}^n-y_i \\bx_i^{\\top}\\bbeta + \\bx_i^{\\top}\\bbeta + \\log\\left(1-e^{-\\bx_i^{\\top}\\bbeta}\\right)\n\\end{align*}$$\n\nWe could work in this form, but it turns out to actually be a bit nicer to \n'invert' some of the work we did above to get rid of a term. In particular, \nnote\n\n$$z + \\log(1-e^{-z}) = \\log(e^z) + \\log(1-e^{-z}) = \\log(e^z + e^{z-z}) = \\log(1 + e^z)$$\n\nwhich gives us: \n\n$$\\hat{\\bbeta} = \\argmin_{\\bbeta} \\sum_{i=1}^n-y_i \\bx_i^{\\top}\\bbeta  + \\log\\left(1+e^{\\bx_i^{\\top}\\bbeta}\\right)$$\n\nWow! Long derivation. And we still can't actually solve this!\n\nUnlike OLS, where we could obtain a closed-form solution, we *have to use* \nan iterative algorithm here. While we could use _gradient_ descent, we can\nget to an answer far more rapidly if we use more advanced approaches. Since\nthis is not an optimization course, we'll instead use some software to solve\nfor us: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data from the logistic model\nn <- 500\np <- 5\nX    <- matrix(rnorm(n * p), ncol=p)\nbeta <- c(1, 2, 3, 0, 0)\n\nP <- 1/(1+exp(-X %*% beta))\ny <- rbinom(n, size=1, prob=P)\n\n# Use optimization software\n# See https://cvxr.rbind.io/cvxr_examples/cvxr_logistic-regression/ for details\nlibrary(CVXR)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'CVXR'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    id\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    is_vector\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    power\n```\n\n\n:::\n\n```{.r .cell-code}\nbeta <- Variable(p)\neta  <- X %*% beta\n\nobjective <- sum(- y * eta + logistic(eta))\nproblem   <- Problem(Minimize(objective))\n\nbeta_hat <- solve(problem)$getValue(beta)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|  beta_hat|\n|---------:|\n| 0.9975707|\n| 1.6927524|\n| 2.8273155|\n| 0.0471099|\n| 0.1771164|\n\n\n:::\n:::\n\n\n\nNot too bad, if we compare this to `R`'s built-in logistic regression function, \nwe see our results basically match: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(y ~ X + 0, family=binomial))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ X + 0, family = binomial)\n\nCoefficients:\n   Estimate Std. Error z value Pr(>|z|)    \nX1  0.99757    0.16318   6.113 9.76e-10 ***\nX2  1.69275    0.19420   8.716  < 2e-16 ***\nX3  2.82732    0.27231  10.383  < 2e-16 ***\nX4  0.04711    0.13997   0.337    0.736    \nX5  0.17712    0.12503   1.417    0.157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 500  degrees of freedom\nResidual deviance: 332.85  on 495  degrees of freedom\nAIC: 342.85\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\nGood match! \n\nNote that, in the above, we had to use the `logistic` function built into `CVX`.\nIf we used `log(1+exp(eta))`, the solver would not be able to prove that it is\nconvex and would refuse to try to solve. See the `CVX` documentation for more \ndetails.\n\nWe can compute accuracy after defining a 'decision rule'. Here, let's\njust round the predicted probability\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_hat <- 1/(1 + exp(-X %*% beta_hat))\ny_hat <- round(p_hat)\n\nmean(y_hat == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.844\n```\n\n\n:::\n:::\n\n\n\nNow that we have this toolkit built up, we can also easily apply ridge and lasso\npenalization: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenalty <- sum(abs(beta))\n\nlasso_problem <- Problem(Minimize(objective + 9 * penalty))\nbeta_hat_lasso <- solve(lasso_problem)$getValue(beta)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|          |\n|---------:|\n| 0.6323610|\n| 1.1913097|\n| 2.0446974|\n| 0.0000000|\n| 0.0375919|\n\n\n:::\n:::\n\n\n\nWe got some sparsity, as we would expect with $\\ell_1$ penalization. \n\n## Generalized Linear Models\n\nThe approach we took here is a special case of a _generalized linear model_: \ngeneralized linear models (GLMs) consist of three parts: \n\n- A linear model for the 'core predictor' $\\eta = \\bX\\bbeta$; \n- A sampling distribution for the data $y \\sim \\text{Dist}$; and\n- An 'adaptor' function[^link], $\\mu$, that maps $\\eta \\in \\R$ to the parameter \n  space of $y$: specifically, we need $\\E[y | \\bx] = \\mu(\\bx^{\\top}\\bbeta)$.\n  \nThe adaptor function needs to be smooth (for nice optimization properties) and\nmonotonic (or else the interpretation gets too weird), but we otherwise have some\nflexibility. As noted above, if we pick $\\mu$ to be the normal or Cauchy CDFs, we\nget alternative 'Bernoulli Regressions' (probit and cauchit). \n\nWe can generalize the model for $\\eta$ by allowing it to be an additive (spline)\nmodel or a kernel method. To fit splines, you can use the `gam` function\nfrom the `mgcv` function. \n\nFinally, we also have choices in the sampling distribution. While normal and\nBernoulli are the most common, you will also sometimes see Poisson and Gamma\nin the wild. For both of these, the mean is a positive value, so we require\nan adaptor that maps $\\R$ to $\\R \\geq 0$ and we typically take $\\mu(z) = e^z$. \n\nA good exercise is to repeat the above analysis for Poisson regression and compare\nyour result (obtained with `CVXR`) to the Poisson regression built into `R`. \n\n## Loss Functions for Classification\n\nWe now have a workable loss function for discriminative classification.\nReturning to our example from above \n(changed to $\\{0, 1\\}$ convention briefly). Let us set \n$\\alpha = y\\hat{y}$ and investigate our loss functions as a property\nof $\\alpha$: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- seq(-3, 3, length.out=201)\nacc     <- function(alpha) ifelse(alpha < 0, 1, 0)\nlr_loss <- function(alpha) log(1 + exp(-2*alpha))\n\nplot(alpha, acc(alpha), type=\"l\",\n     xlab=expression(alpha),\n     ylab=\"Loss\", \n     main=\"Classification Losses\", \n     ylim=c(0, 5))\nlines(alpha, lr_loss(alpha), col=\"red4\")\nlegend(\"topright\", \n       col=c(\"black\", \"red4\"), \n       lwd=2, \n       legend=c(\"Accuracy\", \"Logistic Regression\"))\n```\n\n::: {.cell-output-display}\n![](notes07_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nWe see here that the logistic loss defines a _convex surrogate_ of the \nunderlying accuracy. Just like we used $\\ell_1$ as a tractable\napproximation for best squares, logistic regression can be considered\na tractable approximation for accuracy minimization.[^persp]\n\nOther useful loss functions can be created using this perspective, \nperhaps none less important than the *hinge loss*, which gives rise\nto a classifier known as the *support vector machine* (SVM). \n\n## Support Vector Machines\n\nIn this section, we go back to $\\{\\pm 1\\}$ convention. \n\nInstead of using a logistic loss, consider a hinge loss of the form\n\n$$H(y, \\hat{y}) = (1 - y\\hat{y})_+$$\n\nWhen $y = \\hat{y}=1$ or $y = \\hat{y} = 0$, this is clearly 0 as we\nwould expect from a good loss. What happens for cases where our \nprediction is wrong or not 'full force', say $\\hat{y} = 1/2$. \n\nLooking ahead, we will use a linear combination of features to create\n$\\hat{y} = \\bx^{\\top}\\bbeta$ yielding:\n\n$$H(\\bbeta) = (1 - y \\bx^{\\top}\\bbeta)_+$$\n\nIf $y=1$, we get zero loss so long as $\\bx^{\\top}\\bbeta > 1$ and a \nsmall loss for $0 < \\bx^{\\top}\\bbeta < 1$. As $\\bx^{\\top}\\bbeta$\ncrosses zero and goes negative, the loss grows linearly without\nbound. (Reverse all of this for the case $y = -1$.) This is an\ninteresting loss function: we cannot make our loss decrease\nas our predictions become 'more right', but our loss continues\nto increase as our prediction becomes 'more wrong'.\n\nVisually, we can draw this on our plot from above: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- seq(-3, 3, length.out=201)\nacc     <- function(alpha) ifelse(alpha < 0, 1, 0)\nlr_loss <- function(alpha) log(1 + exp(-2*alpha))\nhinge_loss <- function(alpha) pmax(0, 1-alpha)\n\nplot(alpha, acc(alpha), type=\"l\",\n     xlab=expression(alpha),\n     ylab=\"Loss\", \n     main=\"Classification Losses\", \n     ylim=c(0, 5))\nlines(alpha, lr_loss(alpha), col=\"red4\")\nlines(alpha, hinge_loss(alpha), col=\"green4\")\nlegend(\"topright\", \n       col=c(\"black\", \"red4\", \"green4\"), \n       lwd=2, \n       legend=c(\"Accuracy\", \"Logistic Regression\", \"Hinge Loss\"))\n```\n\n::: {.cell-output-display}\n![](notes07_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\nThis is also a pretty nice surrogate loss. In this case, let's just\ngo 'straight at it' to construct a classifier: \n\n$$\\hat{\\bbeta} = \\argmin \\sum_{i=1}^n (1-y_i \\bx_i^{\\top}\\bbeta)_+$$\n\nThis classifier is not uniquely defined for many problems (because of\nthe long flat part of the loss), so it is conventional to add a \n_regularization_ term to the SVM: \n\n$$\\hat{\\bbeta} = \\argmin \\sum_{i=1}^n (1-y_i \\bx_i^{\\top}\\bbeta)_+ + \\lambda \\|\\bbeta\\|_2^2$$\n\nWe can solve this directly using CVX as follows: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data from the logistic model and fit an SVM\nn <- 500\np <- 5\nX    <- matrix(rnorm(n * p), ncol=p)\nbeta <- c(1, 2, 3, 0, 0)\n\nP <- 1/(1+exp(-X %*% beta))\ny <- rbinom(n, size=1, prob=P)\n## Convert to +/-1 convention\ny <- 2 * y - 1\n\nlibrary(CVXR)\n\nbeta <- Variable(p)\neta  <- X %*% beta\n\nobjective <- sum(pos(1 - y * eta)) + 0.5 * norm2(beta)^2\nproblem   <- Problem(Minimize(objective))\n\nbeta_hat <- solve(problem)$getValue(beta)\n```\n:::\n\n\n\nUsing the decision rule $\\hat{y} = \\text{sign}(\\bx^{\\top}\\bbeta)$, this\ngives us good accuracy: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sign(X %*% beta_hat) == y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.88\n```\n\n\n:::\n:::\n\n\n\nThis is comparable to what we got for logistic regression even\nthough: \n\ni) the loss function is 'wrong' (doesn't model the DGP)\nii) we did no tuning of the ridge parameter\n\nSo, all in all, pretty impressive. \n\nSVMs are a remarkably powerful and general classification technology.\nBefore we move past them, we will now take a second perspective\nfocusing on the _geometry_ of SVMs. \n\n## SVM: The Geometric Viewpoint\n  \n[^link]: For historical reasons, this is known as the _inverse link_, but I prefer\nto simply think of it as an adaptor and to never use the (forward) link. \n\n[^persp]: This 'surrogate loss' perspective was first discussed in: \nP.L. Barlett, M.I. Jordan, and J.D. McAuliffe. \"Convexity, Classification, and Risk Bounds\". *Journal of the American Statistical\nAssociation* **101(473)**, pp.138-156. 2006. DOI:[10.1198/016214505000000907](https://dx.doi.org/10.1198/016214505000000907)\n",
    "supporting": [
      "notes07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}