{
  "hash": "d1aa49a0ba0c059ac67affb14fa6bb4f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"{{< var course.short >}} - Regularized Regression\"\n---\n\n\n\nLast week we showed the following: \n\n- OLS can be derived purely as a 'loss minimization' problem without reference\n  to specific probabilistic models; this is an instance of the general strategy\n  of *empirical risk management*\n- OLS is a *convex* problem, making it particularly easy to solve even if we\n  don't use the closed-form solution\n- The BLUE property of OLS is perhaps less interesting than it sounds as it\n  requires very strong assumptions on the data generating process which are\n  unlikely to hold in practice\n- MSE, the loss we used to pose OLS, can be decomposed as \n  $\\text{Bias}^2 + \\text{Variance}$\n- Because BLUE restricts to _unbiased_ estimators, the MSE of OLS is all variance\n\nWe begin this week by asking if we can do better than OLS. To keep things simple,\nwe begin by assuming we are under a linear DGP (so no 'model error') but that's\nonly a mathematical niceity. It's not something you should always assume - in fact,\nit is really more important to think about how models do on non-linear DGPs. \nAs we will see, it may still be useful to use linear models... \n\nBecause OLS is BLUE under our assumptions, we know that we need to relax one\nor more of our assumptions to beat it. For now, we will focus on relaxing the\nU - unbiasedness; non-linear methods come later in this course. \n\nRecalling our decomposition: \n\n$$\\text{MSE} = \\text{Bias}^2 + \\text{Variance}$$\n\nOur gambit is that we can find an alternative estimator with a bit more bias, \nbut far less variance. Before we attempt to do so for linear regression, let's \nconvince ourselves this is possible for a much simpler problem - estimating\nmeans. \n\nSuppose we have data from a distribution $$X_i \\buildrel \\text{iid} \\over \\sim \n\\mathcal{N}(\\mu, 1)$$ for some unknown $\\mu$ that we seek to estimate. Quite \nreasonably, we might use the sample mean \n$$\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$$\nto estimate $\\mu$. Clearly, this is an _unbiased_ estimator and it has variance\ngiven by $1/n$, which isn't bad. In general, it's pretty hard to top this. \n\nWe can verify all of this empirically: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(purrr)\n\ncompute_mse_sample_mean <- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R <- replicate(1000, {\n        X <- rnorm(n, mean=mu, sd=1)\n        mean(X)\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nMU_GRID <- seq(-5, 5, length.out=201)\nN <- 10\n\nSIMRES <- map(MU_GRID, compute_mse_sample_mean, n=N) |> list_rbind()\n```\n:::\n\n\n\nOur bias is essentially always zero: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES, aes(x=mu, y=bias)) + \n    geom_point() + \n    geom_abline(slope=0, \n                intercept=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(expression(\"Bias\"^2)) + \n    ggtitle(\"Constant Zero Bias of Sample Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nSimilarly, our bias is small, and constant. Specifically, it is around \n$1/n$ as predicted by standard theory: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES, aes(x=mu, y=variance)) + \n    geom_point() + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Variance of Sample Mean\") + \n    ggtitle(\"Constant Variance of Sample Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nAs expected, the MSE is the sum of bias and variance, so it's basically just\nvariance here: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Sample Mean MSE \") + \n    ggtitle(\"Constant MSE of Sample Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nSo far, it looks like the sample mean is hard to beat. In particular,\nthis curve is But... what if we know, *e.g.*, that $\\mu$ is positive. We \nmight still use the sample mean, but with the additional step that we set\nit to zero if the sample mean looks negative. That is, our new estimator is\n$$\\hat{\\mu} = (\\overline{X}_n)_+ \\text{ where } z_+ = \\begin{cases} z & z > 0 \\\\ 0 & z \\leq 0 \\end{cases}$$\n\nThe $(\\cdot)_+$ operator is known as the *positive-part*. How does this \n$\\hat{\\mu}$ do? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npospart <- function(x) ifelse(x > 0, x, 0)\ncompute_mse_positive_mean <- function(mu, n){\n    # Compute the MSE estimating mu\n    # with the positive part of the sample mean from n samples\n    # We repeat this process a large number of times\n    # to get the expected MSE\n    R <- replicate(1000, {\n        X <- rnorm(n, mean=mu, sd=1)\n        pospart(mean(X))\n    })\n    \n    data.frame(n=n, \n               mu=mu, \n               bias=mean(R - mu), \n               variance=var(R), \n               mse=mean((R - mu)^2))\n}\n\nMU_GRID <- seq(-5, 5, length.out=201)\nN <- 10\n\nSIMRES_POSPART <- map(MU_GRID, compute_mse_positive_mean, n=N) |> \n    list_rbind()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(SIMRES_POSPART, aes(x=mu, y=mse)) + \n    geom_point() + \n    geom_abline(intercept=0, \n                slope=0, \n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    geom_abline(intercept=1/N, \n                slope=0,\n                color=\"black\", \n                lwd=2, \n                lty=2) + \n    xlab(expression(\"True Parameter:\" ~ mu)) + \n    ylab(\"Positive Part Mean MSE\") + \n    ggtitle(\"Constant MSE of Positive Part Mean Estimator\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes03_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "notes03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}