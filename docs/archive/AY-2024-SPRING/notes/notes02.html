<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michael Weylandt">

<title>STA 9890 - Regression I: Review of Linear Algebra, Convex Optimization, OLS – STA 9890</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-1c47065645911eb62ace0da5c8a22cd9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./../index.html">
    <span class="navbar-title">STA 9890</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./.././syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./.././notes.html"> 
<span class="menu-text">Handouts and Additional Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./../reports.html"> 
<span class="menu-text">Research Reports</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./.././competition.html"> 
<span class="menu-text">Course Competition</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./.././resources.html"> 
<span class="menu-text">Additional Resources and Policies</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./.././objectives.html"> 
<span class="menu-text">Learning Objectives</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<div id="quarto-announcement" data-announcement-id="6a42bbecdcf2d6ce6b61d47c55fab2f9" class="alert alert-danger">
  <i class="bi bi-info-circle quarto-announcement-icon"></i>
  <div class="quarto-announcement-content">
    <strong>Warning</strong> - This page is for a prior offering of STA 9890. For the latest offering, click <a id="exit-archive-link" href="../../../notes/notes02.html">here</a
>.
  </div>
</div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#linear-algebra-review" id="toc-linear-algebra-review" class="nav-link active" data-scroll-target="#linear-algebra-review">Linear Algebra Review</a>
  <ul class="collapse">
  <li><a href="#vector-arithmetic" id="toc-vector-arithmetic" class="nav-link" data-scroll-target="#vector-arithmetic">Vector Arithmetic</a></li>
  <li><a href="#vector-length-and-angle" id="toc-vector-length-and-angle" class="nav-link" data-scroll-target="#vector-length-and-angle">Vector Length and Angle</a></li>
  <li><a href="#matrices" id="toc-matrices" class="nav-link" data-scroll-target="#matrices">Matrices</a></li>
  <li><a href="#spectral-properties-of-symmetric-matrices" id="toc-spectral-properties-of-symmetric-matrices" class="nav-link" data-scroll-target="#spectral-properties-of-symmetric-matrices">Spectral Properties of Symmetric Matrices</a></li>
  </ul></li>
  <li><a href="#ordinary-least-squares" id="toc-ordinary-least-squares" class="nav-link" data-scroll-target="#ordinary-least-squares">Ordinary Least Squares</a></li>
  <li><a href="#bias-variance-decomposition" id="toc-bias-variance-decomposition" class="nav-link" data-scroll-target="#bias-variance-decomposition">Bias-Variance Decomposition</a></li>
  <li><a href="#introduction-to-convex-optimization" id="toc-introduction-to-convex-optimization" class="nav-link" data-scroll-target="#introduction-to-convex-optimization">Introduction to Convex Optimization</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/michaelweylandt/STA9890/edit/main/archive/AY-2024-SPRING/notes/notes02.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/michaelweylandt/STA9890/blob/main/archive/AY-2024-SPRING/notes/notes02.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/michaelweylandt/STA9890/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">STA 9890 - Regression I: Review of Linear Algebra, Convex Optimization, OLS</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Michael Weylandt </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p><span class="math display">\[\newcommand{\E}{\mathbb{E}} \newcommand{\R}{\mathbb{R}} \newcommand{\bx}{\mathbf{x}}\newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bX}{\mathbf{X}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}}\]</span></p>
<section id="linear-algebra-review" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra-review">Linear Algebra Review</h2>
<p>In mathematics, a <em>vector</em> - random or otherwise - is a fixed-length <em>ordered</em> collection of numbers. When we want to be precise about the size of a vector, we often call it a “tuple”, <em>e.g.</em>, a length-three vector is a “triple”, a length-four vector is a “4-tuple”, a length-five vector is a “5-tuple” <em>etc.</em>.</p>
<p>So, these are all vectors:</p>
<ul>
<li><span class="math inline">\((3, 4)\)</span></li>
<li><span class="math inline">\((1, 1, 1)\)</span></li>
<li><span class="math inline">\((1, 5, 6, 10)\)</span></li>
</ul>
<p>When we want to talk about the set of vectors of a given size, we use the <em>Cartesian product</em> of sets. For two sets, <span class="math inline">\(A, B\)</span>, the product set <span class="math inline">\(A \times B\)</span> is the set of all pairs, with the first element from <span class="math inline">\(A\)</span> and the second from <span class="math inline">\(B\)</span>. In mathematical notation,</p>
<p><span class="math display">\[A \times B = \left\{(a, b): a \in A, b \in B\right\} \]</span></p>
<p>This <em>set-builder</em> notation is read as follows: “The Cartesian Product of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is the set of all pairs <span class="math inline">\((a, b)\)</span> such that <span class="math inline">\(a\)</span> is in <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> is in <span class="math inline">\(B\)</span>.”</p>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are the same set, we define a Cartesian <em>power</em> as follows:</p>
<p><span class="math display">\[A^2 = A \times A = \left\{(a_1, a_2): a_1 \in A, a_2 \in A\right\}\]</span></p>
<p>Note that even though the sets <span class="math inline">\(A\)</span> and <span class="math inline">\(A\)</span> in this product are the same, the elements in each pair may vary. For example, if <span class="math inline">\(A = \{1, 2, 3\}\)</span>, we have</p>
<p><span class="math display">\[A^2 = \left\{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3,2), (3, 3)\right\}\]</span></p>
<p>Note that vectors are <em>ordered</em> pairs so <span class="math inline">\((2, 1) \neq (1, 2)\)</span>. From here, it should be pretty easy to convince yourself that set sizes play nicely with Cartesian products:</p>
<ul>
<li><span class="math inline">\(|A \times B| = |A| |B|\)</span></li>
<li><span class="math inline">\(|A^k| = |A|^k\)</span></li>
</ul>
<p>The most common set of vectors we use are those where each element is an arbitrary real number. The set of vectors of length <span class="math inline">\(n\)</span> (<span class="math inline">\(n\)</span>-tuples) is thus <span class="math inline">\(\R^n\)</span>. We rarely mix vectors of different lengths, so we don’t really have a name or notation for the “combo pack” <span class="math inline">\(\R^2 \cup \R^3 \cup \R^4\)</span>.</p>
<p>Conventionally, vectors are written in bold (if on a computer) or with a little arrow on top (hand written): so a vector called “x” would be denoted <span class="math inline">\(\mathbf{x}\)</span> or <span class="math inline">\(\vec{x}\)</span>. The elements of <span class="math inline">\(\bx\)</span> are denoted by subscripts <span class="math inline">\(\bx = (x_1, x_2, \dots, x_n)\)</span>.</p>
<section id="vector-arithmetic" class="level3">
<h3 class="anchored" data-anchor-id="vector-arithmetic">Vector Arithmetic</h3>
<p>We have three arithmetic operations we can perform on general vectors. The simplest is <em>scalar multiplication</em>. A <em>scalar</em> is a non-vector number, <em>i.e.</em>, a ‘regular’ number. Scalar multiplication consists of applying the scalar independently to each element of a vector.</p>
<p><span class="math display">\[\alpha \bx = \alpha(x_1, x_2, \dots, x_n) = (\alpha x_1, \alpha x_2, \dots, \alpha x_n)\]</span></p>
<p>For example, if <span class="math inline">\(\bx = (3, 4)\)</span> and <span class="math inline">\(\alpha = 2\)</span>, we have <span class="math display">\[\alpha \bx = (6, 8)\]</span></p>
<p>Note that the output of scalar multiplication is always a vector of the same length as the input.</p>
<p>We also have the ability to add vectors. This again is performed element-wise.</p>
<p><span class="math display">\[\bx + \by = (x_1, \dots, x_n) + (y_1, \dots, y_n) = (x_1 + y_1, \dots, x_n + y_n) \]</span></p>
<p>Note that we can’t add vectors of different lengths (recall our “no mixing” rule) and the output length is always the same as the input lengths.</p>
<p>Finally, we have the vector <em>inner product</em>, defined as:</p>
<p><span class="math display">\[\langle \bx, \by \rangle = x_1y_1 + x_2y_2 + \dots + x_ny_n \]</span></p>
<p>You might have seen this previously as the “dot” product. The inner product takes two length-<span class="math inline">\(n\)</span> vectors and gives back a scalar. This structure might seem a bit funny, but as we’ll see below, it’s actually quite useful.</p>
<p>You might ask if there’s a “vector-out” product: there is one, with the fancy name “Hadamard product”, but it doesn’t play nicely with other tools, so we don’t use it very much.</p>
<p>These tools play nicely together:</p>
<ul>
<li><span class="math inline">\(\alpha(\bx + \by) = \alpha \bx + \alpha \by\)</span> (Distributive)</li>
<li><span class="math inline">\(\langle \alpha \bx, \by \rangle = \alpha \langle \bx, \by \rangle\)</span> (Associative)</li>
<li><span class="math inline">\(\langle \bx, \by \rangle = \langle \by, \bx \rangle\)</span> (Commutative)</li>
</ul>
</section>
<section id="vector-length-and-angle" class="level3">
<h3 class="anchored" data-anchor-id="vector-length-and-angle">Vector Length and Angle</h3>
<p>We sometimes want to think about the “size” of a vector, analogous to the absolute value of a scalar. In scalar-world, we say “drop the sign” but there’s not an obvious analogue to a sign for a vector. For instance, if <span class="math inline">\(\bx = (3, -4)\)</span> is <span class="math inline">\(\bx\)</span> “positive”, “negative” or somewhere in beetween?</p>
<p>We note a trick from scalar-land: <span class="math inline">\(|x| = \sqrt{x^2}\)</span>. We can use the same idea for vectors:</p>
<p><span class="math display">\[ \|\bx\| = \sqrt{\langle \bx, \bx\rangle} = \sqrt{\sum_{i=1}^n x_i^2}\]</span></p>
<p>This quantity, <span class="math inline">\(\|\bx\|\)</span>, is called the <em>norm</em> or <em>length</em> of a vector. We use the double bars to distinguish it from the absolute value of a scalar, but it’s fundamentally the same idea.</p>
<p>In <span class="math inline">\(\R^2\)</span>, we recognize this formula for length as the Pythagorean theorem:</p>
<p><span class="math display">\[ \|(3, 4)\| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5 \]</span></p>
<p>We also sometimes want to define the <em>angle</em> between two vectors. We can define this as:</p>
<p><span class="math display">\[ \cos \angle(\bx, \by) = \frac{\langle \bx, \by\rangle}{\|\bx\|\|\by\|} \Leftrightarrow \angle(\bx, \by) = \cos^{-1}\left(\frac{\langle \bx, \by\rangle}{\|\bx\|\|\by\|}\right)\]</span></p>
<p>We won’t use this formula too often for implementation, but it’s good to have it for intuition. In particular, we note that angle is a proxy for sample correlation, justifying the common vernacular of “orthogonal”, meaning “at right angles”, for “uncorrelated” or “unrelated.”</p>
</section>
<section id="matrices" class="level3">
<h3 class="anchored" data-anchor-id="matrices">Matrices</h3>
<p>An <span class="math inline">\(n\)</span>-by-<span class="math inline">\(p\)</span> array of numbers is called a <em>matrix</em>; here the first dimension is the number of rows while the second is the number of columns. So <span class="math display">\[\bA = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{pmatrix}\]</span> is a 3-by-2 matrix. We denote the set of matrices of a given size as <span class="math inline">\(\R^{n \times p}\)</span>, extending slightly the notation we use for vectors.</p>
<p>In this course, we will use matrices for two closely-related reasons:</p>
<ol type="1">
<li>To organize our data</li>
<li>To specify and manipulate linear models</li>
</ol>
<p>Specifically, if we have <span class="math inline">\(n\)</span> training samples, each of <span class="math inline">\(p\)</span> covariates (predictors), we will arrange them in a matrix traditionally called <span class="math inline">\(\bX \in \R^{n \times p}\)</span>. Here, each <em>row</em> of <span class="math inline">\(\bX\)</span> corresponds to an observation. Statisticians tend to call this matrix a <em>design</em> matrix because (historically) it was something designed as part of an experiment; the name got carried forward into the observational (un-designed) setting. You may also hear it called the ‘data matrix’.</p>
<p>Suppose we have a design matrix <span class="math inline">\(\bX \in \R^{n \times p}\)</span> and a vector of regression coefficients <span class="math inline">\(\mathbf{\beta} \in \R^p\)</span>. We can use <em>matrix</em> multiplication to make predictions about all observations simultaneously.</p>
<p>Specifically, recall that the standard (multivariate) linear model looks like:</p>
<p><span class="math display">\[\hat{y} = \sum_{i=1}^p x_i\beta_i\]</span></p>
<p>For a single observation, this can be written in vector notation as</p>
<p><span class="math display">\[\hat{y} = \bx^{\top}\bbeta\]</span></p>
<p>If we have <span class="math inline">\(n\)</span> observations, we can stack them in a vector as:</p>
<p><span class="math display">\[\begin{pmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \vdots \\ \hat{y}_n \end{pmatrix} =
\begin{pmatrix} \bx_1^{\top}\bbeta \\ \bx_2^{\top}\bbeta \\ \vdots \\ \\bx_n^{\top}\bbeta \end{pmatrix}\]</span></p>
<p>We can connect this with our design matrix <span class="math inline">\(\bX\)</span> by using the above as a definition of matrix-vector multiplication:</p>
<p><span class="math display">\[\hat{\by} = \bX\bbeta =  
\begin{pmatrix} \bx_1^{\top}\bbeta \\ \bx_2^{\top}\bbeta \\ \vdots \\ \\bx_n^{\top}\bbeta \end{pmatrix}\]</span></p>
<p>Here, matrix multiplication proceeds by taking the inner product of each <em>row</em> with the (column) vector <span class="math inline">\(\bbeta\)</span>. This may feel a bit unnatural at first, but with a bit of practice, it will become second nature. Note that we don’t need a transpose on <span class="math inline">\(\bX\)</span>: the multiplication ‘auto-transposes’ in some sense.</p>
<p>It is always helpful to keep track of dimensions when doing matrix multiplication: checking that matrices and vectors have the right size is a useful way to make sure you haven’t done anything <em>too wrong</em>. In general, we can only multiply a <span class="math inline">\(m\)</span>-by-<span class="math inline">\(n\)</span> matrix with a <span class="math inline">\(n\)</span>-by-<span class="math inline">\(p\)</span> matrix and the result is a <span class="math inline">\(m\)</span>-by-<span class="math inline">\(p\)</span> matrix (the <span class="math inline">\(n\)</span>-dimension gets reduced to a scalar by the inner product). Formally, we have something like</p>
<p><span class="math display">\[ \R^{m \times n} \times \R^{n \times p} \to \R^{m \times p}\]</span></p>
<p>For purposes of this, you can always think of an <span class="math inline">\(n\)</span>-vector as a <span class="math inline">\(n\)</span>-by-1 “column” matrix, giving us:</p>
<p><span class="math display">\[\underbrace{\bX}_{\R^{n \times p}} \underbrace{\bbeta}_{\R^p} = \underbrace{\hat{\by}}_{\R^{n \times 1}}\]</span></p>
</section>
<section id="spectral-properties-of-symmetric-matrices" class="level3">
<h3 class="anchored" data-anchor-id="spectral-properties-of-symmetric-matrices">Spectral Properties of Symmetric Matrices</h3>
<p>An important class of matrices we will consider are <em>symmetric</em> matrices, which are just what the name sounds like. These come up in several key places in statistics, none more important than the <em>covariance</em> matrix, typically denoted <span class="math inline">\(\Sigma\)</span>. Recall that the covariance operator <span class="math inline">\(\mathbb{C}\)</span> is symmetric (<span class="math inline">\(\mathbb{C}[X, Y] = \mathbb{C}[Y, X]\)</span>) so the covariance matrix of a random vector turns out to be symmetric as well.</p>
<p>Another common source of symmetric matrices is when a matrix is multiplied by its transpose: you should convince yourself that <span class="math inline">\(\bX^{\top}\bX \in \R^{p \times p}\)</span> is a symmetric matrix.</p>
</section>
</section>
<section id="ordinary-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-least-squares">Ordinary Least Squares</h2>
<p>Suppose we want to fit a linear model to a given data set (for any of the reasons we discuss in more detail below): how can we choose which line to fit? (There are infinitely many!)</p>
<p>Since our goal is minimizing test error, and we hope training error is at least a somewhat helpful proxy for training error, we can pick the line that <em>minimizes training error</em>. To do this, we need to commit to a specific measure of error. As the name <em>Ordinary Least Squares</em> suggests, OLS uses (mean) squared error as its target.</p>
<p>Why is MSE the right choice here? It turns out that MSE is very nice computationally, but the reason is actually more fundamental: given a random variable <span class="math inline">\(Z\)</span>, suppose we want to minimize the <span class="math inline">\((Z - \mu)\)</span> for some <span class="math inline">\(\mu\)</span>: it can be shown that</p>
<p><span class="math display">\[ \E[Z] = \text{argmin}_{\mu \in \R} \E[(Z - \mu)^2]\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Pause to Reflect">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pause to Reflect
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Prove this for yourself.</p>
<p><em>Hint</em>: Differentiate with respect to <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
</div>
<p>That is, the quantity that minimizes the MSE is the mean. So when we fit a line to some data by OLS, we are implicitly trying to fit to <span class="math inline">\(\E[y]\)</span> - a very reasonable thing to do!</p>
<p>Specifically, given training data <span class="math inline">\(\mathcal{D} = \{(\bx_i, y_i)\}_{i=1}^n\)</span> where each <span class="math inline">\(\bx_i \in \R^p\)</span>, OLS finds <span class="math inline">\(\bbeta \in \R^p\)</span> such that <span class="math display">\[ \hat{\bbeta} = \text{argmin}_{\bbeta \in \R^p} \frac{1}{n}\sum_{i=1}^n \left(y_i - \sum_{j=1}^p \beta_jx_j\right)^2 = \text{argmin}_{\bbeta \in \R^p} \frac{1}{n}\sum_{i=1}^n \left(y_i - \bx^{\top}\bbeta\right)^2\]</span></p>
<p>Some possibly new notation here:</p>
<ul>
<li><p><span class="math inline">\(\bx^{\top}\bbeta\)</span> is the (inner) product of two vectors: defined as the sum of their elementwise products.</p></li>
<li><p>Optimization problems:</p>
<ul>
<li><span class="math display">\[\hat{x} = \text{argmin}_{x \in \mathcal{C}} f(x)\]</span></li>
<li><span class="math display">\[f_* = \text{min}_{x \in \mathcal{C}} f(x)\]</span></li>
</ul>
<p>These problems say: find the value of <span class="math inline">\(x\)</span> in the set <span class="math inline">\(\mathcal{C}\)</span> that minimizes the function <span class="math inline">\(f\)</span>. <span class="math inline">\(\text{argmin}\)</span> says ‘give me the minimizer’ while <span class="math inline">\(\min\)</span> says give me the minimum value’. These are related by <span class="math inline">\(f_* = f(\hat{x})\)</span></p>
<p>The function <span class="math inline">\(f\)</span> is called the <em>objective</em>; the set <span class="math inline">\(\mathcal{C}\)</span> is called the <em>constraint set</em>.</p></li>
</ul>
<p><em>Ordinary Least Squares</em> refers to the use of an MSE objective without any additional constraints.</p>
<p>Note the general structure of our approach here:</p>
<ul>
<li>Define the loss we care about</li>
<li>Set up an optimization problem to minimize loss on the training set</li>
<li>Solve optimization problem</li>
</ul>
<p>ML folk call this <em>empirical risk minimization</em> (ERM) since we’re minimizing the risk (average loss) on the data we can see (the training data). Statisticians call this <span class="math inline">\(M\)</span>-estimation, since it defines an estimator by <strong>M</strong>inimization of a measure of ‘fit’. Whatever you call it, it’s a very useful ‘meta-method’ for coming up with ML methods.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Pause to Reflect">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pause to Reflect
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>How does this compare with <em>Maximum Likelihood Estimation</em>?</p></li>
<li><p>We set up this ERM method using <em>mean squared error</em> - what happens with other errors? Specifically, formulate this ERM for</p>
<ul>
<li><em>mean absolute error</em></li>
<li><em>mean percent error</em></li>
</ul>
<p>and compare to OLS.</p></li>
</ol>
</div>
</div>
</div>
<p>So far we’ve set up OLS as <span class="math display">\[ \hat{\bbeta} = \text{argmin}_{\bbeta \in \R^p} \frac{1}{n}\sum_{i=1}^n \left(y_i - \bx^{\top}\bbeta\right)^2\]</span></p>
<p>We can clean this up to make additional analysis easier:</p>
<ul>
<li>Let <span class="math display">\[\by = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}\]</span> be the (vertically stacked) vector of responses.</li>
<li>Next look at our predictions: <span class="math display">\[\hat{\by} = \begin{pmatrix} \bx_1^{\top}\bbeta \\ \bx_2^{\top}\bbeta \\ \vdots \\ \bx_n^{\top}\bbeta \end{pmatrix} = \begin{pmatrix} \bx_1^{\top} \\ \bx_2^{\top} \\ \vdots \\ \bx_n^{\top} \end{pmatrix}\bbeta = \bX\bbeta\]</span></li>
</ul>
<p>Hence, OLS is just <span class="math display">\[\hat{\bbeta} = \text{argmin}_{\bbeta \in \R^p} \frac{1}{n} \|\by - \bX\bbeta\|_2^2\]</span> Here <span class="math inline">\(\|\cdot\|_2^2\)</span> is the (squared Euclidean or <span class="math inline">\(L_2\)</span>) norm of a vector: defined by <span class="math inline">\(\|\bz\|_2^2 = \sum z_i^2\)</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Pause to Reflect">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pause to Reflect
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Do we actually need the <span class="math inline">\(1/n\)</span> part?</p></li>
<li><p>Why is this called linear?</p></li>
</ol>
</div>
</div>
</div>
<p>We’ve formulated OLS as <span class="math display">\[\hat{\beta} = \text{argmin}_{\bbeta \in \R^p} \frac{1}{n} \|\by - \bX\bbeta\|_2^2.\]</span> In order to solve this, it will be useful to modify it slightly to <span class="math display">\[\hat{\beta} = \text{argmin}_{\bbeta \in \R^p} \frac{1}{2} \|\by - \bX\bbeta\|_2^2\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Pause to Reflect">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pause to Reflect
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Why is this ok to do?</p>
</div>
</div>
</div>
<p>You will show in <a href="../reports/repot01.html">Report #01</a> that the solution is given by</p>
<p><span class="math display">\[\hat{\beta} = (\bX^{\top}\bX)^{-1}\bX^{\top}\by\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Pause to Reflect">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pause to Reflect
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Prove this for yourself. What conditions are required on <span class="math inline">\(\bX\)</span> for the inverse in the above expression to exist?</p>
</div>
</div>
</div>
<p>With this estimate, our in-sample predictions are given by:</p>
<p><span class="math display">\[\hat{\by} = \bX\hat{\bbeta} = \bX(\bX^{\top}\bX)^{-1}\bX^{\top}\by \]</span></p>
<p>This says that our predictions are, in some sense, just a linear function of the original data <span class="math inline">\(\by\)</span>. The matrix</p>
<p><span class="math display">\[\bX(\bX^{\top}\bX)^{-1}\bX^{\top} = \bH\]</span></p>
<p>is sometimes called the ‘hat’ matrix because it puts a hat on <span class="math inline">\(\by\)</span> (<span class="math inline">\(\hat{\by}=\bH\by\)</span>).</p>
<p><span class="math inline">\(\bH\)</span> can be shown to be a special type of matrix called a <em>projector</em>, meaning that it has eigenvalues all 0 or 1. For the hat matrix specifically, we can show that eigenvalues of <span class="math inline">\(\bH\)</span> are <span class="math inline">\(p\)</span> zeros and <span class="math inline">\(n - p\)</span> ones.</p>
<p>This in turn implies that it is <em>idempotent</em>, meaning <span class="math inline">\(\bH^2 = \bH\bH = \bH\)</span>. (To show this, simply express <span class="math inline">\(\bH\)</span> in terms of its eigendecomposition.) We can use this property to finally justify the in-sample MSE of OLS we have cited several times.</p>
<p>The in-sample MSE is given by:</p>
<p><span class="math display">\[\begin{align*}
\text{MSE} &amp;= \frac{1}{n}\|\by - \hat{\by}\|_2^2 \\
           &amp;= \frac{1}{n}\left\|\by - \bH\by\right\|_2^2 \\
           &amp;= \frac{1}{n}(\by - \bH\by)^{\top}(\by - \bH\by) \\
           &amp;= \frac{1}{n}(\by^{\top} - \by^{\top}\bH)(\by - \bH\by) \\
           &amp;= \frac{\by^{\top}\by - \by^{\top}\bH\by - \by^{\top}\bH\by + \by^{\top}\bH\bH\by}{n} \\
           &amp;= \frac{\by^{\top}\by - \by^{\top}\bH\by}{n} \\
           &amp;= \frac{\by^{\top}(\bI - \bH)\by}{n} \\
\implies \E[\text{MSE}] &amp;= \E\left[\frac{\by^{\top}(\bI - \bH)\by}{n}\right] \\
&amp;= \frac{1}{n}\E\left[\by^{\top}(\bI - \bH)\by\right]
\end{align*}\]</span></p>
<p>To finish this, we need to know that the <a href="https://en.wikipedia.org/wiki/Quadratic_form_(statistics)">expectation of a symmetric quadratic form</a> satisfies</p>
<p><span class="math display">\[\bx \sim (\mu, \Sigma) \implies \E[\bx^{\top}\bA\bx] = \text{Tr}(\bA\Sigma) + \mu^{\top}\bA\mu\]</span></p>
<p>for any random vector <span class="math inline">\(\bx\)</span> with mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>.</p>
<p>To apply this above, we note <span class="math inline">\(\by \sim (\bX\beta_*, \sigma^2 \bI)\)</span>, so we get</p>
<p><span class="math display">\[\begin{align*}
\E\left[\by^{\top}(\bI - \bH)\by\right] &amp;= \text{Tr}((\bI - \bH) \sigma^2 \bI) + (\bX\bbeta_*)^{\top}(\bI - \bH)(\bX\bbeta_*) \\
&amp;= \sigma^2 \text{Tr}(\bI - \bH) + \bbeta_*^{\top}\bX^{\top}(\bI - \bH)\bX\bbeta_* \\
&amp;= \sigma^2 \text{Tr}(\bI - \bH) + \bbeta_*^{\top}(\bX^{\top}\bX - \bX^{\top}\bH\bX)\bbeta_* \\
&amp;= \sigma^2 \text{Tr}(\bI - \bH) + \bbeta_*^{\top}(\bX^{\top}\bX - \bX^{\top}\bX(\bX^{\top}\bX)^{-1}\bX^{\top}\bX)\bbeta_* \\
&amp;= \sigma^2 \text{Tr}(\bI - \bH) + \bbeta_*^{\top}(\bX^{\top}\bX - \bX^{\top}\bX)\bbeta_* \\
&amp;= \sigma^2 \text{Tr}(\bI - \bH) + \bbeta_*^{\top} \mathbf{0}\bbeta_* \\
&amp;= \sigma^2 \text{Tr}(\bI - \bH) \\
&amp;= \sigma^2 (\text{Tr}(\bI) - \text{Tr}(\bH))
\end{align*}\]</span></p>
<p>Recall that the trace is simply the sum of the eigenvalues, so this last term becomes <span class="math inline">\(\sigma^2(n - p)\)</span>, finally giving us:</p>
<p><span class="math display">\[\E[\text{MSE}] = \frac{\sigma^2(n-p)}{n}  = \sigma^2\left(1 - \frac{p}{n}\right)\]</span></p>
<p>Whew! That was a lot of work! But can you imagine how much more work this would have been without all of these matrix tools?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Pause to Reflect">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pause to Reflect
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Make sure you can justify every step in the derivation above. This is a particularly long computation, but we will use the individual steps many more times in this course.</p>
</div>
</div>
</div>
</section>
<section id="bias-variance-decomposition" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-decomposition">Bias-Variance Decomposition</h2>
<p>In <a href="../reports/repot01.html">Report #01</a>, you will show that, under MSE loss, our expected test error can be decomposed as</p>
<p><span class="math display">\[\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}\]</span></p>
<p>Let’s show how we can analyze these quantities for a KNN <em>regression</em> problem. Here, we’re using the ‘regression’ version of KNN since it plays nicely with MSE.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Don't attach the package to avoid weird aliasing issues</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>knn.reg <span class="ot">&lt;-</span> FNN<span class="sc">::</span>knn.reg </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">args</span>(knn.reg)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>function (train, test = NULL, y, k = 3, algorithm = c("kd_tree", 
    "cover_tree", "brute")) 
NULL</code></pre>
</div>
</div>
<p>We also need a ‘true’ function which we’re trying to estimate. Let’s use the following model:</p>
<p><span class="math display">\[\begin{align*}
X &amp;\sim \mathcal{U}([0, 1]) \\
Y &amp;\sim \mathcal{N}(4\sqrt{X} + 0.5 * \sin(4\pi * X), 0.25)
\end{align*}\]</span></p>
<p>That is, <span class="math inline">\(X\)</span> is uniform on the unit interval and <span class="math inline">\(Y\)</span> is a non-linear function of <span class="math inline">\(X\)</span> plus some Gaussian noise.</p>
<p>First let’s plot <span class="math inline">\(X\)</span> vs <span class="math inline">\(\E[X]\)</span> - under MSE loss this is our ‘best’ (Bayes-optimal) possible guess.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>yfun <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="dv">4</span> <span class="sc">*</span> <span class="fu">sqrt</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">sinpi</span>(<span class="dv">4</span> <span class="sc">*</span> x)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>y_mean <span class="ot">&lt;-</span> <span class="fu">yfun</span>(x)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y_mean, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"E[X]"</span>, </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">"True Regression Function"</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To generate training data from this model, we simply implement the PRNG components:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># 25 training points</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(x_train), <span class="at">sd=</span><span class="fl">0.5</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y_mean, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"E[X]"</span>, </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We have some variance of course, but we can still “squint” to get the right shape of our function. Let’s see how KNN looks on this data.</p>
<p>We start with <span class="math inline">\(K=1\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(x_train, <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>plot_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>Y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>plot_grid)<span class="sc">$</span>pred</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y_mean, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"E[X]"</span>, </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x_train, y_train)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(plot_grid, Y_hat, <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This is not a great fit - what happens if we repeat this process may times?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y_mean, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"E[X]"</span>, </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">20</span>)){</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    test_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    Y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>test_grid)<span class="sc">$</span>pred</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(test_grid, Y_hat, <span class="at">col=</span><span class="st">"#FFAA0099"</span>, <span class="at">lwd=</span><span class="fl">0.5</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Clearly we have some variance!</p>
<p>If we repeat with a higher value of <span class="math inline">\(K\)</span>, we see far less variance:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y_mean, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"E[X]"</span>, </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">20</span>)){</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    plot_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    Y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">10</span>, <span class="at">test=</span>test_grid)<span class="sc">$</span>pred</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(test_grid, Y_hat, <span class="at">col=</span><span class="st">"#FFAA0099"</span>, <span class="at">lwd=</span><span class="fl">0.5</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>How well does KNN do <em>on average</em>?</p>
<p>That is, if we could repeat this process (infinitely) many times, how well would it recover the true regression function? Let’s try <span class="math inline">\(K=1\)</span> and <span class="math inline">\(K=10\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y_mean, <span class="at">type=</span><span class="st">"l"</span>, </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"X"</span>, <span class="at">ylab=</span><span class="st">"E[X]"</span>, </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>KNN_AVERAGE_PRED_K1 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(<span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    plot_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>test_grid)<span class="sc">$</span>pred</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(test_grid, KNN_AVERAGE_PRED_K1, <span class="at">col=</span><span class="st">"red4"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>KNN_AVERAGE_PRED_K10 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(<span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    plot_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">10</span>, <span class="at">test=</span>test_grid)<span class="sc">$</span>pred</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(test_grid, KNN_AVERAGE_PRED_K10, <span class="at">col=</span><span class="st">"blue4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We see here that, <em>on average</em>, KNN with <span class="math inline">\(K=1\)</span> (red) basically gets the function just right - no bias!</p>
<p>On the other hand, because KNN with <span class="math inline">\(K=10\)</span> smooths out the function, we see systematic errors (here oversmoothing). That’s some bias.</p>
<p>So which is better? - <span class="math inline">\(K=1\)</span> - High variance, but low bias - <span class="math inline">\(K=10\)</span> - Low variance, but high bias</p>
<p>We’ll have to look at some test error to see. For now, we’ll generate our test data exactly the same way as we generate our training data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>KNN_K1_ERROR <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    plot_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate from same model as before</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    y_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>X_test)<span class="sc">$</span>pred</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    err <span class="ot">=</span> y_test <span class="sc">-</span> y_hat</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>KNN_K1_MSE <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">rowMeans</span>(KNN_K1_ERROR<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>KNN_K10_ERROR <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    plot_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate from same model as before</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    y_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">10</span>, <span class="at">test=</span>X_test)<span class="sc">$</span>pred</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    err <span class="ot">=</span> y_test <span class="sc">-</span> y_hat</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>KNN_K10_MSE <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">rowMeans</span>(KNN_K10_ERROR<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(<span class="at">K1_MSE =</span> KNN_K1_MSE, </span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>      <span class="at">K10_MSE =</span> KNN_K10_MSE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       K1_MSE K10_MSE
[1,] 1.954541 1.46504</code></pre>
</div>
</div>
<p><span class="math inline">\(K=10\)</span> does better overall!</p>
<p>But does it do better <em>everywhere</em> or are some parts of the problem better for <span class="math inline">\(K=1\)</span>?</p>
<p>Now we’ll be systematic in our test data - spacing it equally on the grid and computing ‘pointwise’ MSE:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>KNN_K1_ERROR <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate from same model as before</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    test_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(test_grid, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    y_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(test_grid, <span class="at">mean=</span><span class="fu">yfun</span>(X_test), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>X_test)<span class="sc">$</span>pred</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    err <span class="ot">=</span> y_test <span class="sc">-</span> y_hat</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>KNN_K1_MSE <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(KNN_K1_ERROR<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>KNN_K10_ERROR <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    test_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(test_grid, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>) </span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    y_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">101</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_test), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">10</span>, <span class="at">test=</span>X_test)<span class="sc">$</span>pred</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    err <span class="ot">=</span> y_test <span class="sc">-</span> y_hat</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>KNN_K10_MSE <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(KNN_K10_ERROR<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(KNN_K1_MSE, <span class="at">col=</span><span class="st">"blue4"</span>, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(KNN_K10_MSE, <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">pch=</span><span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>It looks like - for this set up at least - <span class="math inline">\(K=10\)</span> is better <em>everywhere</em> but that’s not always the case.</p>
<p>Play around with the size of the training data, the noise in the samples, and the data generating function (<code>yfun</code>) to see if you can get different behavior.</p>
<p>Also - is <span class="math inline">\(K=10\)</span> really the optimal choice here? What would happen if we changed <span class="math inline">\(n\)</span>?</p>
<p>So, now that we have a good sense of (average) test error, can we verify our MSE decomposition?</p>
<p>Recall <span class="math display">\[\begin{align*}
\E[\text{MSE}] &amp;= \text{Bias}^2 + \text{Variance} + \text{Irreducible Error} \\
\text{ where } \text{Bias}^2 &amp;= \E\left[\left(\E[\hat{y}] - \E[y]\right)^2\right] \\
&amp;= \left(\E[\hat{y}] - \E[y]\right)^2 \text{ (Why can I drop the outer expectation?)} \\
\text{Variance} &amp;= \E\left[\left(\hat{y} - \E[\hat{y}]\right)^2\right] \\
\text{Irreducible Error} &amp;= \E\left[\left(y - \E[y]\right)^2\right]
\end{align*}\]</span></p>
<p>(Make sure you understand these definitions and how they work together!)</p>
<p>Let’s work these out using all the tools we built before.</p>
<p>First, for the bias: - we already have <span class="math inline">\(\E[y]\)</span> - this is just the <code>yfun</code> we selected - we can compute <span class="math inline">\(\E[\hat{y}]\)</span> by running KNN many times and averaging the result</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>sample_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>KNN_AVERAGE_PRED_K1 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(<span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>sample_grid)<span class="sc">$</span>pred</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>KNN_BIAS_K1 <span class="ot">&lt;-</span> KNN_AVERAGE_PRED_K1 <span class="sc">-</span> <span class="fu">yfun</span>(sample_grid)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(sample_grid, KNN_BIAS_K1<span class="sc">^</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">"red4"</span>, </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"l"</span>, <span class="at">main=</span><span class="st">"Squared Bias of KNN with K=1"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Not too much bias, but things do go a bit off the rails near the end points.</p>
<p>Next, we can compute variance pointwise:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sample_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>KNN_VARIANCE_K1 <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(<span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    X_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(<span class="dv">25</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    y_train <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">25</span>, <span class="at">mean=</span><span class="fu">yfun</span>(X_train), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    (<span class="fu">knn.reg</span>(<span class="at">train=</span>X_train, <span class="at">y=</span>y_train, <span class="at">k=</span><span class="dv">1</span>, <span class="at">test=</span>sample_grid)<span class="sc">$</span>pred <span class="sc">-</span> KNN_AVERAGE_PRED_K1)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(sample_grid, KNN_VARIANCE_K1, <span class="at">col=</span><span class="st">"red4"</span>, </span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">"l"</span>, <span class="at">main=</span><span class="st">"Variance of KNN with K=1"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>For this data at least, the variance term is generally larger than the bias term: this is what we expect with a very flexible (high variance + low bias) model like <span class="math inline">\(1\)</span>-NN.</p>
<p>Finally, irreducible error is just 0.25 everywhere (recall <span class="math inline">\(y \sim \mathcal{N}(\E[y], 0.25)\)</span>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>KNN_IE <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(<span class="fu">replicate</span>(<span class="dv">500</span>, {</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    sample_grid <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out=</span><span class="dv">101</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    X_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(sample_grid, <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    y_test <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(sample_grid, <span class="at">mean=</span><span class="fu">yfun</span>(X_test), <span class="at">sd=</span><span class="fl">0.5</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    y_best_pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">yfun</span>(X_test), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    (<span class="fu">as.vector</span>(y_best_pred <span class="sc">-</span> y_test))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(KNN_IE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Put these together and we see the decomposition in action:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(KNN_BIAS_K1)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">mean</span>(KNN_VARIANCE_K1) <span class="sc">+</span> <span class="fu">mean</span>(KNN_IE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5290861</code></pre>
</div>
</div>
<p>as compared to</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(KNN_K1_MSE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5242858</code></pre>
</div>
</div>
<p>Visually,</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>DECOMP_DATA <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">sample_grid =</span> sample_grid, </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">KNN_K1_BIAS2 =</span> KNN_BIAS_K1<span class="sc">^</span><span class="dv">2</span>, </span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">KNN_K1_VARIANCE=</span>KNN_VARIANCE_K1, </span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>   <span class="at">KNN_K1_IE =</span> KNN_IE, </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>   <span class="at">KNN_K1_MSE =</span> KNN_K1_MSE) <span class="sc">|&gt;</span> </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>   <span class="fu">pivot_longer</span>(<span class="sc">-</span>sample_grid) <span class="sc">|&gt;</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mutate</span>(<span class="at">Error=</span>value,</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>          <span class="at">Type=</span><span class="fu">case_when</span>(</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>            name<span class="sc">==</span><span class="st">"KNN_K1_BIAS2"</span> <span class="sc">~</span> <span class="st">"Bias^2"</span>, </span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>            name<span class="sc">==</span><span class="st">"KNN_K1_IE"</span> <span class="sc">~</span> <span class="st">"Irreducible Error"</span>, </span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>            name<span class="sc">==</span><span class="st">"KNN_K1_VARIANCE"</span> <span class="sc">~</span> <span class="st">"Variance"</span>, </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            name<span class="sc">==</span><span class="st">"KNN_K1_MSE"</span> <span class="sc">~</span> <span class="st">"Total Error"</span>)) </span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">data=</span>DECOMP_DATA <span class="sc">|&gt;</span> <span class="fu">filter</span>(Type <span class="sc">!=</span> <span class="st">"Total Error"</span>), </span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>           <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>sample_grid, <span class="at">y=</span>Error, <span class="at">color=</span>Type), </span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>           <span class="at">stat=</span><span class="st">"identity"</span>) <span class="sc">+</span> </span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>DECOMP_DATA <span class="sc">|&gt;</span> <span class="fu">filter</span>(Type <span class="sc">==</span> <span class="st">"Total Error"</span>), </span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>sample_grid, <span class="at">y=</span>Error), </span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            <span class="at">color=</span><span class="st">"red4"</span>, <span class="at">linewidth=</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"X"</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">"Test Error"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>So a bit of weirdness at the left end point - but holds up as well as we might expect for <span class="math inline">\(N=25\)</span> samples.</p>
<p>How do the relative magnitudes of these terms change as you adjust the parameters of the simulation?</p>
</section>
<section id="introduction-to-convex-optimization" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-convex-optimization">Introduction to Convex Optimization</h2>
<p>In this course, we will frequently want to find the minimizer of certain functions. Typically, these will arise as ERMs and the function will take a <span class="math inline">\(p\)</span>-vector of inputs, <em>e.g.</em> regression coefficients, and produce a scalar loss output. Let us generally call this function <span class="math inline">\(f: \R^p \to \R\)</span>.</p>
<p>In some circumstances, we can take the derivative of <span class="math inline">\(f\)</span>, typically called the <em>gradient</em> in this context, and set it equal to zero. For example, suppose we want to minimize an expression of the form:</p>
<p><span class="math display">\[f(\bx) = \frac{1}{2}\bx^{\top}\bA\bx + \bb^{\top}\bx + c\]</span></p>
<p>where <span class="math inline">\(\bA\)</span> is a <em>symmetric</em> strictly positive-definite matrix, <span class="math inline">\(\bb\)</span> is an arbitrary <span class="math inline">\(p\)</span>-vector, and <span class="math inline">\(c\)</span> is a constant. Taking the gradient, we find</p>
<p><span class="math display">\[ \frac{\partial f}{\partial \bx} = \bA\bx + \bb\]</span></p>
<p>We set this to zero and find a crticial point at:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{0} &amp;= \bA\bx + \bb \\
- \bb &amp;= \bA\bx \\
\implies \bx &amp;= -\bA^{-1}\bb
\end{align*}\]</span></p>
<p>assuming that <span class="math inline">\(\bA\)</span> is invertible. (Here, invertibility is implied by assuming <span class="math inline">\(\bA\)</span> is <strong>strictly</strong> positive-definite.) As usual, we are not quite done here, as we must also check that this is a <em>minimizer</em> and not a maximizer or a saddle point. To do so, we take the <em>second</em> derivative to find</p>
<p><span class="math display">\[ \frac{\partial}{\partial \bx}\frac{\partial f}{\partial \bx} = \bA\]</span></p>
<p>In this multivariate context, we need the second derivative, <em>a.k.a</em> the Hessian, to be <em>strictly positive-definite</em> to guarantee that we have found a minimizer and this is indeed exactly what we assumed above. As discussed above, ‘definiteness’ plays the role of sign for many applications of matrix-ness. Here, by assuming strictly positive definite, we are essentially treating the matrix as strictly positive (<span class="math inline">\(&gt;0\)</span>), which is exactly the condition we need to guarantee a minimizer in the scalar case as well.</p>
<p>Hence, we have that the one minimizer of <span class="math inline">\(f\)</span> is found at <span class="math inline">\(\bx_* = \bA^{-1}\bb\)</span>. Compare this to the scalar case of minimizing a quadratic <span class="math inline">\(\frac{1}{2}ax^2 + bx + c\)</span> with minimizer at <span class="math inline">\(x = -b/a\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>The above analysis works well, but it is essentially the only minimization we will be able to do in closed form in this course.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>For other functions, we will need to apply <em>optimization</em>: the mathematical toolkit for finding minimizers (or maximizers) of functions. Fortunately for us, many of the methods we begin this course with fall in the realm of <em>convex optimization</em>, a particularly nice branch of optimization.</p>
<p><em>Convex</em> Optimization refers to the problem of <em>minimizing</em> <em>convex</em> functions over <em>convex</em> sets. Let’s define both of these:</p>
<ul>
<li><p>A <em>convex</em> function is one which satisfies this inequality at all points: <span class="math display">\[f(\lambda \bx + (1-\lambda)\by) \leq \lambda f(\bx) + (1-\lambda) f(\by)\]</span></p>
<p>for any <span class="math inline">\(\bx, \by\)</span> and any <span class="math inline">\(\lambda \in [0, 1]\)</span>.</p>
<p>This definition is a bit non-intuitive, but it basically implies that we have a “bowl-like” function. This definition captures the idea that the actual function value is always less than we might get from linear interpolation. A picture is helpful here:</p></li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes02_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Here, we see that the actual value of the function (blue dot) is less than what we would get if we interpolated the two red points (red line).</p>
<p>An alternative definition is that if <span class="math inline">\(f\)</span> is twice-differentiable, its Hessian (2nd derivative matrix) is positive semi-definite.</p>
<ul>
<li><p>A <em>convex</em> set is one that allows us to look “between” points. Specifically, a set <span class="math inline">\(\mathcal{C} \subseteq \R^p\)</span> is convex if <span class="math display">\[\bx, \by \in \mathcal{C} \implies \lambda \bx + (1-\lambda)\by \in \mathcal{C}\]</span></p>
<p>for all <span class="math inline">\(\bx, \by \in \mathcal{C}\)</span> and any <span class="math inline">\(\lambda \in [0, 1]\)</span>.</p></li>
</ul>
<p>Clearly these are related by the idea of looking “between” two points:</p>
<ol type="i">
<li>Convex functions guarantee that this will produce something lower than naive interpolation; and</li>
<li>Convex sets guarantee that the midpoint will be “allowable”.</li>
</ol>
<p>To tie these together, note another alternative characterization of a <em>convex function</em>: one whose epigraph (the area above the curve in the plot) is a convex set.</p>
<p>For optimization purposes, these two properties imply a rather remarkable fact:</p>
<p>If <span class="math inline">\(\bx_0\)</span> is a <em>local</em> minimum of <span class="math display">\[\min_{\bx \in \mathcal{C}} f(\bx)\]</span>, then it is a <em>global</em> minimum.</p>
<p>This is quite shocking: if we find a point where we can’t improve by going in any direction, we are guaranteed to have found a global minimum and no point could be better. (It is possible to have <em>multiple</em> minimizers however: consider <span class="math inline">\(f(\bx) = 0\)</span>. Any choice of <span class="math inline">\(\bx\)</span> is <em>a</em> global minimizer.) This lets us turn the “global” search problem into a “local” one.</p>
<p>Of course, this only helps us <em>if</em> we can find a local minimizer of <span class="math inline">\(f\)</span>. How might we do so? Let’s recall a basic calculus idea: the gradient of a function is a vector that points in the direction of greatest increase (the “steepest” uphill). So if we go in the opposite direction of the gradient, we actually will go “downhill”. In fact, this is basically all we need to start applying <em>gradient descent</em>.</p>
<p><strong>Gradient Descent</strong>: Given a convex function <span class="math inline">\(f\)</span>:</p>
<ul>
<li>Initialize at (arbitrary) starting point <span class="math inline">\(\bx_0\)</span>.</li>
<li>Initialize step counter <span class="math inline">\(k=0\)</span>.</li>
<li>Repeat until convergence:
<ul>
<li>Compute the gradient of <span class="math inline">\(f\)</span> at <span class="math inline">\(\bx_k\)</span>: <span class="math inline">\(\left.\nabla f\right.|_{\bx=\bx_k}\)</span></li>
<li>Set <span class="math inline">\(\bx_{k+1} = \bx_k - c \left.\nabla f\right.|_{\bx=\bx_k}\)</span></li>
</ul></li>
</ul>
<p>Repeated infinitely many times, this will converge to a local, and hence global, minimizer of <span class="math inline">\(f\)</span>. There are <strong>many</strong> variants of this basic idea<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, mostly related to selecting the optimal step size <span class="math inline">\(c\)</span>, but this is the most important algorithm in convex optimization and it is important to understand it deeply.</p>
<p>We can apply it here to the 3D function <span class="math display">\[f(\bx) = (x_1-2)^2 + (x_2-3)^2 + (x_3 - 4)^2.\]</span> Clearly, we can see that the minimizer has to be at <span class="math inline">\((2, 3, 4)\)</span>, but our algorithm won’t use that fact.</p>
<p>Before we can implement this algorithm, we need the gradient, which is given by</p>
<p><span class="math display">\[f(\bx) = \left\|\bx - (2,3,4)^{\top}\right\|_2^2 \implies \nabla f = 2[\bx - (2, 3, 4)^{\top}]\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">ncol=</span><span class="dv">1</span>) <span class="co"># We want to work exclusively with column vecs</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>converged <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fl">0.001</span> <span class="co"># Small step size</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>grad <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="dv">2</span> <span class="sc">*</span> (x <span class="sc">-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="at">ncol=</span><span class="dv">1</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(<span class="sc">!</span>converged){</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> <span class="fu">grad</span>(x)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    x_new <span class="ot">&lt;-</span> x <span class="sc">-</span> c <span class="sc">*</span> g</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(<span class="fu">sum</span>(<span class="fu">abs</span>(x <span class="sc">-</span> x_new)) <span class="sc">&lt;</span> <span class="fl">1e-5</span>){</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        converged <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> x_new</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         [,1]
[1,] 1.998893
[2,] 2.998340
[3,] 3.997787</code></pre>
</div>
</div>
<p>We don’t get the <em>exact</em> minimizer, but we certainly get something ‘close enough’ for our purposes. In <a href="../reports/repot01.html">Report #01</a>, you will use gradient descent and some variants to study the least squares problem.</p>
<p>Often, we will want to see how the value of <span class="math inline">\(f(\bx_k)\)</span> changes over the course of the optimization. We expect that it will go down monotonically, but it may not be worth continuing the optimization if we have reached a point of ‘diminishing returns.’ You can do this by hand (evaluating <span class="math inline">\(f\)</span> after each update and storing the results), but many optimizers will often track this automatically for you: <em>e.g.</em> <a href="https://www.tensorflow.org/tensorboard">TensorBoard</a>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The Bias-Variance decomposition (and tradeoff) holds approximately for other loss functions, though the math is only this nice for MSE.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Note that we can only minimize this quadratic in the case where <span class="math inline">\(a\)</span> is strictly positive so the parabola is upward facing. This is the scalar equivalent of the strict positive-definiteness condition we put on <span class="math inline">\(\bA\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>OLS and some basic variants (ridge regression) can be written in this ‘quadratic’ style. Can you see why?<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>For instance, of the <a href="https://pytorch.org/docs/stable/optim.html#algorithms">14 optimization algorithms</a> included in base <code>pytorch</code>, all but one are advanced versions of gradient descent. The exception is <code>LBFGS</code> which attempts to (approximately) use both the gradient and the Hessian (second derivative); computing the Hessian is normally quite expensive, so <code>LBFGS</code> uses some clever tricks to <em>approximate</em> the Hessian.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Modern ML toolkits like <code>pytorch</code> or <code>TensorFlow</code> are (at heart) fancy systems to do two things automatically that we did ‘by hand’ in this example:</p>
<ul>
<li>Compute gradients via a process known as ‘back-propogation’ or ‘autodiff’</li>
<li>Implement (fancy) gradient descent</li>
</ul>
<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/michael-weylandt\.com\/STA9890\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/michaelweylandt/STA9890/edit/main/archive/AY-2024-SPRING/notes/notes02.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/michaelweylandt/STA9890/blob/main/archive/AY-2024-SPRING/notes/notes02.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/michaelweylandt/STA9890/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>
