<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STA 9890 - Non-Linear Regression – STA 9890</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7a426326b279d3ba9cc0818763c3c226.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">STA 9890</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././notes.html"> 
<span class="menu-text">Handouts and Additional Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../reports.html"> 
<span class="menu-text">Research Reports</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././competition.html"> 
<span class="menu-text">Course Competition</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././resources.html"> 
<span class="menu-text">Additional Resources and Policies</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href=".././objectives.html"> 
<span class="menu-text">Learning Objectives</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#non-linearity-via-linear-combination-of-non-linear-parts" id="toc-non-linearity-via-linear-combination-of-non-linear-parts" class="nav-link active" data-scroll-target="#non-linearity-via-linear-combination-of-non-linear-parts">Non-Linearity via Linear Combination of Non-Linear Parts</a>
  <ul class="collapse">
  <li><a href="#polynomial-expansion" id="toc-polynomial-expansion" class="nav-link" data-scroll-target="#polynomial-expansion">Polynomial Expansion</a></li>
  <li><a href="#spline-regression-additive-models" id="toc-spline-regression-additive-models" class="nav-link" data-scroll-target="#spline-regression-additive-models">Spline Regression (Additive Models)</a></li>
  <li><a href="#splines" id="toc-splines" class="nav-link" data-scroll-target="#splines">Splines</a></li>
  <li><a href="#ell_1-filtering" id="toc-ell_1-filtering" class="nav-link" data-scroll-target="#ell_1-filtering"><span class="math inline">\(\ell_1\)</span>-Filtering</a></li>
  </ul></li>
  <li><a href="#kernel-methods" id="toc-kernel-methods" class="nav-link" data-scroll-target="#kernel-methods">Kernel Methods</a>
  <ul class="collapse">
  <li><a href="#manual-feature-expansion" id="toc-manual-feature-expansion" class="nav-link" data-scroll-target="#manual-feature-expansion">Manual feature expansion</a></li>
  <li><a href="#ridge-without-coefficients" id="toc-ridge-without-coefficients" class="nav-link" data-scroll-target="#ridge-without-coefficients">Ridge without coefficients</a></li>
  <li><a href="#kernel-trick" id="toc-kernel-trick" class="nav-link" data-scroll-target="#kernel-trick">Kernel Trick</a></li>
  </ul></li>
  <li><a href="#trade-offs-between-linear-and-non-linear-methods" id="toc-trade-offs-between-linear-and-non-linear-methods" class="nav-link" data-scroll-target="#trade-offs-between-linear-and-non-linear-methods">Trade-Offs between Linear and Non-Linear Methods</a></li>
  <li><a href="#other-topics-in-regression" id="toc-other-topics-in-regression" class="nav-link" data-scroll-target="#other-topics-in-regression">Other Topics in Regression</a>
  <ul class="collapse">
  <li><a href="#alternative-loss-functions" id="toc-alternative-loss-functions" class="nav-link" data-scroll-target="#alternative-loss-functions">Alternative Loss Functions</a></li>
  <li><a href="#multi-task-regression" id="toc-multi-task-regression" class="nav-link" data-scroll-target="#multi-task-regression">Multi-Task Regression</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/michaelweylandt/STA9890/edit/main/notes/notes04.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/michaelweylandt/STA9890/blob/main/notes/notes04.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/michaelweylandt/STA9890/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">STA 9890 - Non-Linear Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><span class="math display">\[\newcommand{\E}{\mathbb{E}} \newcommand{\R}{\mathbb{R}} \newcommand{\bx}{\mathbf{x}}\newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bX}{\mathbf{X}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\V}{\mathbb{V}} \newcommand{\argmin}{\text{arg min}} \newcommand{\K}{\mathbb{K}}\]</span></p>
<p>This week, we continue our discussion of ‘fancy’ regression from last week to consider <em>non-linear</em> models. While we introduce several methods for non-linear regression, we also pause to consider <em>when</em> non-linear methods can actually be expected to outperform linear methods.</p>
<p>Also note that we are arguably being a bit ‘sloppy’ about what we mean by ‘linear’ methods. For purposes of these notes, we define a ‘linear’ method as one where the fitted model has a linear response to changes in the inputs; formally, one where</p>
<p><span class="math display">\[\frac{\partial y}{\partial x_i} = \beta_i \text{ (a constant) for all } i \in \{1, \dots, p\}\]</span></p>
<p>This definition includes things like best-subsets and lasso regression, which are not linear functions of <span class="math inline">\(\by\)</span>, but excludes things like polynomial regression.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> In the above definition, each <span class="math inline">\(\beta_i\)</span> is essentially a regression coefficient, and values of <span class="math inline">\(i\)</span> where <span class="math inline">\(\beta_i = 0\)</span> are non-selected variables if we are using a sparse method.</p>
<section id="non-linearity-via-linear-combination-of-non-linear-parts" class="level2">
<h2 class="anchored" data-anchor-id="non-linearity-via-linear-combination-of-non-linear-parts">Non-Linearity via Linear Combination of Non-Linear Parts</h2>
<p>We have spent the previous few weeks developing a set of tools for building <em>linear</em> models of <span class="math inline">\(\by\)</span>. Specifically, we have sought ways to approximate <span class="math inline">\(\E[y | \bx]\)</span> as a weighted sum of each <span class="math inline">\(x_i\)</span>:</p>
<p><span class="math display">\[ \E[y | \bx] \approx \langle \bx, \hat{\bbeta} \rangle = \sum_{i=1}^p x_i \hat{\beta}_i\]</span></p>
<p>For situations where the relationship is nearly (or even truly) linear, this approximation can be quite useful and good estimates of the <span class="math inline">\(\beta_i\)</span> coefficients can lead to accurate estimates of <span class="math inline">\(\E[y | \bx]\)</span>, and ultimately accurate prediction of test set response <span class="math inline">\(\by\)</span>. Using the bias-variance decomposition, we have seen that it is often worthwhile to accept a bit of bias in estimation of each <span class="math inline">\(\beta_i\)</span> if it is compensated by a worthwhile reduction in the variance of <span class="math inline">\(\beta_i\)</span>.</p>
<p>We can further refine this equality by decomposing bias a bit further:</p>
<p><span class="math display">\[\text{MSE} = \text{Irreducible Error} + \text{Variance} + \text{Model Bias}^2 + \text{Estimation Bias}^2\]</span></p>
<p>Here, we have decomposed <span class="math inline">\(\text{Bias}^2\)</span> into two terms:</p>
<ul>
<li><span class="math inline">\(\text{Model Bias}^2\)</span>: a measure of the <em>systematic</em> error arising from use of a linear model to predict a non-linear DGP</li>
<li><span class="math inline">\(\text{Estimation Bias}^2\)</span>: a measure of the <em>systematic</em> error arising from use of a regularized estimation procedure which exhibits shrinkage</li>
</ul>
<p>Put another way, “Model Bias” results from use of a linear model when something non-linear should be used, while “Estimation Bias” arises from the use of a biased method (like ridge regression) instead of something nominally unbiased like OLS. For the previous two weeks, we have mainly focused on linear models for linear DGPs, so model bias has been zero. If we expand our gaze to non-linear DGPs, we have to deal with model bias a bit more directly. As with estimation bias, it is frequently worthwhile to accept model bias to reduce variance; see more discussion below.</p>
<section id="polynomial-expansion" class="level3">
<h3 class="anchored" data-anchor-id="polynomial-expansion">Polynomial Expansion</h3>
<p>You have likely already seen polynomial expansion (or polynomial regression) in previous courses. Essentially, PE fits a low(-ish)-order polynomial instead of a line to data. More formally, let <span class="math inline">\(f(\bx) \equiv \E[y|\bx]\)</span> be the regression function (best possible predictor) that week seek to approximate from noisy observations. Standard linear regression essentially seeks to fit a first-order Taylor approximation of <span class="math inline">\(f\)</span> around some point <span class="math inline">\(\overline{\bx}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
f(\bx) &amp;\approx f(\overline{\bx}) + \nabla_f(\bx_0)^{\top}(\bx - \overline{\bx}) \\
       &amp;= f(\overline{\bx}) + \sum_{i=1}^p \left. \frac{\partial f}{\partial x_i}\right|_{x = \overline{x}_i}(x_i - \overline{x}_i)
\end{align*}\]</span></p>
<p>If we rearrange this a bit, we get</p>
<p><span class="math display">\[\begin{align*}
f(\bx) &amp;\approx f(\overline{\bx}) + \sum_{i=1}^p \left. \frac{\partial f}{\partial x_i}\right|_{x = \overline{x}_i}(x_i - \overline{x}_i) \\
&amp;= \underbrace{\left(f(\overline{\bx}) - \sum_{i=1}^p \left.\frac{\partial f}{\partial x_i}\right|_{x_i=\overline{x}_i}\overline{x}_i\right)}_{=\beta_0} + \sum_{i=1}^p \underbrace{\left.\frac{\partial f}{\partial x_i}\right|_{x_i=\overline{x}_i}}_{=\beta_i} x_i\\
&amp;= \beta_0 + \sum_{i=1}^p x_i \beta_i
\end{align*}\]</span></p>
<p>so we see that the regression coefficients are more-or-less the (suitably-averaged) partial derivatives of <span class="math inline">\(f\)</span> while the intercept is the value of <span class="math inline">\(f\)</span> at the center of our expansion (<span class="math inline">\(f(\overline{\bx})\)</span>) plus some differential adjustments. Note that, if the variables <span class="math inline">\(\bX\)</span> are centered so that <span class="math inline">\(\E[\bX] = \mathbf{0}\)</span>, the intercept is exactly the average value of <span class="math inline">\(f\)</span>, as we would expect.</p>
<p>Clearly, this linear approximation will do better in the situations where Taylor expansions generally perform better: when i) the derivative of <span class="math inline">\(f\)</span> is roughly constant and ii) the higher order terms are small. This makes sense: if <span class="math inline">\(f\)</span> is very-close-to-linear, there is minimal loss from fitting a linear approximation to <span class="math inline">\(f\)</span>; if <span class="math inline">\(f\)</span> is very non-linear, however, a linear approximation can only be be so good, and we are saddled with significant model bias.</p>
<p>If we want to mitigate some of that model bias, we can choose to use a higher-order Taylor series. If we examine a second order Taylor series, we get a similar approximation as before, where <span class="math inline">\(\mathcal{H}\)</span> denotes the <em>Hessian</em> matrix of second derivatives:</p>
<p><span class="math display">\[\begin{align*}
f(\bx) &amp;\approx f(\overline{\bx}) + \nabla_f(\bx_0)^{\top}(\bx - \overline{\bx}) + \frac{1}{2}(\bx - \overline{\bx})^{\top}\mathcal{H}_f(\overline{\bx})(\bx - \overline{\bx})^{\top}
\end{align*}\]</span></p>
<p>After some rearrangement this becomes:</p>
<p><span class="math display">\[f(\bx) \approx \beta_0 + \sum_{i=1}^p \frac{\partial f}{\partial x_i} x_i +\sum_{i=1}^p \frac{\partial^2 f}{\partial x_i^2} x_i^2 +  \sum_{\substack{i, j = 1 \\i \neq j}}^p \frac{\partial^2 f}{\partial x_i \partial x_j} x_ix_j\]</span></p>
<p>We recognize the constant term (intercept) and first order terms (linear slopes) from before, but now we have added two types:</p>
<ul>
<li>Second order terms (<span class="math inline">\(x_i^2\)</span>): these capture higher order non-linearities in a single variable. For example, if <span class="math inline">\(y = x^2\)</span>, then the ‘true fit’ really lies in the second order term <span class="math inline">\(\partial^2f / \partial x^2 = 2\)</span> instead of any linear approximation</li>
<li>Cross terms (<span class="math inline">\(x_ix_j\)</span>): these capture non-linear (non-additive) multi-variate relationships between features. You may have seen these before as <em>interaction</em> terms.</li>
</ul>
<p>Rules of thumb vary as to which set of terms are more important. Historically, statisticians have tended to put in the interaction (cross) terms first, though this is far from universal practice. In particular, when dealing with ‘binary’ variables, the higher order term is unhelpful: if a feature is <span class="math inline">\(0/1\)</span> (did the patient receive the drug or not?), adding a squared term has no effect since <span class="math inline">\(0^2 = 0\)</span> and <span class="math inline">\(1^2 = 1\)</span>.</p>
<p>Regardless, this is why we sometimes use ‘polynomial expansion’ of our original variables. We are trying to capture the higher-order (non-linear) terms of the Taylor approximation.</p>
<p>It is worth thinking about what happens when we add higher-order terms to our model. In particular, note that we i) pick up some correlation among features; and ii) we will generally have more variance since we have more features. In practice, the correlation isn’t too much of a problem and the actual polynomials fit by, <em>e.g.</em>, <code>poly</code> in <code>R</code> are modified to remove correlation. The variance however can be a more significant problem.</p>
<p>Let’s see this in action. Let’s first consider fitting polynomial regression to a simple (univariate) non-linear function: <span class="math inline">\(f(x) = \sqrt{|x|^3 + 5} * \cos(x)\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">sqrt</span>(<span class="fu">abs</span>(x)<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="dv">5</span>) <span class="sc">*</span> <span class="fu">cos</span>(x), <span class="at">from=</span><span class="dv">1</span>, <span class="at">to=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This is not <em>too</em> non-linear. It’s still very smooth (<em>analytic</em> in the jargon) and should be relatively easy to fit. Let’s also generate some noisy observations of this function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear regression doesn't care about the order of our data, but plotting does</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># so sorting gives better visuals</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">2</span>, <span class="dv">10</span>)) </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>Ey <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">abs</span>(x)<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="dv">5</span>) <span class="sc">*</span> <span class="fu">cos</span>(x)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Ey <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd=</span><span class="dv">3</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>x_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out=</span><span class="dv">501</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>Ey_grid <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">abs</span>(x_grid)<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="dv">5</span>) <span class="sc">*</span> <span class="fu">cos</span>(x_grid)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can now fit some linear models to this data (shown in color): we use R’s built-in <code>poly</code> function to automatically create the polynomials we seek. You can think of <code>poly(x, k)</code> as creating <code>k</code> features <span class="math inline">\(x^1, x^2,
\dots, x^k\)</span> but it actually does something a bit more subtle under the hood to make model fitting a bit more numerically stable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, k))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x_grid, yhat, <span class="at">col=</span>k<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomleft"</span>,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(<span class="st">"True Regression Function"</span>, </span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>         <span class="st">"First Order (Linear) Fit"</span>,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Second Order (Quadratic) Fit"</span>, </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Third Order (Cubic) Fit"</span>, </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Fourth Order (Quartic) Fit"</span>,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Fifth Order (Quintic) Fit"</span>), </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">lty=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Clearly, as we increase the order of the polynomial, we get a better (in-sample) fit. This makes sense: we know that more features gives us a better fit all else being equal, so what’s the issue?</p>
<p>As usual, the issue occurs for extrapolation. You can actually see some issues already beginning to manifest at the ends of our prediction intervals: the higher-order polynomials go radically different directions as we extrapolate and even the ‘best fit’ (quintic) looks like it’s going to be too steep as we go further.</p>
<p>Let’s expand the prediction region for these fits:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x_pred <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">15</span>, <span class="at">length.out=</span><span class="dv">501</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Ey_pred <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">abs</span>(x_pred)<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="dv">5</span>) <span class="sc">*</span> <span class="fu">cos</span>(x_pred)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_pred, Ey_pred, <span class="at">type=</span><span class="st">"l"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, y)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, k))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x=</span>x_pred))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x_pred, yhat, <span class="at">col=</span>k<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomleft"</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">c</span>(<span class="st">"True Regression Function"</span>, </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>         <span class="st">"First Order (Linear) Fit"</span>,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Second Order (Quadratic) Fit"</span>, </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Third Order (Cubic) Fit"</span>, </span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Fourth Order (Quartic) Fit"</span>,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>         <span class="st">"Fifth Order (Quintic) Fit"</span>), </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">lty=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There’s some problems!</p>
<p>To be fair, it is a bit unreasonable to expect these models to perform too well outside of our original sampling area. What’s more worrying is that the extrapolations don’t just miss the curves of the true regression function, they actually create their own even more intense wiggles.</p>
<p>We can also manifest this without extrapolation by fitting very high-order polynomials:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">9</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="at">degree=</span><span class="dv">10</span> <span class="sc">+</span> k, <span class="at">raw=</span><span class="cn">TRUE</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="at">degree=</span><span class="dv">14</span> <span class="sc">+</span> k, <span class="at">raw=</span><span class="cn">TRUE</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x_grid, yhat, <span class="at">col=</span>k)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Note that we have to use <code>raw=TRUE</code> here to avoid <code>R</code> complaining about having too-high a degree in the polynomial. (<code>R</code> gives an error about “unique points” but the real issue is one about matrix rank, not ties in <code>x</code>.) I’m also surpressing some warnings here about a sub-standard fit.</p>
<p>So how can we address this ‘over-wiggliness’?</p>
<p>As we discussed last time, we need to use a different set of functions: one that is smooth (like polynomials), but not too-high order. It would also be really nice if we could get something ‘adaptive’ - allowing for more wiggliness where we have more data (and we need more wiggliness) and less wiggliness where we don’t have enough data (fall-back to linearity). ### Local Linear Models</p>
<p>One way to do this is the idea of “local linear (polynomial) models.” Instead of fitting a single (global) line, we can fit different lines in different parts of our data: that way, we can get an upward line when the true (non-linear) relationship is increasing and a downward line when the true relationship is decreasing. Or at least that’s the hope!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Divide our data into five 'buckets' and fit sub-models</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This works because we've already sorted our data</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># (What would happen if we hadn't?)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    xk <span class="ot">&lt;-</span> x[(<span class="dv">5</span> <span class="sc">*</span> k <span class="sc">-</span> <span class="dv">4</span>)<span class="sc">:</span>(<span class="dv">5</span><span class="sc">*</span>k)]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    yk <span class="ot">&lt;-</span> y[(<span class="dv">5</span> <span class="sc">*</span> k <span class="sc">-</span> <span class="dv">4</span>)<span class="sc">:</span>(<span class="dv">5</span><span class="sc">*</span>k)]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    mk <span class="ot">&lt;-</span> <span class="fu">lm</span>(yk <span class="sc">~</span> xk)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(mk)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(xk, y_hat, <span class="at">col=</span><span class="st">"red4"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Definitely some rough edges here - particularly in the areas between our buckets, but we’re on a good path. The <code>locfit</code> package will help us with the details:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(locfit)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>locfit 1.5-9.11      2025-01-27</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> (<span class="fu">locfit</span>(y <span class="sc">~</span> <span class="fu">lp</span>(x)))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimation type: Local Regression 

Call:
locfit(formula = y ~ lp(x))

Number of data points:  25 
Independent variables:  x 
Evaluation structure: Rectangular Tree 
Number of evaluation points:  7 
Degree of fit:  2 
Fitted Degrees of Freedom:  5.194 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_hat, <span class="at">col=</span><span class="st">"red4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Not perfect, but pretty nice compared to what we had before.</p>
<p>You can also see a discussion of the “degrees of freedom” in the model output. This is not exactly the DoF you learned in earlier classes, but it’s sort of “morally equivalent.” This local fit is about as flexible as a 4th degree polynomial would be for this problem. Even though this model is made out of quadratics, it’s more flexible than a single quadratic. But we avoid the extreme variability associated with a polynomial of that high order. Win-win!</p>
<p>We can tweak some parameters of the local fit to get different responses: the big ones are the degree (<code>deg</code>) and the number of neighbors to use.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>?lp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> (<span class="fu">locfit</span>(y <span class="sc">~</span> <span class="fu">lp</span>(x, <span class="at">deg=</span><span class="dv">4</span>)))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimation type: Local Regression 

Call:
locfit(formula = y ~ lp(x, deg = 4))

Number of data points:  25 
Independent variables:  x 
Evaluation structure: Rectangular Tree 
Number of evaluation points:  7 
Degree of fit:  4 
Fitted Degrees of Freedom:  6.986 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_hat, <span class="at">col=</span><span class="st">"red4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Pretty nice. If we ‘turn down’ the number of neighbors used:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> (<span class="fu">locfit</span>(y <span class="sc">~</span> <span class="fu">lp</span>(x, <span class="at">nn=</span><span class="fl">0.2</span>, <span class="at">deg=</span><span class="dv">4</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in lfproc(x, y, weights = weights, cens = cens, base = base, geth =
geth, : Estimated rdf &lt; 1.0; not estimating variance</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimation type: Local Regression 

Call:
locfit(formula = y ~ lp(x, nn = 0.2, deg = 4))

Number of data points:  25 
Independent variables:  x 
Evaluation structure: Rectangular Tree 
Number of evaluation points:  27 
Degree of fit:  4 
Fitted Degrees of Freedom:  24.257 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_hat, <span class="at">col=</span><span class="st">"red4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Too far - super crazy! But not entirely unexpected - we know that <span class="math inline">\(K\)</span>-Nearest Neighbors for small <span class="math inline">\(K\)</span> has a huge variance.</p>
<p>Too far!</p>
<p>The <code>loess</code> function in base <code>R</code> does this very nicely as well without requiring additional packages. For practical work, it’s a very nice tool for univariate modeling.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
loess(formula = y ~ x)

Number of Observations: 25 
Equivalent Number of Parameters: 4.64 
Residual Standard Error: 4.098 
Trace of smoother matrix: 5.1  (exact)

Control settings:
  span     :  0.75 
  degree   :  2 
  family   :  gaussian
  surface  :  interpolate     cell = 0.2
  normalize:  TRUE
 parametric:  FALSE
drop.square:  FALSE </code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_hat, <span class="at">col=</span><span class="st">"red4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="spline-regression-additive-models" class="level3">
<h3 class="anchored" data-anchor-id="spline-regression-additive-models">Spline Regression (Additive Models)</h3>
</section>
<section id="splines" class="level3">
<h3 class="anchored" data-anchor-id="splines">Splines</h3>
<p>We can generalize this idea a bit using splines. Splines are functions that solve a penalized approximation problem:</p>
<p><span class="math display">\[\hat{f} = \text{arg min}_{f} \frac{1}{2n} \|y - f(x)\|_2^2 + \lambda \int |f''(x)|^2 \text{d}x\]</span></p>
<p>Here we are saying we’ll take <em>any function</em> (not just a polynomial) that achieves the optimal trade-off between data fit (first / loss term) and not being too rough (second / penalty term).</p>
<p>Why is this the right penalty to use? In essence, we are trying to penalize ‘deviation from linearity’ and since linear functions have second derivative 0 (by definition) the integral of the second derivative gives us a measure of non-linearity.</p>
<p>In some remarkable work, <a href="https://en.wikipedia.org/wiki/Grace_Wahba">Grace Wahba</a> and co-authors showed that the solutions to that optimization problem are piecewise polynomials with a few additional constraints - these functions are <em>splines</em>. In addition to piecewise polynomialness, splines also guarantee:</p>
<ul>
<li>continuity of the function and its derivatives</li>
<li>linearity outside the range of data fit (‘natural splines’)</li>
</ul>
<p>Because splines not only match the value of the function at the knots (places where the ‘pieces’ match up) but also the derivatives, they are very smooth indeed.</p>
<p>Stepping back, splines are just a different sort of feature engineering. Instead of using polynomial basis functions (<span class="math inline">\(x^1, x^2, \dots, x^k\)</span>), splines use a much smoother basis and hence give smoother results, avoiding the ‘wiggles’ problem we saw above.</p>
<p><code>R</code> provides the basic tools for spline fitting in the <code>splines</code> package. For real work, you almost surely want to use the advanced functionality of the <code>mgcv</code> package. For more on splines and models using them (‘additive models’), see <a href="https://noamross.github.io/gams-in-r-course/">this online course</a>.</p>
<p>We can see that individual splines are quite nice little functions and usually are only non-zero on small regions:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>nsx <span class="ot">&lt;-</span> <span class="fu">ns</span>(x_grid, <span class="at">df=</span><span class="dv">5</span>) <span class="co"># The natural spline basis</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_grid, <span class="fu">apply</span>(nsx, <span class="dv">1</span>, max), <span class="at">ylim=</span><span class="fu">range</span>(nsx), <span class="at">type=</span><span class="st">"n"</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x_grid, nsx[,i], <span class="at">col=</span>i)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can use linear combinations of these ‘bumps’ to fit non-linear functions, including our example from above:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df=</span><span class="dv">5</span>))</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_hat, <span class="at">col=</span><span class="st">"red4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>(This basically matches what we had before with <span class="math inline">\(\text{DoF} \approx 4.7\)</span>, but you can get different answers by messing with the <code>df</code> parameter here.)</p>
<p>Our predicted response is actually the sum of the individual spline effects:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df=</span><span class="dv">5</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x_grid, <span class="fu">ns</span>(x_grid, <span class="at">df=</span><span class="dv">5</span>)[,i] <span class="sc">*</span> <span class="fu">coef</span>(m)[<span class="sc">-</span><span class="dv">1</span>][i], <span class="at">col=</span>i<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>If we were able to sum up the colored lines, we would get the black line back.</p>
<p>We can also see that natural splines give smooth predictions outside the range of the original data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_pred))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_pred, Ey_pred, <span class="at">type=</span><span class="st">"l"</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_pred, y_pred, <span class="at">col=</span><span class="st">"red4"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Admittedly, not ideal, but it will do. In particular, note that outside of the ‘data region’, we fall back on a nice comfortable</p>
<p>(You can repeat this process with the slightly more common <span class="math inline">\(b\)</span>-splines, but the differences aren’t huge.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">df=</span><span class="dv">5</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">xlim=</span><span class="fu">range</span>(x_grid))</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(x_grid, <span class="fu">bs</span>(x_grid, <span class="at">df=</span><span class="dv">5</span>)[,i] <span class="sc">*</span> <span class="fu">coef</span>(m)[<span class="sc">-</span><span class="dv">1</span>][i], <span class="at">col=</span>i<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred, <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>For a ‘full-strength’ version of spline fitting, you can use the <code>mgcv</code> package:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: nlme</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>This is mgcv 1.9-1. For overview type 'help("mgcv-package")'.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">gam</span>(y <span class="sc">~</span> <span class="fu">s</span>(x))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Family: gaussian 
Link function: identity 

Formula:
y ~ s(x)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -4.2852     0.5974  -7.173  1.1e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms:
       edf Ref.df     F p-value    
s(x) 5.954  7.038 70.49  &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) =  0.954   Deviance explained = 96.5%
GCV = 12.359  Scale est. = 8.9212    n = 25</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">gam</span>(y <span class="sc">~</span> <span class="fu">s</span>(x))</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(m, <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">xlim=</span><span class="fu">range</span>(x_grid))</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, Ey_grid)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_hat, <span class="at">col=</span><span class="st">"red4"</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="notes04_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Run <code>?s</code> to see the <em>many</em> features of the <code>mgcv</code> package. <code>mgcv</code> also includes several specialized splines (<em>e.g.</em> periodic) which you may find useful in some problems.</p>
<p>Splines are not normally consider a ‘machine learning’ tool but they are <em>incredibly</em> useful in applied statistics work. (The two methods you will rely on most from this class are spline regression and random forests.)</p>
</section>
<section id="ell_1-filtering" class="level3">
<h3 class="anchored" data-anchor-id="ell_1-filtering"><span class="math inline">\(\ell_1\)</span>-Filtering</h3>
</section>
</section>
<section id="kernel-methods" class="level2">
<h2 class="anchored" data-anchor-id="kernel-methods">Kernel Methods</h2>
<section id="manual-feature-expansion" class="level3">
<h3 class="anchored" data-anchor-id="manual-feature-expansion">Manual feature expansion</h3>
<p>We can think of feature expansion as mapping our data to a higher-dimensional space <span class="math inline">\(\Phi: \R^p \to \R^P\)</span> and fitting a linear model there. As we have seen above, splines and their kin provide a useful way of constructing the mapping <span class="math inline">\(\Phi\)</span>, but we are still constrained to work with a fairly restricted type of problem.</p>
<p>Can we generalize this formally to more interesting maps <span class="math inline">\(\Phi(\cdot)\)</span>? Yes - via kernel methods!</p>
</section>
<section id="ridge-without-coefficients" class="level3">
<h3 class="anchored" data-anchor-id="ridge-without-coefficients">Ridge without coefficients</h3>
<p>Before introducing kernel methods formally, let’s look back to ridge regression. We showed that the ridge solution is given by</p>
<p><span class="math display">\[\hat{\beta}_{\text{Ridge}} = (\bX^T\bX + \lambda \bI)^{-1}(\bX^T\by)\]</span></p>
<p>Some nifty linear algebra will let us rewrite this as</p>
<p><span class="math display">\[\hat{\beta}_{\text{Ridge}} = \lambda^{-1}\bX^T(\bX\bX^{\top}/\lambda + \bI)^{-1}\by = \bX^{\top}(\bX\bX^{\top}+\lambda \bI)^{-1}\by\]</span></p>
<p>You can prove this using the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury Matrix Identity</a> or we can just check it for a single data set and trust that it generalizes:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span> </span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), <span class="at">nrow=</span>n, <span class="at">ncol=</span>p) </span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>Ey <span class="ot">&lt;-</span> X[,<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">sqrt</span>(X[,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">5</span>) <span class="sc">+</span> <span class="fl">0.1</span> <span class="sc">*</span> X[,<span class="dv">3</span>]<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="fu">cos</span>(<span class="fu">abs</span>(X[,<span class="dv">4</span>])) <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>(<span class="fu">abs</span>(X[,<span class="dv">5</span>]) <span class="sc">+</span> <span class="dv">3</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Ey <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd=</span><span class="fu">sqrt</span>(<span class="fl">0.25</span>))</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>eye <span class="ot">&lt;-</span> <span class="cf">function</span>(p) <span class="fu">diag</span>(<span class="dv">1</span>, p, p)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>beta_hat_ridge <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">crossprod</span>(X) <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">eye</span>(p), <span class="fu">crossprod</span>(X, y))</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>beta_hat_ridge_alt <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">solve</span>(<span class="fu">tcrossprod</span>(X) <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">eye</span>(n)) <span class="sc">%*%</span> y</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(beta_hat_ridge, beta_hat_ridge_alt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]       [,2]
[1,] 1.42746826 1.42746826
[2,] 0.15151370 0.15151370
[3,] 0.57121814 0.57121814
[4,] 0.04425193 0.04425193
[5,] 0.58742212 0.58742212</code></pre>
</div>
</div>
<p>So they are the same! But why did we do this?</p>
<p>It is sometimes a bit more computationally efficient if we have to invert an <span class="math inline">\(n \times n\)</span> matrix instead of a <span class="math inline">\(p \times p\)</span> matrix, but there are faster methods if we’re really interested in speed.</p>
<p>Note that, if we want to make a ridge regression <em>prediction</em> now for a new <span class="math inline">\(\tilde{\bx}\)</span>, we just need to compute:</p>
<p><span class="math display">\[\newcommand{\bx}{\mathbf{x}}\begin{align*}
\hat{\beta}_{\text{Ridge}}^T\tilde{\bx} &amp;= \left[\bX^{\top}(\bX\bX^{\top}+\lambda \bI)^{-1}\by\right]^T\tilde{\bx} \\
&amp;= \left[\by^T(\bX\bX^{\top}+\lambda \bI)^{-T}\bX\right]\tilde{\bx} \\
&amp;= \by^T(\bX\bX^{\top}+\lambda \bI)^{-1} \bX\tilde{\bx}
\end{align*}\]</span></p>
<p>Intuitively, recall that the inner product of two vectors measures the angle between them and can, up to some scaling, be used as a ‘similarity’ measure. This result essentially says that our prediction on a new data set is a weighted average of the training data, with weights based on similarity of the new point with the training data. This is not a crazy structure…</p>
<p>If we look closer, we see that we <strong>only</strong> need to compute products of the form <span class="math inline">\(\bx_1\bx_2\)</span> to make this work. This is where the magic of kernels lies.</p>
</section>
<section id="kernel-trick" class="level3">
<h3 class="anchored" data-anchor-id="kernel-trick">Kernel Trick</h3>
<p>Earlier, we considered non-linear regression by <em>feature expansion</em>, where we replaced <span class="math inline">\(\bX\)</span> by <span class="math inline">\(\Phi(\bX)\)</span>, where <span class="math inline">\(\Phi: \R^p \to \R^P\)</span> maps to a higher-dimensional space (applied row-wise). We can use this mapping in our new ridge formula to get our non-linear predictions as</p>
<p><span class="math display">\[\begin{align*}
\hat{y}(\tilde{\bx}) &amp;= \by^T(\Phi(\bX)\Phi(\bX)^{\top}+\lambda \bI)^{-1} \Phi(\bX)\Phi(\tilde{\bx})
\end{align*}\]</span></p>
<p>Longer, but not necessarily better. If <span class="math inline">\(P\)</span> is very large, computing with <span class="math inline">\(\Phi(X)\)</span> can be <em>incredibly</em> expensive.</p>
<p>It turns out, however, that we never need to actually form <span class="math inline">\(\Phi(X)\)</span>, we only need <span class="math inline">\(\kappa(\bx_1, \bx_2) = \Phi(\bx_1)^T\Phi(\bx_2)\)</span>. If we can compute <span class="math inline">\(\kappa\)</span> directly, we never need to work with <span class="math inline">\(\Phi\)</span>.</p>
<p>Functions that allow this are called (Mercer) kernels and they are just a little bit magical.</p>
<p>If we let <span class="math inline">\(\K = \Phi(X)\Phi(X)^{\top}\)</span> be defined by <span class="math inline">\(\K_{ij} = \kappa(\bx_i, \bx_j)\)</span>, then we can write our feature-augmented ridge regression as:</p>
<p><span class="math display">\[\hat{y}(\tilde{\bx}) = \by^T(\K+\lambda \bI)^{-1} \kappa(\bX, \tilde{\bx})\]</span></p>
<p>If we break this apart, we see that our predictions at the new point <span class="math inline">\(\tilde{\bx}\)</span> are essentially just weighted averages of our original observations <span class="math inline">\(\by\)</span>, weighted by the similarity between the new point and our training points <span class="math inline">\(\kappa(\bX, \cdot)\)</span>. This intuition is super important and we’ll dig into it further below.</p>
<p>Let’s try out <em>Kernel(ized) Ridge Regression</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Define a kernel function - we'll spell this out below.</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>rbf <span class="ot">&lt;-</span> <span class="fu">rbfdot</span>(<span class="at">sigma =</span> <span class="fl">0.05</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="do">## calculate kernel matrix</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">kernelMatrix</span>(rbf, X)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(K)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 100 100</code></pre>
</div>
</div>
<p>Note that this is a <span class="math inline">\(n \times n\)</span> matrix - not a <span class="math inline">\(p \times p\)</span> matrix!</p>
<p>We can now use this to make predictions:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>krr <span class="ot">&lt;-</span> <span class="cf">function</span>(x, <span class="at">lambda=</span><span class="dv">1</span>) {</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">crossprod</span>(y, <span class="fu">solve</span>(K <span class="sc">+</span> lambda <span class="sc">*</span> <span class="fu">eye</span>(n), <span class="fu">kernelMatrix</span>(rbf, X, x)))</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>krr_MSE <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> <span class="fu">krr</span>(X))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(krr_MSE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4916979</code></pre>
</div>
</div>
<p>This is <em>better</em> than the OLS MSE:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">resid</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X))<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5484549</code></pre>
</div>
</div>
<p>Why? How is this not a contradiction of everything said before.</p>
<p>But how do we actually <em>do</em> kernel multiplication?</p>
<p>There are <em>many</em> kernel functions in the world. The most common are defined by:</p>
<ul>
<li><span class="math inline">\(\kappa(\bx_1, \bx_2) = \bx_1^{\top}\bx_2\)</span>. This is the linear kernel, equivalent to non-kernel methods</li>
<li><span class="math inline">\(\kappa(\bx_1, \bx_2) = (\bx_1^{\top}\bx_2 + c)^d\)</span>. This is the <em>polynomial kernel</em>, equivalent to fitting polynomial regression up to degree <span class="math inline">\(d\)</span> with all cross products. (The role of <span class="math inline">\(c\)</span> is tricky, but it essentially controls the relative weight of the higher and lower order terms)</li>
<li><span class="math inline">\(\kappa(\bx_1, \bx_2) = e^{-\sigma^2\|\bx_1 - \bx_2\|^2}\)</span>. This is the <em>squared exponential</em> or <em>radial basis function</em> (RBF) kernel; it is very popular in spatial statistics, where it is closely related to a method known as <a href="https://en.wikipedia.org/wiki/Kriging">kriging</a>.</li>
</ul>
<p>While the first two kernels give us a natural <span class="math inline">\(\Phi(\cdot)\)</span> mapping, the <span class="math inline">\(\Phi(\cdot)\)</span> for the RBF kernel is actually <em>infinite dimensional</em>: it lets us fit function classes we could not fit without the kernel trick.</p>
<p>For even more kernels, see:</p>
<ul>
<li><a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">David Duvenaud’s Kernel Cookbook</a></li>
<li><a href="http://gaussianprocess.org/gpml/chapters/RW.pdf">GPML Section 4.2.1</a></li>
</ul>
<p>Much of our current understanding of deep learning is based on specialized kernel methods:</p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Neural_tangent_kernel">Neural Tangent Kernel</a></li>
<li><a href="https://papers.nips.cc/paper_files/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html">Kernel Methods for Deep Learning</a></li>
<li><a href="https://arxiv.org/abs/2012.00152">Gradient Descent as a Kernel Machine</a> (This paper is a bit controversial)</li>
</ul>
<p><a href="https://rajatvd.github.io/NTK/">This blog</a> is a particularly nice (but still somewhat technical) introduction to the Neural Tangent Kernel and its relationship to neural networks.</p>
<p>Spline theory has also found use to explain ReLU (piecewise linear activation) neural networks:</p>
<ul>
<li><a href="https://arxiv.org/abs/1805.06576">Mad Max: Spline Insights into Deep Learning</a></li>
<li><a href="https://jmlr.csail.mit.edu/papers/v22/20-583.html">Representer Theorems for Neural Networks and Ridge Splines</a></li>
</ul>
<p>We can also <em>kernelize</em> some of the other methods we’ve studied in this course.</p>
<p>Notably, we can kernelize <span class="math inline">\(K\)</span>-Nearest Neighbors to get “KKNN” if we modify the definition of distance used to be “kernel-ish.” We first recall that we can write distances solely in terms of inner products:</p>
<p><span class="math display">\[\begin{align*}
\|\bx_1 - \bx_2\|_2^2 &amp;= (\bx_1 - \bx_2)^T(\bx_1 - \bx_2) \\
&amp;= \bx_1^T\bx_1 - \bx_2^T\bx_1 - \bx_1^T\bx_2 + \bx_2^T\bx_2 \\
&amp;= \bx_1^T\bx_1  - 2\bx_1^T\bx_2 + \bx_2^T\bx_2 \\
\implies \text{Kernel Distances} &amp;= \kappa(\bx_1, \bx_1) - 2\kappa(\bx_1, \bx_2) + \kappa(\bx_2, \bx_2)
\end{align*}\]</span></p>
<p>If we compute distances this way for a given <span class="math inline">\(\kappa\)</span>, we get kernel KNN. This is particularly nice when we have kernels that capture specific behaviors (<em>e.g.</em>, periodic or embedded categorical) that we can’t really treat as Euclidean.</p>
</section>
</section>
<section id="trade-offs-between-linear-and-non-linear-methods" class="level2">
<h2 class="anchored" data-anchor-id="trade-offs-between-linear-and-non-linear-methods">Trade-Offs between Linear and Non-Linear Methods</h2>
<p>Having developed some tools for non-linear regression, let’s step back and ask whether they are worthwhile. Recall our error decomposition:</p>
<p><span class="math display">\[\text{MSE} = \text{Irreducible Error} + \text{Variance} + \text{Model Bias}^2 + \text{Estimation Bias}^2\]</span></p>
<p>We have already discussed how tools like ridge and lasso let us remove variance at the cost of (estimation) bias.</p>
<p>We argued that this was a worthwhile trade when the problem had a lot of ‘innate’ variance, either from having large noise variance (<span class="math inline">\(\V[\epsilon] \gg 0\)</span>) or from having many features. In particular, use of some sort of shrinkage (ridge or lasso penalization) was essential in the ‘high-dimensional’ case (<span class="math inline">\(p &gt; n\)</span>) where OLS had so much variance that it was not even uniquely defined. We also argued that, as we got more and more data (<span class="math inline">\(n \to \infty\)</span>), the variance took care of itself in the usual statistical way.</p>
<p>These lessons generalize to the non-linear vs linear debate as well. Choosing to use a linear model is itself a variance reducing choice - there are ‘more’ curves than lines in some sense. If we restrict our attention to linear models only, we are potentially accepting some Model Bias, again with a hope of reducing variance to get overall better performance. As such, the same intuition as above applies: linear models are preferred when variance is the primary concern, either from noisy data or from small data; as we get more data, variance decreases naturally, so going to non-linear models reduces bias, giving overall smaller error.</p>
<p>In the non-linear context, the ‘estimation bias / variance’ trade-off remains, but modern tools like <code>mgcv</code> essentially handle this automatically for us. It is of course still there, but <code>mgcv</code> has some <a href="https://doi.org/10.1080/01621459.2016.1180986">nifty auto-tuning</a> built-in.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
</section>
<section id="other-topics-in-regression" class="level2">
<h2 class="anchored" data-anchor-id="other-topics-in-regression">Other Topics in Regression</h2>
<section id="alternative-loss-functions" class="level3">
<h3 class="anchored" data-anchor-id="alternative-loss-functions">Alternative Loss Functions</h3>
<p>For most of this class, we have used MSE as our loss function, building on a foundation of least squares.</p>
<p>Squared (<span class="math inline">\(\ell_2^2\)</span>) error is a bit interesting: as an error gets <em>larger</em>, it counts even more. This makes OLS/MSE methods sensitive to outliers (in the same way the mean is), but it also implies that OLS won’t work too hard to “over-minimize” small errors.</p>
<p>We can consider some other loss functions: - <span class="math inline">\(\ell_1\)</span> error (absolute difference): robust to outliers, fits to the conditional <em>median</em> instead of of the conditional <em>mean</em>.</p>
<p>The resulting <em>Median Absolute Deviation</em> (MAD) regression no longer has a closed form, but can be solved quickly using tools like <code>CVX.</code></p>
<ul>
<li><p>Huber loss: a blend of <span class="math inline">\(\ell_2\)</span> and <span class="math inline">\(\ell_1\)</span> error, which uses <span class="math inline">\(\ell_2\)</span> for small errors and <span class="math inline">\(\ell_1\)</span> for big errors. <span class="math display">\[\text{HL}_{\delta}(x) = \begin{cases} \frac{1}{2}x^2 &amp; |x| &lt; \delta \\ \delta * (|x| - \delta/2) &amp; |x| \geq \delta \end{cases}\]</span></p>
<p>This is still convex, so we can solve it with <code>CVX.</code></p></li>
<li><p><span class="math inline">\(\epsilon\)</span>-insensitive: <span class="math display">\[L_{\epsilon}(x) = (|x| - \epsilon)_+ = \begin{cases} 0 &amp; |x| &lt; \epsilon \\ |x| - \epsilon &amp; |x| \geq \epsilon \end{cases}\]</span> This captures the idea of “close enough is good enough” with “close enough” being defined by <span class="math inline">\(\epsilon\)</span>. It’s a bit unnatural statistically, but relates to an important classification method we’ll cover in a few weeks.</p></li>
</ul>
</section>
<section id="multi-task-regression" class="level3">
<h3 class="anchored" data-anchor-id="multi-task-regression">Multi-Task Regression</h3>
<p>In some contexts, we may want to perform <em>multiple regressions</em> at the same time. That is, we have a <em>vector</em> of responses for each observation. The OLS generalization is straightforward: <span class="math display">\[\newcommand{\bbeta}{\mathbf{\beta}}\newcommand{\bB}{\mathbf{B}}\newcommand{\bY}{\mathbf{Y}}\argmin_{\bB} \frac{1}{2}\|\bY - \bX\bB\|_F^2 \implies \hat{\bB} = (\bX^T\bX)^{-1}\bX^T\bY\]</span> so we essentially just fit different OLS models for each element of the response. This is not super interesting.</p>
<p>Multi-task regression becomes interesting if we want to add structure to <span class="math inline">\(\bB\)</span>: a particularly common requirement is to select the <em>same set of features</em> for each of the regression targets. We do this using a group lasso on <em>each row</em> of <span class="math inline">\(\bB\)</span>: under this structure, if we select a feature for one part of the response, we’ll use it for every part. (Can you see why?)</p>
<p><span class="math display">\[\argmin_{\bB} \frac{1}{2}\|\bY - \bX\bB\|_F^2 + \lambda \sum_{j=1}^p \|\bB_{j\cdot}\|_2\]</span></p>
<p>Another common requirement is for <span class="math inline">\(\bB\)</span> to be “low-rank” but we won’t consider that here. If you want to look into it, keywords are “nuclear-norm regularized.”</p>
<section id="weights-and-generalization" class="level4">
<h4 class="anchored" data-anchor-id="weights-and-generalization">Weights and generalization</h4>
<ul>
<li><p>WLS loss: <span class="math inline">\(\newcommand{\bW}{\mathbf{W}}(\by - \bX\bbeta)^T\bW(\by - \bX\bbeta)\)</span> for known diagonal <span class="math inline">\(\bW\)</span>. Weight samples based on their variance (higher variance gets smaller weights)</p></li>
<li><p>Weighted lasso: <span class="math inline">\(+\lambda\sum_{j=1}^p w_j|\beta_j|\)</span>. Higher weights are less likely to be selected</p>
<p>Basis of ‘adaptive’ lasso methods (not discussed)</p></li>
<li><p>Generalized ridge: <span class="math inline">\(+\frac{\lambda}{2}\bbeta^T\Omega\beta\)</span></p>
<p>Used to ‘smooth’ <span class="math inline">\(\hat{\beta}\)</span> - useful for ridge analogue of fused lasso</p></li>
</ul>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In other classes, you may have been taught that polynomial regression is a linear model. It is definitely linear in the sense that that class would have used (linear in <span class="math inline">\(\by\)</span>) but we’re using a different sense here. The core insight of that analysis - that a non-linear function can be expressed as a linear combination of non-linear parts - will appear several times in these notes.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In Python, the <code>statsmodels</code> package also implements <a href="https://www.statsmodels.org/dev/examples/notebooks/generated/lowess.html">LOESS fits</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>If you want to go much deeper into the practical use of additive (spline) models, Simon Wood’s book <a href="https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331">Generalized Additive Models</a> is fantastic.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/michael-weylandt\.com\/STA9890\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/michaelweylandt/STA9890/edit/main/notes/notes04.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/michaelweylandt/STA9890/blob/main/notes/notes04.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/michaelweylandt/STA9890/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>